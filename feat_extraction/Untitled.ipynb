{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dbscan is density based, but you don't have to input any # of clsuters \n",
    "\n",
    "# still put in stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for exam laptops need intenret conneiton sumbit your work: cov ers all topics: assignments 1.5 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DB scan = ensity based spatial clsuteirn gof appicaitons with noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it's very powerfula nd CAPUTRES COMPLEX STHAPES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN IS SLOWER THAN KMEANS \n",
    "\n",
    "# BUT DBSCAN SCALES WELL TO LARGE DATASETS \n",
    "\n",
    "# the two main parameters are minpts and Epsilon. \n",
    "\n",
    "# it will find n clusters based on the size of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dbscan algorithm: \n",
    "\n",
    "# input:  D,  Epsilon (radius parameter), minpts (the neighborhood density threshold)\n",
    "\n",
    "#output: A set of density-based clusters, or \"a clustering\"\n",
    "################################################################3\n",
    "# How to start it####\n",
    "\n",
    "# 1: mark all objects as unvisited\n",
    "\n",
    "\n",
    "# do:  # so it's a do untill ;) \n",
    "    #2: Randomly select an unvisited object, p\n",
    "\n",
    "    #3 mark p as visited \n",
    "    #4: if the epsilon-neighborhood of p has AT LEAST minpts objects\n",
    "        #Create a new cluster, C and add p to C\n",
    "        # let N be the set of objects in the epsilon-neighborhood  (of p?)\n",
    "        #5: for each point, p' in N:\n",
    "            # if p' is unvisited\n",
    "                #mark p' as visited\n",
    "                #if the epsilon-neighborhood of p' has at least minpts points\n",
    "                     # add those points to N\n",
    "            # if p' is NOT yet a member of any cluster, add p' to C.\n",
    "        #   end for: output C, the cluster\n",
    "       # else: (for if the points doesn't have minpts objects)\n",
    "       #mark pa s noise....\n",
    "        \n",
    "# until  no objects is unvisited.... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## he says it's good to mark all objects as unvisited?\n",
    "# you don't want to go over all of the datapoints many times, so we know what to visit when we scale out...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# he draws out on board: \n",
    "\n",
    "# shows a vector with base point at p.... then you draw out distance epsilon, which is the radius of your point \n",
    "# but remember, the radius is like.... n-dimensional... so not jsut circle or sphere it's a hyper-sphere or something.... \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so at every point we draw that sphere around your point and then search for # of objects.... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# think of the first step of drawing a cluster.... it begins with point p.....\n",
    "# but thenw e look at at p' points in the neighborhood of p... if ONE OF THEM... has minpts in its neighborhood.... \n",
    "# so apparently N grows.... so it is part of the 1st cluster C???? \n",
    "\n",
    "\n",
    "# so the neighborhood of points close to the INITIAL POINT..... expands... so you extend the cluster... \n",
    "# selim says it lets us expand the cluster\n",
    "\n",
    "# so it will let us go slowly with epsilon.... \n",
    "\n",
    "# essentially it goes in the direciton of the density (or where points are) AND EXTENDS IT AS MUCH AS POSSIBLE.... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# he suggests we implement it on our own... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so selim looks @ a bunh of stuff like the do-until i.e. we visit all unvisite dobjects do it until everything is visited.... \n",
    "\n",
    "# we will have some 'notions' \n",
    "\n",
    "# we will talk about DENSITY REACHABLE OBJECTS.... \n",
    "\n",
    "# remember kids, p is a core object, (one that has at LEAST minpts....)\n",
    "# if you have a core object you have room to expand....\n",
    "\n",
    "# we do the expansion BY THE NEIGHBORS..., which can be another core object... \n",
    "\n",
    "# if they are 'what he calls.... EDGE' i.e. they are IN the clsuter BUT THEY ARE NOT CORE POINTS.... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so selim tells us that the 'if the epsilon neighborhood of p' has at LEAST minPts points...a d those points to N' \n",
    "# part is where we expand.... \n",
    "\n",
    "\n",
    "# oh, so like..... you go from p' to p'' esssentially because you get new core points to be 'p'..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so.... will you ever get more than one object in the same cluster? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# right so if you find a noise point it can't move on from there.... \n",
    "\n",
    "# selim said in k-means you can force it to do worse by giving it terrible staritn points but it doesn't really matter there... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-WHAT applicaiton... you CAN make it much faster with multi-threading or parallel programming oh that's what he does \n",
    "\n",
    "# what he needs to kno is if a point is visited, what cluster is it in, and if it is... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in implementation noise objecst DON'T have to be one object.... they give a value of -1 to a noise object??? \n",
    "\n",
    "# YEAH YOU CAN HAVE  more than one obejct be noise??? \n",
    "\n",
    "# there are 2general sets of points... \n",
    "\n",
    "# N and C???? \n",
    "\n",
    "# there is another set of points that comes from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so I bleeive I was correct, when you check if a new point, p' is in a cluster, the only cluster it COUlD be in is the one your'e techicallly already working with \n",
    "\n",
    "# not another one that was considered uncreachable..... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how do you choose epsilon and the minpts\n",
    "#??? it's easy if you HAVE labels.... but if not... it's not great \n",
    "\n",
    "# it's hard to find objective measures... he will talk about some kind of measure to minimize (well you technically have to grid search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok, so he drawss three points in essentially a straight line, and thena bunch of points around them. \n",
    "\n",
    "# now he says that some points are 'density reachable'\n",
    "\n",
    "# i.e. you can move from x1 to x2... x2 to x3.... \n",
    "\n",
    "# AS LONG AS ... there is some point in the clsuter that is epsilon away from another point.... etc. until you hit the point.... \n",
    "\n",
    "# and then N Is not reachable.... /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as we mentioned beofre... THE IS... A DISTANCE METRIC we use here... obviously \n",
    "\n",
    "# it could be minowski with p3.... hamming distance.... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\seanp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk as nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "import os\n",
    "import re \n",
    "import sklearn \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stpwds = set(stopwords.words('english'))\n",
    "#nltk.download('wordnet')\n",
    "nlp = spacy.load('C://Users//seanp//Anaconda3//Lib//site-packages//en_core_web_sm//en_core_web_sm-2.0.0')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "from nltk.stem.porter import *\n",
    "porter_stemmer = PorterStemmer()\n",
    "import nltk.data\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "treebank_tokenizer = TreebankWordTokenizer()\n",
    "from nltk.util import bigrams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = 'C:\\\\Users\\\\seanp\\\\OneDrive\\\\Documents\\\\Github\\\\NLP_Project\\\\NLP-Movie_Scripts'\n",
    "# grab the locations of those lil jsons\n",
    "json_files = [pos_json for pos_json in os.listdir(wd + '\\\\diag_jsons\\\\') if pos_json.endswith('.json')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_multiple_strings(cur_string, replace_list,low= False):\n",
    "    for cur_word in replace_list:\n",
    "        if low == False:\n",
    "            cur_string = cur_string.replace(cur_word, '')\n",
    "        else:\n",
    "            cur_string = cur_string.replace(cur_word.lower(), '')\n",
    "    return cur_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have processed 10 scripts\n",
      "We have procesed 30 scripts\n",
      "We have processed 100 scripts\n",
      " HALFWAY We have processed 330 Scripts\n"
     ]
    }
   ],
   "source": [
    "# Original Process with LEMMATIZATION but no bigrams (we do remove all character names, we will also want to get rid of stopwords )\n",
    "slim_pos = ['JJ','JJR','JJS','NN','NNS','RB','RBR','RBS','VB','VBD','VBG','VBN','VBP','VBZ']\n",
    "fat_pos = ['JJ','JJR','JJS','NN','NNS','NNP','NNPS','RB','RBR','RBS','VB','VBD','VBG','VBN','VBP','VBZ','WDT','WP','WRB']\n",
    "\n",
    "mov_dict = {}\n",
    "countar = 0\n",
    "char_series = pd.Series()\n",
    "for fil in json_files:\n",
    "    countar +=1\n",
    "    if countar == 10:\n",
    "        print('We have processed 10 scripts')\n",
    "        \n",
    "    \n",
    "    if countar == 30:\n",
    "        print('We have procesed 30 scripts')\n",
    "    if countar == 100:\n",
    "        print('We have processed 100 scripts')\n",
    "    if countar == 330: \n",
    "        print(' HALFWAY We have processed 330 Scripts')\n",
    "    \n",
    "    # read the json file from the directory \n",
    "    with open(wd+ '\\\\diag_jsons\\\\'+fil, \"r\") as read_file:\n",
    "        data = json.load(read_file)\n",
    "        \n",
    "    mov_dict[fil.replace('.json','')] = data\n",
    "    # now that we have done this \n",
    "    # we may as well start loading up our pandas series with the character texts.\n",
    "    # so we will start iterating through each character in the new json file \n",
    "    # added to our dictionary and make each character a new iterm in our series \n",
    "    # which will be indexed by the title of the script/movie and the character's name \n",
    "    # with four '_'s between them \n",
    "    # so we can do easy retrieval of which character was from which movie :)\n",
    "    mov_chars = mov_dict[fil.replace('.json','')]['dialogues'].keys()\n",
    "    for char in mov_dict[fil.replace('.json','')]['dialogues'].keys():\n",
    "        #char_series.at[char+'____'+fil.replace('.json','')] = mov_dict[fil.replace('.json','')]['dialogues'][char]\n",
    "        char_series.at[char+'____'+fil.replace('.json','')] = re.sub(\"\\\\n\\[\\d+\\]\",'',mov_dict[fil.replace('.json','')]['dialogues'][char])\n",
    "        # until I think of a better way of replacing the names, \n",
    "        # I have to make the entire text lowercase \n",
    "        # so that way I get all instances of the character names being used.... \n",
    "        char_series.at[char+'____'+fil.replace('.json','')] = char_series.at[char+'____'+fil.replace('.json','')].lower()\n",
    "        # now we want to remove the names of the characters, because those seemed to be too important in creating clusters\n",
    "        char_series.at[char+'____'+fil.replace('.json','')] = remove_multiple_strings(char_series.at[char+'____'+fil.replace('.json','')],mov_chars,low=True)\n",
    "        \n",
    "        \n",
    "        tok_chars = sentence_tokenizer.tokenize(char_series.at[char+'____'+fil.replace('.json','')])\n",
    "            # so... we need to do this for every sentence.... YAY.... \n",
    "        lemz = ''\n",
    "        # I think I need to get rid of punctuation unless lemmatization gets rid of it \n",
    "        for sentence in tok_chars:\n",
    "            toks = treebank_tokenizer.tokenize(sentence)\n",
    "            narrow_toks = [word[0] for word in nltk.pos_tag(toks) if word[1] in slim_pos]\n",
    "            lemmatized_words = [wordnet_lemmatizer.lemmatize(token) for token in narrow_toks]\n",
    "            lemz += \" \".join(narrow_toks) + ' '\n",
    "            lemz = [w for w in treebank_tokenizer.tokenize(lemz) if w not  in stpwds]\n",
    "            lemz = \" \".join(lemz) + ' '\n",
    "\n",
    "            #lemz = [tok for tok in nlp(lemz) if tok.pos_ in ['NOUN','VERB','ADV','ADJ'] ]\n",
    "            \n",
    "        char_series.at[char+'____'+fil.replace('.json','')] = lemz\n",
    "        \n",
    "        # we get rid of those pesky \\n[number] patterns that denote a line change \n",
    "        # which I suppose I can add back IN as a second column if necessary , but that's just going to be unnecessary for the clustering!\n",
    "\n",
    "# I'm guessing that every thime there is a new line it will go with that kind of scheme \n",
    "# so I suppose my job will be to remove those bois if they still occur after tokenization.... hoping they just dissapear, lmao !!!\n",
    "\n",
    "# also, very nice, I can iterate over the ['dialogues.keys()']\n",
    "# this is VERY fun!!!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "661"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it took roughly 10 minutes to get 100 scripts.... \n",
    "c1 = 0 \n",
    "for fil in json_files:\n",
    "    c1 +=1\n",
    "c1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KAT____10-Things-I-Hate-About-You_script           leave said leave n't read hardy boys book guy ...\n",
       "PATRICK____10-Things-I-Hate-About-You_script       missed bratwurst eating lunch mother goose tou...\n",
       "BIANCA____10-Things-I-Hate-About-You_script        change hair wan na think really nowhere hi dad...\n",
       "CAMERON____10-Things-I-Hate-About-You_script       n't think ma'am tell north actually yeah coupl...\n",
       "MICHAEL____10-Things-I-Hate-About-You_script       new guy c'mon 'm supposed give tour dakota kid...\n",
       "JOEY____10-Things-I-Hate-About-You_script          opposed bitter self-righteous hag friends look...\n",
       "WALTER____10-Things-I-Hate-About-You_script        hope dinner ready ten minutes mrs. johnson squ...\n",
       "MANDELLA____10-Things-I-Hate-About-You_script      uh read n't sister amazingly never read idea a...\n",
       "MISS PERKY____10-Things-I-Hate-About-You_script    'm sure n't find padua different old school li...\n",
       "CHASTITY____10-Things-I-Hate-About-You_script      great oily dry n't think highlights dating dor...\n",
       "SHARON____10-Things-I-Hate-About-You_script        microwave 's synonym throbbing swollen turgid ...\n",
       "BRUCE____10-Things-I-Hate-About-You_script         count go ahead take easy guys next time leave ...\n",
       "TEACHER____10-Things-I-Hate-About-You_script       realize language mr. shakespeare makes bit dau...\n",
       "COLE____12-Monkeys_script                          ssssst 's going n't volunteer trouble sir went...\n",
       "RAILLY____12-Monkeys_script                        's tested drugs restraints explain bruises gue...\n",
       "JEFFREY____12-Monkeys_script                       much gon pay huh job okay thousand 's enough d...\n",
       "ASTROPHYSICIST____12-Monkeys_script                want tell last night wake determined proper au...\n",
       "RASPY VOICE____12-Monkeys_script                   hey 's hey bob 's name talk wah'dja bobby boy ...\n",
       "FALE____12-Monkeys_script                          help looking something particular want money b...\n",
       "BOTANIST____12-Monkeys_script                      's important observe everything volunteer real...\n",
       "MICROBIOLOGIST____12-Monkeys_script                thank wait outside n't sit mr . get spider lat...\n",
       "JOSE____12-Monkeys_script                          volunteers god pulling tooth man nuts take kid...\n",
       "ENGINEER____12-Monkeys_script                      n't think 's going hurt 're going hurt mr . ap...\n",
       "BILLINGS____12-Monkeys_script                      lem see head jimbo see got creepy crawlies got...\n",
       "GEOLOGIST____12-Monkeys_script                     time waste drugs women helped reclaim planet m...\n",
       "BEN____12-Monkeys_script                           told fuckhead mason get something yeah tucking...\n",
       "ZOOLOGIST____12-Monkeys_script                     possibly play important role returning human r...\n",
       "WALLACE____12-Monkeys_script                       territory bitch 're kind tough guy wan na hero...\n",
       "FRANKI____12-Monkeys_script                        get ask guy real nice kind i.d. gets agitated ...\n",
       "RAILLY'S VOICE____12-Monkeys_script                according accounts local officials time gentle...\n",
       "TEDDY____12-Monkeys_script                         n't know anything army twelve monkeys n't frie...\n",
       "dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions, These are from Stephen\n",
    "def get_keys(topic_matrix):\n",
    "    '''returns an integer list of predicted topic categories for a given topic matrix'''\n",
    "    keys = []\n",
    "    for i in range(topic_matrix.shape[0]):\n",
    "        keys.append(topic_matrix[i].argmax())\n",
    "    return keys\n",
    "###################################################################################\n",
    "def keys_to_counts(keys):\n",
    "    '''returns a tuple of topic categories and their accompanying magnitudes for a given list of keys'''\n",
    "    count_pairs = Counter(keys).items()\n",
    "    categories = [pair[0] for pair in count_pairs]\n",
    "    counts = [pair[1] for pair in count_pairs]\n",
    "    return (categories, counts)\n",
    "\n",
    "##############################################################################\n",
    "def get_top_n_words(n, keys, document_term_matrix, count_vectorizer):\n",
    "    '''returns a list of n_topic strings, where each string contains the n most common \n",
    "        words in a predicted category, in order'''\n",
    "    top_word_indices = []\n",
    "    for topic in range(n_topics):\n",
    "        temp_vector_sum = 0\n",
    "        for i in range(len(keys)):\n",
    "            if keys[i] == topic:\n",
    "                temp_vector_sum += document_term_matrix[i]\n",
    "        temp_vector_sum = temp_vector_sum.toarray()\n",
    "        top_n_word_indices = np.flip(np.argsort(temp_vector_sum)[0][-n:],0)\n",
    "        top_word_indices.append(top_n_word_indices)   \n",
    "    top_words = []\n",
    "    for topic in top_word_indices:\n",
    "        topic_words = []\n",
    "        for index in topic:\n",
    "            temp_word_vector = np.zeros((1,document_term_matrix.shape[1]))\n",
    "            temp_word_vector[:,index] = 1\n",
    "            the_word = count_vectorizer.inverse_transform(temp_word_vector)[0][0]\n",
    "            topic_words.append(the_word.encode('ascii').decode('utf-8'))\n",
    "        top_words.append(\" \".join(topic_words))         \n",
    "    return top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seanp\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<11075x10000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1341714 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:  know got want right ve think going good come time let look tell man say yeah okay way need gon\n",
      "Topic 1:  got fuck gon fuckin shit yeah ta ai let man hey guy goin barry oh double fucking ass swamp die\n",
      "Topic 2:  door looks man turns hand room eyes face got head away open car comes pulls gun takes starts looking look\n",
      "Topic 3:  looks yeah turns hey door eyes takes walks car stares sees hand away camera look pulls water floor steps okay\n",
      "Topic 4:  07 revision pink 20 white 19 yellow seconds war run coffee bridge sheets african stocks nation american oldest la event\n",
      "Topic 5:  schultz dr know nigger horse man good big little got looks ai german candie white dollars says mr yes say\n",
      "Topic 6:  fuck fucking know shit fuckin man little bitch gon god look yeah shut car thing tell want hell talking turn\n",
      "Topic 7:  man hey fucking dr shit authorization yeah really fuck ass little stones god good high wow think sorry let old\n",
      "Topic 8:  dr going jones schultz box right hot got die quite want big plan base fact iron talk men look think\n",
      "Topic 9:  want daddy come mr fuckin okay money say look love good gon rothstein tell papa thank right house sorry let\n",
      "Topic 10:  witness ve atmosphere court percent lieutenant defendants lt sustained morning objection step charge guilty defense mr counsel case members meeting\n",
      "Topic 11:  mr sir ve right president reid point let order minutes ready left captain gentlemen ship times okay course yeah mrs\n",
      "Topic 12:  dragon toothless final 02 sanders 13 2010 deblois draft red eyes head know rum dragons kill okay turns ve death\n",
      "Topic 13:  little girl roll staring sausage went strange bring lovely yah present heir finding figuring filing files filed file figures filling\n",
      "Topic 14:  man want death men flight black text monte draft exile hearing seen november getting die ma sweat government ve today\n",
      "Topic 15:  good love mr new money time night rose life torrance old great beautiful day turned prince live evening miss pounds\n",
      "Topic 16:  think going mr said president sir work wilson people kind killed white right house wallet mean feel hell started new\n",
      "Topic 17:  yes sir sorry mr good help thank ve course think got need sure said know looks wait absolutely um going\n",
      "Topic 18:  good need minutes yes guys sure thing luck help number london area ai son day work stay place time people\n",
      "Topic 19:  right stand questions brown yeah yes way hi lock shut oooh thank special life think quite messages load like story\n",
      "Topic 20:  come yeah father kill dad help going mom mommy daddy mother asgard home god people majesty hey die need holiness\n",
      "Topic 21:  sir captain know right ve think got yes say thank good sorry want going mr aye way come dead admiral\n",
      "Topic 22:  sequence original script ted hey look pounds mr guys going wan come says slaves god think bonjour fish water tom\n",
      "Topic 23:  right destination love enter larry guys sorry looks left order place asked moment ah picture talking hot takes look destroy\n",
      "Topic 24:  ya got say buddy know want let bloody good right tell ha fight hi man years tired think nothin yo\n",
      "Topic 25:  think dad mom note okay hand italian minister great drill quit gals coup recognize hear left excused old came good\n",
      "Topic 26:  mr time brown way like good senor torpedo reynolds chairman second movie john vote machine ve final set long science\n",
      "Topic 27:  come president let like crazy fuck prime minister special line love follow grace today turn better hey jackson comes want\n",
      "Topic 28:  tell let hell minutes car love counting shoot girl death say heard shit away law line comes massey happened hold\n",
      "Topic 29:  going gon walk buoy god die lord water man rise old say chair swell eat feel look line children drops\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words='english', max_features=10000)\n",
    "text_sample = char_series.as_matrix()\n",
    "# so we essenatially create our document_term matrix.... \n",
    "#display(small_text_sample)\n",
    "document_term_matrix = count_vectorizer.fit_transform(text_sample)\n",
    "display(document_term_matrix)\n",
    "\n",
    "n_topics = 30\n",
    "\n",
    "lsa_model = TruncatedSVD(n_components=n_topics)\n",
    "lsa_topic_matrix = lsa_model.fit_transform(document_term_matrix)\n",
    "   # display(lsa_topic_matrix)\n",
    "\n",
    "lsa_keys = get_keys(lsa_topic_matrix)\n",
    "lsa_categories, lsa_counts = keys_to_counts(lsa_keys)\n",
    "\n",
    "top_n_words_lsa = get_top_n_words(20, lsa_keys, document_term_matrix, count_vectorizer)\n",
    "\n",
    "for i in range(len(top_n_words_lsa)):\n",
    "    print(\"Topic {}: \".format(i), top_n_words_lsa[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing numbers is still kinda a thing!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
