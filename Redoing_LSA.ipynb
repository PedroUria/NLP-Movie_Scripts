{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\seanp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk as nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "import os\n",
    "import re \n",
    "import sklearn \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stpwds = set(stopwords.words('english'))\n",
    "#nltk.download('wordnet')\n",
    "nlp = spacy.load('C://Users//seanp//Anaconda3//Lib//site-packages//en_core_web_sm//en_core_web_sm-2.0.0')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "from nltk.stem.porter import *\n",
    "porter_stemmer = PorterStemmer()\n",
    "import nltk.data\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "treebank_tokenizer = TreebankWordTokenizer()\n",
    "from nltk.util import bigrams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = os.getcwd() \n",
    "# grab the locations of those lil jsons\n",
    "json_files = [pos_json for pos_json in os.listdir(wd + '\\\\diag_jsons\\\\') if pos_json.endswith('.json')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10-Things-I-Hate-About-You_script'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(wd+ '\\\\diag_jsons\\\\'+json_files[0], \"r\") as read_file:\n",
    "    data = json.load(read_file)\n",
    "json_files[0].replace('.json','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_multiple_strings(cur_string, replace_list,low= False):\n",
    "    for cur_word in replace_list:\n",
    "        if low == False:\n",
    "            cur_string = cur_string.replace(cur_word, '')\n",
    "        else:\n",
    "            cur_string = cur_string.replace(cur_word.lower(), '')\n",
    "    return cur_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have processed 10 scripts\n"
     ]
    }
   ],
   "source": [
    "# Original Process with LEMMATIZATION but no bigrams (we do remove all character names, we will also want to get rid of stopwords )\n",
    "slim_pos = ['JJ','JJR','JJS','NN','NNS','RB','RBR','RBS','VB','VBD','VBG','VBN','VBP','VBZ']\n",
    "fat_pos = ['JJ','JJR','JJS','NN','NNS','NNP','NNPS','RB','RBR','RBS','VB','VBD','VBG','VBN','VBP','VBZ','WDT','WP','WRB']\n",
    "\n",
    "mov_dict = {}\n",
    "countar = 0\n",
    "char_series = pd.Series()\n",
    "for fil in json_files:\n",
    "    countar +=1\n",
    "    if countar == 10:\n",
    "        print('We have processed 10 scripts')\n",
    "    \n",
    "    # read the json file from the directory \n",
    "    with open(wd+ '\\\\diag_jsons\\\\'+fil, \"r\") as read_file:\n",
    "        data = json.load(read_file)\n",
    "        \n",
    "    mov_dict[fil.replace('.json','')] = data\n",
    "    # now that we have done this \n",
    "    # we may as well start loading up our pandas series with the character texts.\n",
    "    # so we will start iterating through each character in the new json file \n",
    "    # added to our dictionary and make each character a new iterm in our series \n",
    "    # which will be indexed by the title of the script/movie and the character's name \n",
    "    # with four '_'s between them \n",
    "    # so we can do easy retrieval of which character was from which movie :)\n",
    "    mov_chars = mov_dict[fil.replace('.json','')]['dialogues'].keys()\n",
    "    for char in mov_dict[fil.replace('.json','')]['dialogues'].keys():\n",
    "        #char_series.at[char+'____'+fil.replace('.json','')] = mov_dict[fil.replace('.json','')]['dialogues'][char]\n",
    "        char_series.at[char+'____'+fil.replace('.json','')] = re.sub(\"\\\\n\\[\\d+\\]\",'',mov_dict[fil.replace('.json','')]['dialogues'][char])\n",
    "        # until I think of a better way of replacing the names, \n",
    "        # I have to make the entire text lowercase \n",
    "        # so that way I get all instances of the character names being used.... \n",
    "        char_series.at[char+'____'+fil.replace('.json','')] = char_series.at[char+'____'+fil.replace('.json','')].lower()\n",
    "        # now we want to remove the names of the characters, because those seemed to be too important in creating clusters\n",
    "        char_series.at[char+'____'+fil.replace('.json','')] = remove_multiple_strings(char_series.at[char+'____'+fil.replace('.json','')],mov_chars,low=True)\n",
    "        \n",
    "        \n",
    "        tok_chars = sentence_tokenizer.tokenize(char_series.at[char+'____'+fil.replace('.json','')])\n",
    "            # so... we need to do this for every sentence.... YAY.... \n",
    "        lemz = ''\n",
    "        # I think I need to get rid of punctuation unless lemmatization gets rid of it \n",
    "        for sentence in tok_chars:\n",
    "            toks = treebank_tokenizer.tokenize(sentence)\n",
    "            narrow_toks = [word[0] for word in nltk.pos_tag(toks) if word[1] in slim_pos]\n",
    "            #lemmatized_words = [wordnet_lemmatizer.lemmatize(token) for token in toks]\n",
    "            lemz += \" \".join(narrow_toks) + ' '\n",
    "            lemz = [w for w in treebank_tokenizer.tokenize(lemz) if w not  in stpwds]\n",
    "            lemz = \" \".join(lemz) + ' '\n",
    "\n",
    "            #lemz = [tok for tok in nlp(lemz) if tok.pos_ in ['NOUN','VERB','ADV','ADJ'] ]\n",
    "            \n",
    "        char_series.at[char+'____'+fil.replace('.json','')] = lemz\n",
    "        \n",
    "        # we get rid of those pesky \\n[number] patterns that denote a line change \n",
    "        # which I suppose I can add back IN as a second column if necessary , but that's just going to be unnecessary for the clustering!\n",
    "\n",
    "# I'm guessing that every thime there is a new line it will go with that kind of scheme \n",
    "# so I suppose my job will be to remove those bois if they still occur after tokenization.... hoping they just dissapear, lmao !!!\n",
    "\n",
    "# also, very nice, I can iterate over the ['dialogues.keys()']\n",
    "# this is VERY fun!!!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions, These are from Stephen\n",
    "def get_keys(topic_matrix):\n",
    "    '''returns an integer list of predicted topic categories for a given topic matrix'''\n",
    "    keys = []\n",
    "    for i in range(topic_matrix.shape[0]):\n",
    "        keys.append(topic_matrix[i].argmax())\n",
    "    return keys\n",
    "###################################################################################\n",
    "def keys_to_counts(keys):\n",
    "    '''returns a tuple of topic categories and their accompanying magnitudes for a given list of keys'''\n",
    "    count_pairs = Counter(keys).items()\n",
    "    categories = [pair[0] for pair in count_pairs]\n",
    "    counts = [pair[1] for pair in count_pairs]\n",
    "    return (categories, counts)\n",
    "\n",
    "##############################################################################\n",
    "def get_top_n_words(n, keys, document_term_matrix, count_vectorizer):\n",
    "    '''returns a list of n_topic strings, where each string contains the n most common \n",
    "        words in a predicted category, in order'''\n",
    "    top_word_indices = []\n",
    "    for topic in range(n_topics):\n",
    "        temp_vector_sum = 0\n",
    "        for i in range(len(keys)):\n",
    "            if keys[i] == topic:\n",
    "                temp_vector_sum += document_term_matrix[i]\n",
    "        temp_vector_sum = temp_vector_sum.toarray()\n",
    "        top_n_word_indices = np.flip(np.argsort(temp_vector_sum)[0][-n:],0)\n",
    "        top_word_indices.append(top_n_word_indices)   \n",
    "    top_words = []\n",
    "    for topic in top_word_indices:\n",
    "        topic_words = []\n",
    "        for index in topic:\n",
    "            temp_word_vector = np.zeros((1,document_term_matrix.shape[1]))\n",
    "            temp_word_vector[:,index] = 1\n",
    "            the_word = count_vectorizer.inverse_transform(temp_word_vector)[0][0]\n",
    "            topic_words.append(the_word.encode('ascii').decode('utf-8'))\n",
    "        top_words.append(\" \".join(topic_words))         \n",
    "    return top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seanp\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<11075x10000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1341714 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:  know got want right ve think going good come time let look tell man say yeah okay way need gon\n",
      "Topic 1:  got fuck gon fuckin shit yeah ta ai let man hey guy goin barry oh double fucking ass swamp die\n",
      "Topic 2:  door looks man turns hand room eyes face got head away open comes car pulls takes starts looking gun look\n",
      "Topic 3:  looks turns yeah hey door takes eyes walks stares car sees hand away camera floor pulls steps starts shower curtain\n",
      "Topic 4:  07 revision pink 20 white 19 yellow seconds war run coffee bridge sheets african stocks nation american oldest la event\n",
      "Topic 5:  schultz dr know nigger horse man good big little got looks ai german candie white dollars says mr yes say\n",
      "Topic 6:  fuck fucking know shit fuckin man bitch little gon shut god look yeah tell thing hell want car right holy\n",
      "Topic 7:  man hey fucking dr yeah authorization really fuck stones ass shit wow little good high sorry god wrong dad hell\n",
      "Topic 8:  dr going jones schultz box right hot got die quite want big plan base fact iron talk men look think\n",
      "Topic 9:  want come daddy mr fuckin gon okay love thank rothstein good right money let way look god casino tell sorry\n",
      "Topic 10:  witness atmosphere ve court percent lieutenant sustained defendants lt morning mr guilty defense step charge objection counsel marine jim aware\n",
      "Topic 11:  mr sir right ve president reid minutes order let gentlemen ready left ship captain yes times yeah okay control mrs\n",
      "Topic 12:  dragon toothless final 02 sanders 13 2010 deblois draft red eyes head know rum dragons kill okay turns ve death\n",
      "Topic 13:  little girl roll staring sausage went strange bring lovely yah present heir finding figuring filing files filed file figures filling\n",
      "Topic 14:  man want men flight death hearing exile seen getting ve ma number philip press today hurry merely message heh brought\n",
      "Topic 15:  good mr love night new old day rose torrance money life time beautiful prince turned great girls gentlemen ladies favorite\n",
      "Topic 16:  think going mr said president sir wilson work people brown killed white hell office wallet feel right waiting really maybe\n",
      "Topic 17:  yes sir sorry good thank help mr ve course think got know need said looks absolutely wait sure um guineas\n",
      "Topic 18:  good need ya yes help sure broke work luck guys number hand thing day guy son smart center place nice\n",
      "Topic 19:  okay dad need help hey release miss hurry sorry father looks told mother prisoner wedding send gave believe going fine\n",
      "Topic 20:  come yeah dad help going father mom daddy mommy mother asgard home kill people die want hey shut holiness dee\n",
      "Topic 21:  sir captain know right ve think got say yes thank good sorry want going mr come aye way admiral dead\n",
      "Topic 22:  hey mr pounds look going wan sir come miss think bonjour ya ma way water edward new guys huh bird\n",
      "Topic 23:  right destination gon enter ok looks guys larry left order hold clicks place picture succeed takes car talking hey moment\n",
      "Topic 24:  ya got say buddy want bloody good know right let tell ha hi man years tired think yo ai goin\n",
      "Topic 25:  think come ask tell minister major america red gals sounds dance room house war prime coup drill caine left dr\n",
      "Topic 26:  come people let police vous special crowd fuck crazy traitor say voices jack minister president senator prime nearest like house\n",
      "Topic 27:  mr time like brown minutes senor torpedo good reynolds second chairman closed movie vote set watch machine thing drop slow\n",
      "Topic 28:  good people king hear world sing new evening make looks princess gods army london beating broadcasting drums flag god meeting\n",
      "Topic 29:  going gon buoy die lord walk god water old chair rise swell man look feel say line complain burn ship\n",
      "Topic 30:  let love point need world new access think course help sorry denied mean happy light leave best welcome massey kill\n",
      "Topic 31:  shit girl group says let ai si heard rule house father ass papa baseball hell telling mouth old hole priest\n",
      "Topic 32:  say tell warrior arctic kill let scotland mr papa way shoot vessel united si north affirmative good king states island\n",
      "Topic 33:  acknowledged request god ok key dead oh man kind kitty money juvenile men le sorry pee guns kid pick police\n",
      "Topic 34:  look wait engines attention stomach good car minutes time seconds overload lucas da daddy love man thank fat water ha\n",
      "Topic 35:  god guilty defendant texas set court verdict judge count evening judgment record yea years council return great jury capital hi\n",
      "Topic 36:  god game yes gun thing life haines boss kill hell hear let jesus son help servant dead light stay want\n",
      "Topic 37:  money november monte draft like water employer okay bush george kuwait pancakes food cards likes says fuck films karl native\n",
      "Topic 38:  said ellen leave sorry hey bad momma let water dream day gave miss fireplace heard maybe key burning called eat\n",
      "Topic 39:  script original sequence ted video money make blood breathe game sure come american hide cut embassy box hop pretty harris\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words='english', max_features=10000)\n",
    "text_sample = char_series.as_matrix()\n",
    "# so we essenatially create our document_term matrix.... \n",
    "#display(small_text_sample)\n",
    "document_term_matrix = count_vectorizer.fit_transform(text_sample)\n",
    "display(document_term_matrix)\n",
    "\n",
    "n_topics = 40\n",
    "\n",
    "lsa_model = TruncatedSVD(n_components=n_topics)\n",
    "lsa_topic_matrix = lsa_model.fit_transform(document_term_matrix)\n",
    "   # display(lsa_topic_matrix)\n",
    "\n",
    "lsa_keys = get_keys(lsa_topic_matrix)\n",
    "lsa_categories, lsa_counts = keys_to_counts(lsa_keys)\n",
    "\n",
    "top_n_words_lsa = get_top_n_words(20, lsa_keys, document_term_matrix, count_vectorizer)\n",
    "\n",
    "for i in range(len(top_n_words_lsa)):\n",
    "    print(\"Topic {}: \".format(i), top_n_words_lsa[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#''.join([i for i in X if not i.isdigit()])\n",
    "# THIS IS IT BOYS!!!! HOW THE NUMBERS GO AWAY!!! \n",
    "char_series = pd.DataFrame(char_series)\n",
    "for index, row in char_series.iterrows():\n",
    "    char_series.loc[index,row[0]] = ''.join([i for i in row[0] if not i.isdigit()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'char_series' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b150134aa306>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mchar_series\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'char_series' is not defined"
     ]
    }
   ],
   "source": [
    "char_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
