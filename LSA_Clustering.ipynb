{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\seanp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# step 1: import dem libraries kiddos!\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk as nltk\n",
    "import spacy\n",
    "import os\n",
    "import re \n",
    "import sklearn \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from readability import Readability  #https://github.com/cdimascio/py-readability-metrics\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stpwds = set(stopwords.words('english'))\n",
    "#nltk.download('wordnet')\n",
    "nlp = spacy.load('C://Users//seanp//Anaconda3//Lib//site-packages//en_core_web_sm//en_core_web_sm-2.0.0')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "from nltk.stem.porter import *\n",
    "porter_stemmer = PorterStemmer()\n",
    "import nltk.data\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "treebank_tokenizer = TreebankWordTokenizer()\n",
    "from nltk.util import bigrams \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the diaglogue Json Files!!!!!\n",
    "\n",
    "wd = os.getcwd() \n",
    "# grab the locations of those lil jsons\n",
    "json_files = [pos_json for pos_json in os.listdir(wd + '\\\\diag_jsons\\\\') if pos_json.endswith('.json')]\n",
    "\n",
    "# read those guys in!\n",
    "\n",
    "# the first step will be to READ each file, then the NEXT step will be to store it in something that will EVENTUALLY \n",
    "# be converted into a dataframe (actually, series)\n",
    "\n",
    "# so I reckon the way to start this bad boy off is to just store them all in a SHINY NEW DICTIONARY\n",
    "\n",
    "# and let the key be the movie name  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KAT____10-Things-I-Hate-About-You_script           leave_it it_i i_said said_, ,_leave leave_it i...\n",
       "PATRICK____10-Things-I-Hate-About-You_script       i_missed missed_you you_. it_was was_a a_bratw...\n",
       "BIANCA____10-Things-I-Hate-About-You_script        did_you you_change change_your your_hair hair_...\n",
       "CAMERON____10-Things-I-Hate-About-You_script       i_do do_n't n't_think think_so so_, ,_ma'am ma...\n",
       "MICHAEL____10-Things-I-Hate-About-You_script       you_the the_new new_guy guy_? c'mon_. i_'m 'm_...\n",
       "JOEY____10-Things-I-Hate-About-You_script          as_opposed opposed_to to_a a_bitter bitter_sel...\n",
       "WALTER____10-Things-I-Hate-About-You_script        i_hope hope_dinner dinner_'s 's_ready ready_be...\n",
       "MANDELLA____10-Things-I-Hate-About-You_script      uh_, ,_yeah yeah_, ,_i i_read read_it it_all a...\n",
       "MISS PERKY____10-Things-I-Hate-About-You_script    i_'m 'm_sure sure_you you_wo wo_n't n't_find f...\n",
       "CHASTITY____10-Things-I-Hate-About-You_script      no_. who_? great_is is_he he_oily oily_or or_d...\n",
       "SHARON____10-Things-I-Hate-About-You_script        in_the the_microwave microwave_. what_'s 's_a ...\n",
       "BRUCE____10-Things-I-Hate-About-You_script         i_can can_count count_. go_ahead ahead_. and_y...\n",
       "TEACHER____10-Things-I-Hate-About-You_script       i_realize realize_the the_language language_of...\n",
       "COLE____12-Monkeys_script                          ssssst_! ,_what what_'s 's_going going_on on_?...\n",
       "RAILLY____12-Monkeys_script                        he_'s 's_been been_tested tested_for for_drugs...\n",
       "JEFFREY____12-Monkeys_script                       how_much much_you you_gon gon_na na_pay pay_me...\n",
       "ASTROPHYSICIST____12-Monkeys_script                we_want want_you you_to to_tell tell_us us_abo...\n",
       "RASPY VOICE____12-Monkeys_script                   hey_! who_'s 's_that that_? hey_, ,_bob bob_.....\n",
       "FALE____12-Monkeys_script                          uh_, ,_can can_we we_help help_you you_? excus...\n",
       "BOTANIST____12-Monkeys_script                      it_'s 's_important important_to to_observe obs...\n",
       "MICROBIOLOGIST____12-Monkeys_script                thank_you you_. you_two two_wait wait_outside ...\n",
       "JOSE____12-Monkeys_script                          ``_volunteers volunteers_'' ''_again again_.  ...\n",
       "ENGINEER____12-Monkeys_script                      i_do do_n't n't_think think_he he_'s 's_going ...\n",
       "BILLINGS____12-Monkeys_script                      lem_me me_see see_your your_head head_, ,_jimb...\n",
       "GEOLOGIST____12-Monkeys_script                     what_did did_you you_do do_with with_your your...\n",
       "BEN____12-Monkeys_script                           i_told told_you you_that that_fuckhead fuckhea...\n",
       "ZOOLOGIST____12-Monkeys_script                     and_possibly possibly_play play_an an_importan...\n",
       "WALLACE____12-Monkeys_script                       this_is is_my my_territory territory_, ,_bitch...\n",
       "FRANKI____12-Monkeys_script                        --_so so_they they_get get_there there_and and...\n",
       "RAILLY'S VOICE____12-Monkeys_script                according_to to_the the_accounts accounts_of o...\n",
       "                                                                         ...                        \n",
       "PILOT____Zero-Dark-Thirty_script                   ten_minutes minutes_. three_mikes mikes_to to_...\n",
       "FARAJ____Zero-Dark-Thirty_script                   you_'re 're_thinking thinking_of of_abu abu_kh...\n",
       "JARED____Zero-Dark-Thirty_script                   hey_, ,_what what_are are_you you_listening li...\n",
       "COMMANDING OFFICER____Zero-Dark-Thirty_script      negative_. i_'m 'm_internal internal_, ,_i i_'...\n",
       "SOLDIER INTERROGATOR____Zero-Dark-Thirty_script    you_and and_i i_are are_gon gon_na na_talk tal...\n",
       "REPORTER____Zero-Dark-Thirty_script                this_is is_what what_remains remains_of of_the...\n",
       "SECURTY GUARD____Zero-Dark-Thirty_script           quick_question question_: :_all all_is is_taki...\n",
       "WOLF____Zero-Dark-Thirty_script                    alaykum_salam salam_. where_you you_gon gon_na...\n",
       "SABER____Zero-Dark-Thirty_script                   that_'s 's_not not_a a_door door_. khaled_! wa...\n",
       "HOPPS____Zootopia_script                           thank_you you_! oh_hi hi_, ,_i i_'m 'm_! your_...\n",
       "NICK____Zootopia_script                            i_'m 'm_not not_looking looking_for for_any an...\n",
       "BOGO____Zootopia_script                            all_right right_, ,_all all_right right_. ever...\n",
       "STU HOPPS____Zootopia_script                       ,_you you_ever ever_wonder wonder_how how_your...\n",
       "BELLWETHER____Zootopia_script                      oh_! yes_right right_. congratulations_, ,_off...\n",
       "BONNIE HOPPS____Zootopia_script                    oh_yes yes_, ,_that that_'s 's_right right_, ,...\n",
       "FLASH____Zootopia_script                           nice_to to_... ..._see see_you you_... ..._too...\n",
       "CLAWHAUSER____Zootopia_script                      o-m_goodness goodness_! they_really really_did...\n",
       "YOUNG JUDY____Zootopia_script                      fear_. treachery_. bloodlust_! thousands_of of...\n",
       "LIONHEART____Zootopia_script                       as_mayor mayor_of of_zootopia zootopia_, ,_i i...\n",
       "DUKE WEASELTON____Zootopia_script                  catch_me me_if if_you you_can can_, ,_cottonta...\n",
       "GIDEON GREY____Zootopia_script                     bunny_cop cop_. that_is is_the the_most most_s...\n",
       "JUDY____Zootopia_script                            i_wo wo_n't n't_let let_you you_down down_. th...\n",
       "GAZELLE____Zootopia_script                         i_am am_. welcome_to to_zootopia zootopia_! wo...\n",
       "FRU FRU SHREW____Zootopia_script                   ohmygawd_, ,_did did_you you_see see_those tho...\n",
       "YAX THE HIPPIE YAK____Zootopia_script              oooooooooohmmmmmmmm_. oooooooooooohmmmmmm_. oo...\n",
       "MAJOR FRIEDKIN____Zootopia_script                  listen_up up_cadets cadets_! zootopia_has has_...\n",
       "ORYX POOTOSSER____Zootopia_script                  do_n't n't_expect expect_us us_to to_apologize...\n",
       "YAX____Zootopia_script                             uh_, ,_emmitt emmitt_otterton otterton_? been_...\n",
       "MEAN KID ANIMAL____Zootopia_script                 okay_, ,_! ready_for for_initiation initiation...\n",
       "DOUG____Zootopia_script                            you_got got_here here_. what_'s 's_the the_mar...\n",
       "Length: 11075, dtype: object"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I commented out an example. \n",
    "with open(wd+ '\\\\diag_jsons\\\\'+json_files[0], \"r\") as read_file:\n",
    "    data = json.load(read_file)\n",
    "json_files[0].replace('.json','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# found this on stack overflow\n",
    "def remove_multiple_strings(cur_string, replace_list,low= False):\n",
    "    for cur_word in replace_list:\n",
    "        if low == False:\n",
    "            cur_string = cur_string.replace(cur_word, '')\n",
    "        else:\n",
    "            cur_string = cur_string.replace(cur_word.lower(), '')\n",
    "    return cur_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original Process with LEMMATIZATION but no bigrams (we do remove all character names, we will also want to get rid of stopwords )\n",
    "mov_dict = {}\n",
    "char_series = pd.Series()\n",
    "for fil in json_files:\n",
    "    # read the json file from the directory \n",
    "    with open(wd+ '\\\\diag_jsons\\\\'+fil, \"r\") as read_file:\n",
    "        data = json.load(read_file)\n",
    "        \n",
    "    mov_dict[fil.replace('.json','')] = data\n",
    "    # now that we have done this \n",
    "    # we may as well start loading up our pandas series with the character texts.\n",
    "    # so we will start iterating through each character in the new json file \n",
    "    # added to our dictionary and make each character a new iterm in our series \n",
    "    # which will be indexed by the title of the script/movie and the character's name \n",
    "    # with four '_'s between them \n",
    "    # so we can do easy retrieval of which character was from which movie :)\n",
    "    mov_chars = mov_dict[fil.replace('.json','')]['dialogues'].keys()\n",
    "    for char in mov_dict[fil.replace('.json','')]['dialogues'].keys():\n",
    "        #char_series.at[char+'____'+fil.replace('.json','')] = mov_dict[fil.replace('.json','')]['dialogues'][char]\n",
    "        char_series.at[char+'____'+fil.replace('.json','')] = re.sub(\"\\\\n\\[\\d+\\]\",'',mov_dict[fil.replace('.json','')]['dialogues'][char])\n",
    "        # until I think of a better way of replacing the names, \n",
    "        # I have to make the entire text lowercase \n",
    "        # so that way I get all instances of the character names being used.... \n",
    "        char_series.at[char+'____'+fil.replace('.json','')] = char_series.at[char+'____'+fil.replace('.json','')].lower()\n",
    "        # now we want to remove the names of the characters, because those seemed to be too important in creating clusters\n",
    "        char_series.at[char+'____'+fil.replace('.json','')] = remove_multiple_strings(char_series.at[char+'____'+fil.replace('.json','')],mov_chars,low=True)\n",
    "        \n",
    "        \n",
    "        tok_chars = sentence_tokenizer.tokenize(char_series.at[char+'____'+fil.replace('.json','')])\n",
    "            # so... we need to do this for every sentence.... YAY.... \n",
    "        lemz = ''\n",
    "        # I think I need to get rid of punctuation unless lemmatization gets rid of it \n",
    "        for sentence in tok_chars:\n",
    "            toks = treebank_tokenizer.tokenize(sentence)\n",
    "            lemmatized_words = [wordnet_lemmatizer.lemmatize(token) for token in toks]\n",
    "    \n",
    "            lemz += \" \".join(lemmatized_words) + ' '\n",
    "            lemz = [w for w in treebank_tokenizer.tokenize(lemz) if w not  in stpwds]\n",
    "            lemz = [tok for tok in nlp(lemz) if tok.pos_ in ['NOUN','VERB','ADV','ADJ'] ]\n",
    "            \n",
    "        char_series.at[char+'____'+fil.replace('.json','')] = lemz\n",
    "        \n",
    "        # we get rid of those pesky \\n[number] patterns that denote a line change \n",
    "        # which I suppose I can add back IN as a second column if necessary , but that's just going to be unnecessary for the clustering!\n",
    "\n",
    "# I'm guessing that every thime there is a new line it will go with that kind of scheme \n",
    "# so I suppose my job will be to remove those bois if they still occur after tokenization.... hoping they just dissapear, lmao !!!\n",
    "\n",
    "# also, very nice, I can iterate over the ['dialogues.keys()']\n",
    "# this is VERY fun!!!! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_frame\n",
    "#thingz = sentence_tokenizer.tokenize(miniboi[0])\n",
    "# so... we need to do this for every sentence.... YAY.... \n",
    "#lemz = ''\n",
    "# I think I need to get rid of punctuation unless lemmatization gets rid of it \n",
    "#for sentence in thingz:\n",
    "#    toks = treebank_tokenizer.tokenize(sentence)\n",
    "#    lemmatized_words = [wordnet_lemmatizer.lemmatize(token) for token in toks]\n",
    "    \n",
    "#    lemz += \" \".join(lemmatized_words) + ' '\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for examples sake... run from the line that says mniboi  until the one that says miniboi[0]\n",
    "#miniboi\n",
    "\n",
    "#miniboi[0] = re.sub(\"\\\\n\\[\\d+\\]\",'',miniboi[0])\n",
    "#         char_series.at[char+'____'+fil.replace('.json','')] = re.sub(\"\\\\n\\[\\d+\\]\",'',mov_dict[fil.replace('.json','')]['dialogues'][char])\n",
    "\n",
    "#miniboi[0]\n",
    "\n",
    "# you want to lemmatize the the text or tokenize etc, \n",
    "\n",
    "# tokenize by sentence 1st...then apply the lemmatization to each sentence\n",
    "\n",
    "# then we have to find a way to concatenate or .join everything together across all of the lists!!! \n",
    "#miniboi[0]\n",
    "# Do some Data preprocessing on the text\n",
    "# maybe tokenizing, stemming or lemmatizing will get rid of the bad woyds!\n",
    "# so now I know to do this to the text to preprocess it, again, I can do it in the initial loop, or after. \n",
    "# it's kinda whatever. \n",
    "\n",
    "# time to lemmatize!!!!\n",
    "# or friggin tokenize, at least!\n",
    "#thingz = sentence_tokenizer.tokenize(miniboi[0])\n",
    "#print(thingz)\n",
    "#lemz = []\n",
    "#for sentence in thingz:\n",
    "#    lemmatized_words = [wordnet_lemmatizer.lemmatize(word) for word in sentence]\n",
    "#    lemz += lematized_words\n",
    "#lemmatized_words\n",
    "#\" \".join([\"John\", \"Charles\", \"Smith\"])\n",
    "\n",
    "# Time to get those term counts!!!!!\n",
    "#lemz\n",
    "#miniboi[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seanp\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# LSA, LSA!!!!\n",
    "# wondeirng what the count_vecotrizer does??? Here's the basic documentation: \n",
    "# Convert a collection of text documents to a matrix of token counts\n",
    "#This implementation produces a sparse representation of the counts using scipy.sparse.csr_matrix.\n",
    "#If you do not provide an a-priori dictionary and you do not use an analyzer that does some kind of feature selection then the number of features will be equal to the vocabulary size found by analyzing the data.\n",
    "count_vectorizer = CountVectorizer(stop_words='english', max_features=10000)\n",
    "text_sample = char_series.as_matrix()\n",
    "# so we essenatially create our document_term matrix.... \n",
    "#display(small_text_sample)\n",
    "document_term_matrix = count_vectorizer.fit_transform(text_sample)\n",
    "#display(document_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's reduce the dimensionality of that Document term matrix and do some clustering! \n",
    "\n",
    "# I\"m doing this for 2 reasons: 1, I essentially copy pasted from Stephen \n",
    "\n",
    "#2:.... we have a lot of data.... let's make the run time as fast as possible!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions, These are from Stephen\n",
    "def get_keys(topic_matrix):\n",
    "    '''returns an integer list of predicted topic categories for a given topic matrix'''\n",
    "    keys = []\n",
    "    for i in range(topic_matrix.shape[0]):\n",
    "        keys.append(topic_matrix[i].argmax())\n",
    "    return keys\n",
    "###################################################################################\n",
    "def keys_to_counts(keys):\n",
    "    '''returns a tuple of topic categories and their accompanying magnitudes for a given list of keys'''\n",
    "    count_pairs = Counter(keys).items()\n",
    "    categories = [pair[0] for pair in count_pairs]\n",
    "    counts = [pair[1] for pair in count_pairs]\n",
    "    return (categories, counts)\n",
    "\n",
    "##############################################################################\n",
    "def get_top_n_words(n, keys, document_term_matrix, count_vectorizer):\n",
    "    '''returns a list of n_topic strings, where each string contains the n most common \n",
    "        words in a predicted category, in order'''\n",
    "    top_word_indices = []\n",
    "    for topic in range(n_topics):\n",
    "        temp_vector_sum = 0\n",
    "        for i in range(len(keys)):\n",
    "            if keys[i] == topic:\n",
    "                temp_vector_sum += document_term_matrix[i]\n",
    "        temp_vector_sum = temp_vector_sum.toarray()\n",
    "        top_n_word_indices = np.flip(np.argsort(temp_vector_sum)[0][-n:],0)\n",
    "        top_word_indices.append(top_n_word_indices)   \n",
    "    top_words = []\n",
    "    for topic in top_word_indices:\n",
    "        topic_words = []\n",
    "        for index in topic:\n",
    "            temp_word_vector = np.zeros((1,document_term_matrix.shape[1]))\n",
    "            temp_word_vector[:,index] = 1\n",
    "            the_word = count_vectorizer.inverse_transform(temp_word_vector)[0][0]\n",
    "            topic_words.append(the_word.encode('ascii').decode('utf-8'))\n",
    "        top_words.append(\" \".join(topic_words))         \n",
    "    return top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seanp\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<11075x4856542 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 5480525 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:  deblois sanders final draft draft 02 13 2010 sanders final draft 02 dragon deblois sanders final final draft 02 13\n",
      "Topic 1:  what are you doing you re going to you re gon na we re going to what do you mean\n",
      "Topic 2:  pink revision 20 07 pink revision 19 07 yellow revision 20 07 real slow real slow one of the most\n",
      "Topic 3:  am am am am approaching newark station newark to camera to camera now approaching newark station station newark station next\n",
      "Topic 4:  la la la la the scene dissolve to the scene cut to scene cut to the in front of him\n",
      "********\n",
      "Topic 0:  sanders final draft 02 deblois sanders final draft draft 02 13 2010 dragon deblois sanders final final draft 02 13\n",
      "Topic 1:  what are you doing you re going to you re gon na what do you mean we re going to\n",
      "Topic 2:  red rum red rum rum red rum red you may be sure in the midst of in the world the\n",
      "Topic 3:  to camera to camera camera to camera to in and out of in the history of the heart of the\n",
      "Topic 4:  pink revision 20 07 from original script sequence sequence from original script pink revision 19 07 original script sequence from\n",
      "Topic 5:  wild hog writer first hog writer first draft first draft 06 05 writer first draft 06 we re going to\n",
      "********\n",
      "Topic 0:  dragon deblois sanders final sanders final draft 02 draft 02 13 2010 final draft 02 13 deblois sanders final draft\n",
      "Topic 1:  what are you doing you re going to you re gon na what do you mean we re going to\n",
      "Topic 2:  one of the most you may be sure in the midst of to one of the out of the country\n",
      "Topic 3:  in and out of he said he wa in the name of the heart of the to the warren commission\n",
      "Topic 4:  pink revision 20 07 from original script sequence sequence from original script pink revision 19 07 script sequence from original\n",
      "Topic 5:  hog writer first draft wild hog writer first first draft 06 05 writer first draft 06 am am am am\n",
      "Topic 6:  official green script of green script of 09 blue valentine official green valentine official green script blue revision 03 26\n",
      "********\n",
      "Topic 0:  dragon deblois sanders final final draft 02 13 sanders final draft 02 deblois sanders final draft draft 02 13 2010\n",
      "Topic 1:  what are you doing you re going to you re gon na what do you mean we re going to\n",
      "Topic 2:  one of the most you may be sure in the midst of in the world the out of the country\n",
      "Topic 3:  in and out of in the name of the heart of the of the united state he said he wa\n",
      "Topic 4:  la la la la the scene dissolve to the scene cut to scene cut to the in front of him\n",
      "Topic 5:  blue revision 03 26 revision 03 26 10 4th blue revision 03 wild hog writer first hog writer first draft\n",
      "Topic 6:  from original script sequence sequence from original script original script sequence from script sequence from original ted from original script\n",
      "Topic 7:  official green script of green script of 09 valentine official green script blue valentine official green japanese no subtitle japanese\n",
      "********\n",
      "Topic 0:  final draft 02 13 draft 02 13 2010 sanders final draft 02 deblois sanders final draft dragon deblois sanders final\n",
      "Topic 1:  what are you doing you re going to you re gon na we re going to what do you mean\n",
      "Topic 2:  in the midst of you may be sure in the world the at the end of the de belle fast\n",
      "Topic 3:  to camera to camera camera to camera to in and out of in the name of of the united state\n",
      "Topic 4:  la la la la the scene dissolve to the scene cut to scene cut to the in front of him\n",
      "Topic 5:  hog writer first draft wild hog writer first first draft 06 05 writer first draft 06 we re going to\n",
      "Topic 6:  from original script sequence sequence from original script script sequence from original original script sequence from sequence ted from original\n",
      "Topic 7:  valentine official green script official green script of blue valentine official green green script of 09 subtitle japanese no subtitle\n",
      "Topic 8:  pink revision 20 07 pink revision 19 07 yellow revision 20 07 what do you mean 07 pink revision 20\n",
      "********\n",
      "Topic 0:  draft 02 13 2010 dragon deblois sanders final deblois sanders final draft final draft 02 13 sanders final draft 02\n",
      "Topic 1:  what are you doing you re going to you re gon na we re going to what do you mean\n",
      "Topic 2:  one of the most in the midst of you may be sure out of the country to one of the\n",
      "Topic 3:  in and out of in the name of he said he wa of the united state the heart of the\n",
      "Topic 4:  la la la la the scene dissolve to the scene cut to scene cut to the in front of him\n",
      "Topic 5:  wild hog writer first hog writer first draft first draft 06 05 writer first draft 06 look at each other\n",
      "Topic 6:  get out of here you re gon na out of here you took care of the wa at that dinner\n",
      "Topic 7:  valentine official green script green script of 09 blue valentine official green official green script of pink revision 20 07\n",
      "Topic 8:  red rum red rum rum red rum red real slow real slow request acknowledged request acknowledged slow real slow real\n",
      "Topic 9:  from original script sequence sequence from original script original script sequence from script sequence from original ted from original script\n",
      "********\n",
      "Topic 0:  final draft 02 13 sanders final draft 02 deblois sanders final draft draft 02 13 2010 dragon deblois sanders final\n",
      "Topic 1:  what are you doing you re going to you re gon na we re going to what do you mean\n",
      "Topic 2:  one of the most you may be sure in the midst of in the world the out of the country\n",
      "Topic 3:  am am am am in and out of in the name of he said he wa of the united state\n",
      "Topic 4:  la la la la the scene dissolve to the scene cut to scene cut to the the scene fade out\n",
      "Topic 5:  wild hog writer first hog writer first draft first draft 06 05 writer first draft 06 we re going to\n",
      "Topic 6:  valentine official green script official green script of green script of 09 blue valentine official green no subtitle japanese no\n",
      "Topic 7:  pink revision 20 07 pink revision 19 07 get out of here yellow revision 20 07 you re gon na\n",
      "Topic 8:  cedar rapid 12 09 rapid 12 09 oh yes mr yes mr we re gon na have seen evil dead\n",
      "Topic 9:  4th blue revision 03 blue revision 03 26 revision 03 26 10 from original script sequence sequence from original script\n",
      "Topic 10:  red rum red rum rum red rum red you re looking for real slow real slow what we re doing\n",
      "********\n",
      "Topic 0:  draft 02 13 2010 sanders final draft 02 deblois sanders final draft final draft 02 13 dragon deblois sanders final\n",
      "Topic 1:  what are you doing you re going to you re gon na what do you mean we re going to\n",
      "Topic 2:  you may be sure one of the most in the midst of to one of the out of the country\n",
      "Topic 3:  in and out of in the name of the heart of the the time of the in the second floor\n",
      "Topic 4:  la la la la the scene dissolve to the scene cut to scene cut to the in front of him\n",
      "Topic 5:  hog writer first draft wild hog writer first first draft 06 05 writer first draft 06 look at each other\n",
      "Topic 6:  get out of here real slow real slow you re gon na out of here you slow real slow real\n",
      "Topic 7:  blue valentine official green green script of 09 official green script of valentine official green script japanese no subtitle japanese\n",
      "Topic 8:  revision 03 26 10 4th blue revision 03 blue revision 03 26 from original script sequence sequence from original script\n",
      "Topic 9:  thank you thank you thank you so much my name is cohen do you hear the you do have to\n",
      "Topic 10:  pink revision 20 07 cedar rapid 12 09 red rum red rum rum red rum red pink revision 19 07\n",
      "Topic 11:  you want me to to camera to camera camera to camera to we re going to for the rest of\n",
      "********\n",
      "Topic 0:  sanders final draft 02 deblois sanders final draft final draft 02 13 dragon deblois sanders final draft 02 13 2010\n",
      "Topic 1:  what are you doing you re going to you re gon na we re going to what do you mean\n",
      "Topic 2:  one of the most you may be sure to one of the in the world the out of the country\n",
      "Topic 3:  in and out of in the name of the heart of the in the second floor to the warren commission\n",
      "Topic 4:  la la la la the scene dissolve to the scene cut to scene cut to the in front of him\n",
      "Topic 5:  hog writer first draft wild hog writer first first draft 06 05 writer first draft 06 we re going to\n",
      "Topic 6:  green script of 09 valentine official green script official green script of blue valentine official green japanese no subtitle japanese\n",
      "Topic 7:  get out of here you re gon na took care of the wa at that dinner out of here you\n",
      "Topic 8:  from original script sequence sequence from original script script sequence from original original script sequence from sequence ted from original\n",
      "Topic 9:  thank you thank you to camera to camera camera to camera to real slow real slow thank you so much\n",
      "Topic 10:  you ll have to the edge of the we ll have to what you re saying nellis air force base\n",
      "Topic 11:  pink revision 20 07 cedar rapid 12 09 red rum red rum rum red rum red pink revision 19 07\n",
      "Topic 12:  blue revision 03 26 revision 03 26 10 4th blue revision 03 and the warrior three what do you mean\n",
      "********\n",
      "Topic 0:  draft 02 13 2010 final draft 02 13 sanders final draft 02 dragon deblois sanders final deblois sanders final draft\n",
      "Topic 1:  what are you doing you re going to you re gon na what do you mean we re going to\n",
      "Topic 2:  one of the most in the midst of you may be sure in the world the out of the country\n",
      "Topic 3:  in and out of in the name of the heart of the to the warren commission in the second floor\n",
      "Topic 4:  la la la la the scene dissolve to the scene cut to scene cut to the in front of him\n",
      "Topic 5:  hog writer first draft wild hog writer first first draft 06 05 writer first draft 06 look at each other\n",
      "Topic 6:  get out of here you re gon na out of here you wa at that dinner took care of the\n",
      "Topic 7:  green script of 09 valentine official green script official green script of blue valentine official green japanese no subtitle japanese\n",
      "Topic 8:  yes mr yes mr what do you mean nothing to do with evil dead ii yet for the rest of\n",
      "Topic 9:  thank you thank you thank you so much real slow real slow slow real slow real my name is cohen\n",
      "Topic 10:  you re looking for he said he wa the little deaf girl we re going to what we re doing\n",
      "Topic 11:  am am am am we ll have to request acknowledged request acknowledged she shake her head we re gon na\n",
      "Topic 12:  you want me to ha ha ha ha you re gon na we re going to it used to be\n",
      "Topic 13:  blue revision 03 26 revision 03 26 10 4th blue revision 03 pink revision 20 07 from original script sequence\n",
      "********\n",
      "Topic 0:  dragon deblois sanders final final draft 02 13 sanders final draft 02 draft 02 13 2010 deblois sanders final draft\n",
      "Topic 1:  what are you doing you re going to you re gon na we re going to what do you mean\n",
      "Topic 2:  one of the most in the midst of you may be sure in the world the out of the country\n",
      "Topic 3:  in and out of in the name of the heart of the of the united state the time of the\n",
      "Topic 4:  la la la la the scene dissolve to the scene cut to scene cut to the the scene fade out\n",
      "Topic 5:  wild hog writer first hog writer first draft first draft 06 05 writer first draft 06 we re going to\n",
      "Topic 6:  get out of here out of here you you re gon na took care of the wa at that dinner\n",
      "Topic 7:  blue valentine official green green script of 09 valentine official green script official green script of subtitle japanese no subtitle\n",
      "Topic 8:  from original script sequence sequence from original script script sequence from original original script sequence from ted from original script\n",
      "Topic 9:  cedar rapid 12 09 thank you thank you thank you so much request acknowledged request acknowledged do you hear the\n",
      "Topic 10:  red rum red rum rum red rum red we ll have to the edge of the he shake his head\n",
      "Topic 11:  pink revision 20 07 pink revision 19 07 you re looking for yellow revision 20 07 he said he wa\n",
      "Topic 12:  am am am am my name is and the front of the what do you think where are you from\n",
      "Topic 13:  that ai no crime ai no crime that crime that ai no no crime that ai you re gon na\n",
      "Topic 14:  revision 03 26 10 4th blue revision 03 blue revision 03 26 and the warrior three what are you doing\n",
      "********\n",
      "Topic 0:  sanders final draft 02 draft 02 13 2010 dragon deblois sanders final final draft 02 13 deblois sanders final draft\n",
      "Topic 1:  what are you doing you re going to you re gon na we re going to what do you mean\n",
      "Topic 2:  one of the most you may be sure in the midst of out of the country in the world the\n",
      "Topic 3:  in and out of in the name of the heart of the he said he wa in the second floor\n",
      "Topic 4:  la la la la the scene dissolve to the scene cut to scene cut to the in front of him\n",
      "Topic 5:  hog writer first draft wild hog writer first writer first draft 06 first draft 06 05 look at each other\n",
      "Topic 6:  get out of here you re gon na out of here you took care of the wa at that dinner\n",
      "Topic 7:  green script of 09 blue valentine official green official green script of valentine official green script no subtitle japanese no\n",
      "Topic 8:  yes mr yes mr for the rest of have seen evil dead nothing to do with mr yes mr yes\n",
      "Topic 9:  thank you thank you thank you so much guardian screening script 12 screening script 12 avid do you hear the\n",
      "Topic 10:  we ll have to real slow real slow the back of the you ll have to the edge of the\n",
      "Topic 11:  you re looking for we re gon na we re going to what we re doing on baby light my\n",
      "Topic 12:  revision 03 26 10 blue revision 03 26 4th blue revision 03 from original script sequence red rum red rum\n",
      "Topic 13:  my name is and where are you from is and want to you re going to name is and want\n",
      "Topic 14:  you want me to what do you mean you re gon na we re gon na we re going to\n",
      "Topic 15:  pink revision 20 07 cedar rapid 12 09 pink revision 19 07 head of mall security yellow revision 20 07\n",
      "********\n",
      "Topic 0:  sanders final draft 02 deblois sanders final draft final draft 02 13 draft 02 13 2010 dragon deblois sanders final\n",
      "Topic 1:  what are you doing you re going to you re gon na we re going to what do you mean\n",
      "Topic 2:  one of the most in the midst of you may be sure out of the country in the world the\n",
      "Topic 3:  in and out of in the name of the heart of the of the united state to the warren commission\n",
      "Topic 4:  la la la la the scene dissolve to the scene cut to scene cut to the in front of him\n",
      "Topic 5:  wild hog writer first hog writer first draft first draft 06 05 writer first draft 06 look at each other\n",
      "Topic 6:  get out of here out of here you you re gon na wa at that dinner took care of the\n",
      "Topic 7:  green script of 09 blue valentine official green valentine official green script official green script of japanese no subtitle japanese\n",
      "Topic 8:  yes mr yes mr seen evil dead ii have seen evil dead nothing to do with mr yes mr yes\n",
      "Topic 9:  thank you thank you thank you so much do you hear the you do have to my name is cohen\n",
      "Topic 10:  cedar rapid 12 09 we ll have to the edge of the you ll have to rapid 12 09 oh\n",
      "Topic 11:  pink revision 20 07 from original script sequence sequence from original script pink revision 19 07 script sequence from original\n",
      "Topic 12:  revision 03 26 10 blue revision 03 26 4th blue revision 03 and the warrior three my name is and\n",
      "Topic 13:  red rum red rum rum red rum red united state of america the sound of the will overload in minute\n",
      "Topic 14:  you know what mean you re gon na keep an eye on what do you mean to keep an eye\n",
      "Topic 15:  head of mall security request acknowledged request acknowledged in charge of this you want to and what do you mean\n",
      "Topic 16:  what do you mean you want me to we re going to you do have to camera to camera to\n",
      "********\n",
      "Topic 0:  sanders final draft 02 deblois sanders final draft draft 02 13 2010 final draft 02 13 dragon deblois sanders final\n",
      "Topic 1:  what are you doing you re going to you re gon na we re going to what do you mean\n",
      "Topic 2:  one of the most you may be sure to one of the out of the country in the world the\n",
      "Topic 3:  in and out of in the name of of the united state the heart of the the time of the\n",
      "Topic 4:  la la la la the scene dissolve to the scene cut to scene cut to the the scene fade out\n",
      "Topic 5:  hog writer first draft wild hog writer first writer first draft 06 first draft 06 05 look at each other\n",
      "Topic 6:  get out of here you re gon na took care of the out of here you wa at that dinner\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 7:  blue valentine official green green script of 09 valentine official green script official green script of no subtitle japanese no\n",
      "Topic 8:  yes mr yes mr seen evil dead ii nothing to do with mr yes mr yes what do you mean\n",
      "Topic 9:  thank you thank you thank you so much do you hear the my name is cohen you do have to\n",
      "Topic 10:  we ll have to you ll have to the edge of the he shake his head she shake her head\n",
      "Topic 11:  you re looking for we re going to what we re doing to be able to oh dear oh dear\n",
      "Topic 12:  4th blue revision 03 blue revision 03 26 revision 03 26 10 and the warrior three the side of the\n",
      "Topic 13:  am am am am camera to camera to you re gon na to camera to camera what do you mean\n",
      "Topic 14:  from original script sequence sequence from original script script sequence from original original script sequence from ted from original script\n",
      "Topic 15:  cedar rapid 12 09 red rum red rum rum red rum red will overload in minute engine will overload in\n",
      "Topic 16:  you re gon na we ve got to it going to be do give him time into the next room\n",
      "Topic 17:  pink revision 20 07 pink revision 19 07 you re gon na yellow revision 20 07 head of mall security\n",
      "********\n",
      "Topic 0:  final draft 02 13 dragon deblois sanders final sanders final draft 02 draft 02 13 2010 deblois sanders final draft\n",
      "Topic 1:  what are you doing you re going to you re gon na we re going to what do you mean\n",
      "Topic 2:  one of the most in the midst of you may be sure out of the country to one of the\n",
      "Topic 3:  in and out of in the name of the heart of the of the united state to the warren commission\n",
      "Topic 4:  la la la la the scene dissolve to the scene cut to scene cut to the the scene fade out\n",
      "Topic 5:  wild hog writer first hog writer first draft first draft 06 05 writer first draft 06 look at each other\n",
      "Topic 6:  from original script sequence sequence from original script script sequence from original original script sequence from sequence ted from original\n",
      "Topic 7:  blue valentine official green green script of 09 valentine official green script official green script of no subtitle japanese no\n",
      "Topic 8:  yes mr yes mr for the rest of mr yes mr yes what do you mean seen evil dead ii\n",
      "Topic 9:  thank you thank you thank you so much my name is cohen you do have to do you hear the\n",
      "Topic 10:  the edge of the we ll have to you ll have to he shake his head the back of the\n",
      "Topic 11:  you re looking for what we re doing we re going to re back you re you re back you\n",
      "Topic 12:  where are you from my name is and want to recruit you name is and want is and want to\n",
      "Topic 13:  real slow real slow would you like to you re gon na slow real slow real what do you mean\n",
      "Topic 14:  4th blue revision 03 revision 03 26 10 blue revision 03 26 red rum red rum rum red rum red\n",
      "Topic 15:  you want me to you re gon na pick up the phone we re going to we re gon na\n",
      "Topic 16:  am am am am we ve got to what do you want we just have to you re gon na\n",
      "Topic 17:  pink revision 20 07 guardian screening script 12 screening script 12 avid pink revision 19 07 what do you mean\n",
      "Topic 18:  no no no no camera to camera to to camera to camera request acknowledged request acknowledged what do you want\n",
      "********\n",
      "Topic 0:  deblois sanders final draft final draft 02 13 draft 02 13 2010 dragon deblois sanders final sanders final draft 02\n",
      "Topic 1:  what are you doing you re going to you re gon na we re going to what do you mean\n",
      "Topic 2:  one of the most in the midst of you may be sure in the world the out of the country\n",
      "Topic 3:  in and out of in the name of of the united state the heart of the he said he wa\n",
      "Topic 4:  la la la la the scene dissolve to the scene cut to scene cut to the the scene fade out\n",
      "Topic 5:  hog writer first draft wild hog writer first first draft 06 05 writer first draft 06 we re going to\n",
      "Topic 6:  get out of here you re gon na out of here you wa at that dinner took care of the\n",
      "Topic 7:  blue valentine official green green script of 09 valentine official green script official green script of subtitle japanese no subtitle\n",
      "Topic 8:  yes mr yes mr seen evil dead ii have seen evil dead evil dead ii yet nothing to do with\n",
      "Topic 9:  thank you thank you thank you so much my name is cohen do you hear the you do have to\n",
      "Topic 10:  red rum red rum rum red rum red we ll have to he shake his head the edge of the\n",
      "Topic 11:  you re looking for what we re doing to be able to we re going to re back you re\n",
      "Topic 12:  screening script 12 avid guardian screening script 12 ha ha ha ha the lequint dickey mining lequint dickey mining company\n",
      "Topic 13:  cedar rapid 12 09 real slow real slow slow real slow real engine will overload in rapid 12 09 oh\n",
      "Topic 14:  from original script sequence sequence from original script script sequence from original original script sequence from sequence ted from original\n",
      "Topic 15:  you want me to what do you mean ever hear that expression it because you looked hear that expression it\n",
      "Topic 16:  you know what mean am am am am no no no no you re gon na we re supposed to\n",
      "Topic 17:  4th blue revision 03 blue revision 03 26 revision 03 26 10 pink revision 20 07 pink revision 19 07\n",
      "Topic 18:  you re going to what do you want what are you doing she look at him the man in the\n",
      "Topic 19:  that ai no crime we ve got to crime that ai no no crime that ai ai no crime that\n",
      "********\n",
      "Topic 0:  sanders final draft 02 deblois sanders final draft draft 02 13 2010 dragon deblois sanders final final draft 02 13\n",
      "Topic 1:  what are you doing you re going to you re gon na we re going to what do you mean\n",
      "Topic 2:  one of the most in the midst of you may be sure to one of the in the world the\n",
      "Topic 3:  in and out of in the name of the heart of the of the united state to the warren commission\n",
      "Topic 4:  la la la la the scene dissolve to the scene cut to scene cut to the the scene fade out\n",
      "Topic 5:  wild hog writer first hog writer first draft first draft 06 05 writer first draft 06 we re going to\n",
      "Topic 6:  get out of here you re gon na out of here you wa at that dinner took care of the\n",
      "Topic 7:  blue valentine official green official green script of valentine official green script green script of 09 no subtitle japanese no\n",
      "Topic 8:  yes mr yes mr have seen evil dead seen evil dead ii mr yes mr yes for the rest of\n",
      "Topic 9:  thank you thank you thank you so much request acknowledged request acknowledged do you hear the my name is cohen\n",
      "Topic 10:  the edge of the we ll have to you ll have to he shake his head she shake her head\n",
      "Topic 11:  you re looking for what we re doing he said he wa we re going to if you re looking\n",
      "Topic 12:  ha ha ha ha the side of the by the neck until the sound of the the lequint dickey mining\n",
      "Topic 13:  from original script sequence sequence from original script script sequence from original original script sequence from ted from original script\n",
      "Topic 14:  what do you want pick up the phone you want me to the top of the up the phone and\n",
      "Topic 15:  head of mall security we ve got to penny whistle toy penny toy penny whistle toy whistle toy penny whistle\n",
      "Topic 16:  the fuck out of get the fuck out you re gon na what the hell is nellis air force base\n",
      "Topic 17:  you know what mean you ll have to you want me to we re going to what do you think\n",
      "Topic 18:  that ai no crime ai no crime that no crime that ai crime that ai no you want me to\n",
      "Topic 19:  cedar rapid 12 09 red rum red rum rum red rum red what do you mean we re gon na\n",
      "Topic 20:  revision 03 26 10 4th blue revision 03 blue revision 03 26 pink revision 20 07 pink revision 19 07\n",
      "********\n",
      "Topic 0:  draft 02 13 2010 final draft 02 13 sanders final draft 02 dragon deblois sanders final deblois sanders final draft\n",
      "Topic 1:  what are you doing you re going to you re gon na we re going to what do you mean\n",
      "Topic 2:  one of the most in the midst of you may be sure out of the country to one of the\n",
      "Topic 3:  in and out of in the name of of the united state the heart of the the time of the\n",
      "Topic 4:  la la la la the scene dissolve to the scene cut to scene cut to the the scene fade out\n",
      "Topic 5:  wild hog writer first hog writer first draft writer first draft 06 first draft 06 05 we re going to\n",
      "Topic 6:  get out of here out of here you you re gon na took care of the wa at that dinner\n",
      "Topic 7:  valentine official green script green script of 09 blue valentine official green official green script of subtitle japanese no subtitle\n",
      "Topic 8:  yes mr yes mr seen evil dead ii have seen evil dead what do you mean evil dead ii yet\n",
      "Topic 9:  thank you thank you thank you so much you do have to do you hear the my name is cohen\n",
      "Topic 10:  the edge of the we ll have to you ll have to the back of the he shake his head\n",
      "Topic 11:  blue revision 03 26 revision 03 26 10 4th blue revision 03 you re looking for and the warrior three\n",
      "Topic 12:  it thrill to be keep an eye on you re supposed to did you see the what do you mean\n",
      "Topic 13:  ha ha ha ha the sound of the the lequint dickey mining lequint dickey mining company when it come to\n",
      "Topic 14:  my name is and where are you from name is and want want to recruit you is and want to\n",
      "Topic 15:  head of mall security out of the way to be the one you to know that to thank you for\n",
      "Topic 16:  from original script sequence sequence from original script script sequence from original original script sequence from ted from original script\n",
      "Topic 17:  you want me to you do have to and that change everything it cause once you cause once you see\n",
      "Topic 18:  you re going to what do you want re going to have will overload in minute engine will overload in\n",
      "Topic 19:  pink revision 20 07 cedar rapid 12 09 red rum red rum rum red rum red pink revision 19 07\n",
      "Topic 20:  no no no no oh no no no we re gon na you know what mean you re gon na\n",
      "Topic 21:  screening script 12 avid guardian screening script 12 what are you doing what do you want am am am am\n",
      "********\n",
      "Topic 0:  sanders final draft 02 deblois sanders final draft draft 02 13 2010 dragon deblois sanders final final draft 02 13\n",
      "Topic 1:  what are you doing you re going to you re gon na we re going to what do you mean\n",
      "Topic 2:  one of the most in the midst of you may be sure out of the country to one of the\n",
      "Topic 3:  in and out of in the name of of the united state the heart of the to the warren commission\n",
      "Topic 4:  la la la la the scene dissolve to the scene cut to scene cut to the in front of him\n",
      "Topic 5:  hog writer first draft wild hog writer first first draft 06 05 writer first draft 06 look at each other\n",
      "Topic 6:  get out of here out of here you you re gon na wa at that dinner took care of the\n",
      "Topic 7:  green script of 09 blue valentine official green valentine official green script official green script of no subtitle japanese no\n",
      "Topic 8:  yes mr yes mr what do you mean nothing to do with have seen evil dead for the rest of\n",
      "Topic 9:  thank you thank you thank you so much you do have to my name is cohen do you hear the\n",
      "Topic 10:  we ll have to the edge of the he shake his head she shake her head the center of the\n",
      "Topic 11:  you re looking for what we re doing we re going to to be able to he said he wa\n",
      "Topic 12:  ha ha ha ha eleven thousand five hundred lequint dickey mining company of the big house on the auction block\n",
      "Topic 13:  what do you mean the end of the it thrill to be keep an eye on he go over to\n",
      "Topic 14:  the limit is not suppose the order to what is your name the telephone continues to telephone continues to ring\n",
      "Topic 15:  we ve got to head of mall security you might want to only one way to we just have to\n",
      "Topic 16:  pink revision 20 07 from original script sequence sequence from original script pink revision 19 07 original script sequence from\n",
      "Topic 17:  red rum red rum rum red rum red get the fuck out none of my business it none of my\n",
      "Topic 18:  you ort not to you want me to know what he doing the end of the the leader of the\n",
      "Topic 19:  guardian screening script 12 screening script 12 avid you re gon na you want me to what do you mean\n",
      "Topic 20:  you know what mean no no no no what do you want we re going to you have no idea\n",
      "Topic 21:  in the middle of we re gon na you do have to you re gon na no sir no sir\n",
      "Topic 22:  blue revision 03 26 revision 03 26 10 4th blue revision 03 cedar rapid 12 09 what are you doing\n",
      "********\n",
      "Topic 0:  deblois sanders final draft dragon deblois sanders final final draft 02 13 draft 02 13 2010 sanders final draft 02\n",
      "Topic 1:  what are you doing you re going to you re gon na we re going to what do you mean\n",
      "Topic 2:  one of the most in the midst of you may be sure out of the country in the world the\n",
      "Topic 3:  in and out of in the name of of the united state the heart of the the time of the\n",
      "Topic 4:  la la la la the scene dissolve to the scene cut to scene cut to the the scene fade out\n",
      "Topic 5:  wild hog writer first hog writer first draft writer first draft 06 first draft 06 05 look at each other\n",
      "Topic 6:  get out of here out of here you you re gon na took care of the wa at that dinner\n",
      "Topic 7:  valentine official green script blue valentine official green green script of 09 official green script of japanese no subtitle japanese\n",
      "Topic 8:  yes mr yes mr what do you mean for the rest of evil dead ii yet mr yes mr yes\n",
      "Topic 9:  thank you thank you thank you so much my name is cohen do you hear the you do have to\n",
      "Topic 10:  the edge of the we ll have to he shake his head the back of the the center of the\n",
      "Topic 11:  you re looking for what we re doing he said he wa we re going to great to see you\n",
      "Topic 12:  ha ha ha ha the sound of the the lequint dickey mining in front of the by the neck until\n",
      "Topic 13:  where are you from my name is and name is and want want to recruit you is and want to\n",
      "Topic 14:  pick up the phone up the phone and he pick up the for no particular reason to play ping pong\n",
      "Topic 15:  you want me to it ha to be it used to be because you looked at see it you re\n",
      "Topic 16:  red rum red rum rum red rum red head of mall security in charge of this we re going to\n",
      "Topic 17:  what do you mean nellis air force base have you ever had would you like to let me talk to\n",
      "Topic 18:  we re gon na you know what mean you re gon na no no no no real slow real slow\n",
      "Topic 19:  no no no no to camera to camera you re going to camera to camera to in the middle of\n",
      "Topic 20:  cedar rapid 12 09 rapid 12 09 oh end of the day rapid 12 09 you hog farm risk mitigation\n",
      "Topic 21:  what do you want get the fuck out it none of my you want me to none of my business\n",
      "Topic 22:  4th blue revision 03 blue revision 03 26 revision 03 26 10 pink revision 20 07 pink revision 19 07\n",
      "Topic 23:  guardian screening script 12 screening script 12 avid from original script sequence sequence from original script script sequence from original\n",
      "********\n",
      "Topic 0:  sanders final draft 02 draft 02 13 2010 dragon deblois sanders final final draft 02 13 deblois sanders final draft\n",
      "Topic 1:  what are you doing you re going to you re gon na we re going to what do you mean\n",
      "Topic 2:  one of the most you may be sure in the midst of out of the country to one of the\n",
      "Topic 3:  in and out of in the name of of the united state the heart of the he said he wa\n",
      "Topic 4:  la la la la the scene dissolve to the scene cut to scene cut to the the scene fade out\n",
      "Topic 5:  wild hog writer first hog writer first draft first draft 06 05 writer first draft 06 we re going to\n",
      "Topic 6:  get out of here you re gon na took care of the out of here you wa at that dinner\n",
      "Topic 7:  green script of 09 blue valentine official green official green script of valentine official green script japanese no subtitle japanese\n",
      "Topic 8:  yes mr yes mr what do you mean evil dead ii yet seen evil dead ii mr yes mr yes\n",
      "Topic 9:  thank you thank you thank you so much you do have to my name is cohen do you hear the\n",
      "Topic 10:  you ll have to we ll have to the edge of the he shake his head she shake her head\n",
      "Topic 11:  you re looking for guardian screening script 12 screening script 12 avid we re going to what we re doing\n",
      "Topic 12:  ha ha ha ha the lequint dickey mining the sound of the lequint dickey mining company on the auction block\n",
      "Topic 13:  we re going to you want me to you do have to it used to be it ha to be\n",
      "Topic 14:  look like we have four or five moment 6th mobile infantery division telephone continues to ring bug planet planet hostile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 15:  we ve got to pick up the phone we just have to there only one way ve got to think\n",
      "Topic 16:  what do you mean it thrill to be keep an eye on her against the wall to keep an eye\n",
      "Topic 17:  pink revision 20 07 pink revision 19 07 head of mall security to camera to camera yellow revision 20 07\n",
      "Topic 18:  you know what mean none of my business it none of my get the fuck out the fuck out of\n",
      "Topic 19:  we re gon na you re gon na what do you think you want me to what do you mean\n",
      "Topic 20:  cedar rapid 12 09 from original script sequence sequence from original script original script sequence from script sequence from original\n",
      "Topic 21:  red rum red rum rum red rum red and then you re then you re gone you re going to\n",
      "Topic 22:  revision 03 26 10 blue revision 03 26 4th blue revision 03 and the warrior three what are you doing\n",
      "Topic 23:  what do you mean what the matter with you re going to we re going to the side of the\n",
      "Topic 24:  no no no no that ai no crime no crime that ai crime that ai no ai no crime that\n",
      "********\n",
      "Topic 0:  deblois sanders final draft sanders final draft 02 dragon deblois sanders final draft 02 13 2010 final draft 02 13\n",
      "Topic 1:  what are you doing you re going to you re gon na we re going to what do you mean\n",
      "Topic 2:  one of the most you may be sure in the midst of to one of the out of the country\n",
      "Topic 3:  in and out of in the name of the heart of the to the warren commission and this is not\n",
      "Topic 4:  la la la la the scene dissolve to the scene cut to scene cut to the the scene fade out\n",
      "Topic 5:  wild hog writer first hog writer first draft first draft 06 05 writer first draft 06 we re going to\n",
      "Topic 6:  get out of here you re gon na took care of the out of here you wa at that dinner\n",
      "Topic 7:  valentine official green script green script of 09 blue valentine official green official green script of subtitle japanese no subtitle\n",
      "Topic 8:  real slow real slow slow real slow real yes mr yes mr mr yes mr yes evil dead ii yet\n",
      "Topic 9:  thank you thank you thank you so much you do have to do you hear the my name is cohen\n",
      "Topic 10:  you ll have to we ll have to the edge of the the center of the he shake his head\n",
      "Topic 11:  you re looking for what we re doing to be able to we re going to he said he wa\n",
      "Topic 12:  ha ha ha ha the side of the the sound of the lequint dickey mining company in front of the\n",
      "Topic 13:  where are you from my name is and name is and want want to recruit you is and want to\n",
      "Topic 14:  you want me to it ha to be it used to be ever hear that expression you looked at it\n",
      "Topic 15:  what do you mean keep an eye on you re gon na it thrill to be did you see the\n",
      "Topic 16:  pick up the phone up the phone and sex with my wife hang up the phone to have sex with\n",
      "Topic 17:  cedar rapid 12 09 that ai no crime ai no crime that crime that ai no no crime that ai\n",
      "Topic 18:  pink revision 20 07 pink revision 19 07 yellow revision 20 07 none of my business 07 pink revision 20\n",
      "Topic 19:  sequence from original script script sequence from original from original script sequence original script sequence from am am am am\n",
      "Topic 20:  we re gon na you re gon na what do you mean we re going to united state of america\n",
      "Topic 21:  blue revision 03 26 4th blue revision 03 revision 03 26 10 red rum red rum rum red rum red\n",
      "Topic 22:  we re going to what do you mean have you ever had you re looking for what do you want\n",
      "Topic 23:  guardian screening script 12 screening script 12 avid ted from original script sequence ted from original script sequence ted from\n",
      "Topic 24:  what do you mean no no no no what the matter with what do you want the building and loan\n",
      "Topic 25:  no no no no what are you doing are you talking about what are you talking you want me to\n",
      "********\n",
      "Topic 0:  dragon deblois sanders final deblois sanders final draft sanders final draft 02 draft 02 13 2010 final draft 02 13\n",
      "Topic 1:  what are you doing you re going to you re gon na we re going to what do you mean\n",
      "Topic 2:  one of the most you may be sure in the midst of to one of the out of the country\n",
      "Topic 3:  in and out of in the name of of the united state the heart of the in the second floor\n",
      "Topic 4:  la la la la the scene dissolve to the scene cut to scene cut to the the scene fade out\n",
      "Topic 5:  hog writer first draft wild hog writer first writer first draft 06 first draft 06 05 look at each other\n",
      "Topic 6:  get out of here you re gon na out of here you took care of the wa at that dinner\n",
      "Topic 7:  green script of 09 valentine official green script blue valentine official green official green script of subtitle japanese no subtitle\n",
      "Topic 8:  yes mr yes mr for the rest of mr yes mr yes nothing to do with evil dead ii yet\n",
      "Topic 9:  thank you thank you thank you so much you do have to do you hear the my name is cohen\n",
      "Topic 10:  you ll have to the edge of the we ll have to he shake his head the bottom of the\n",
      "Topic 11:  you re looking for what we re doing he said he wa we re going to if you re looking\n",
      "Topic 12:  ha ha ha ha lequint dickey mining company the sound of the the lequint dickey mining of the big house\n",
      "Topic 13:  what do you mean keep an eye on it thrill to be to get into his think he good looking\n",
      "Topic 14:  my name is and where are you from name is and want is and want to want to recruit you\n",
      "Topic 15:  head of mall security you re telling me out of the way you to know that to be the one\n",
      "Topic 16:  pick up the phone the top of the he pick up the up the phone and and every one of\n",
      "Topic 17:  you want me to it used to be it ha to be change every time you trying to force me\n",
      "Topic 18:  you know what mean get the fuck out the fuck out of we re supposed to you re gon na\n",
      "Topic 19:  out of the family what did you think know what he doing and they dock ya she gon na be\n",
      "Topic 20:  from original script sequence sequence from original script script sequence from original original script sequence from ted from original script\n",
      "Topic 21:  screening script 12 avid guardian screening script 12 what do you mean what are you doing what do you want\n",
      "Topic 22:  red rum red rum rum red rum red we re gon na you re gon na we ve got to\n",
      "Topic 23:  revision 03 26 10 4th blue revision 03 blue revision 03 26 united state of america and the warrior three\n",
      "Topic 24:  cedar rapid 12 09 pink revision 20 07 pink revision 19 07 to talk to you you re going to\n",
      "Topic 25:  that ai no crime crime that ai no no crime that ai ai no crime that you re gon na\n",
      "Topic 26:  what are you doing no no no no am am am am what do you mean are you talking about\n",
      "********\n",
      "Topic 0:  sanders final draft 02 draft 02 13 2010 deblois sanders final draft final draft 02 13 dragon deblois sanders final\n",
      "Topic 1:  what are you doing you re going to you re gon na we re going to what do you mean\n",
      "Topic 2:  one of the most you may be sure to one of the in the world the in the midst of\n",
      "Topic 3:  in and out of in the name of of the united state the heart of the the time of the\n",
      "Topic 4:  la la la la the scene dissolve to the scene cut to scene cut to the the scene fade out\n",
      "Topic 5:  hog writer first draft wild hog writer first writer first draft 06 first draft 06 05 we re going to\n",
      "Topic 6:  get out of here you re gon na out of here you wa at that dinner took care of the\n",
      "Topic 7:  blue valentine official green green script of 09 official green script of valentine official green script subtitle japanese no subtitle\n",
      "Topic 8:  yes mr yes mr have seen evil dead for the rest of what do you mean mr yes mr yes\n",
      "Topic 9:  thank you thank you thank you so much you do have to my name is cohen do you hear the\n",
      "Topic 10:  the edge of the we ll have to he shake his head you ll have to the center of the\n",
      "Topic 11:  you re looking for what we re doing he said he wa we re going to re back you re\n",
      "Topic 12:  ha ha ha ha the side of the in front of the lequint dickey mining company the lequint dickey mining\n",
      "Topic 13:  where are you from my name is and want to recruit you if we re going first order of business\n",
      "Topic 14:  keep an eye on you re supposed to did you see the it thrill to be what do you mean\n",
      "Topic 15:  cedar rapid 12 09 no no no no rapid 12 09 oh rapid 12 09 you rapid 12 09 no\n",
      "Topic 16:  you want me to it ha to be we re going to looked at it cause got ta do first\n",
      "Topic 17:  take look at me st john the divine look at me now there he said now you familiar with the\n",
      "Topic 18:  head of mall security you re gon na it none of my get out of here none of my business\n",
      "Topic 19:  am am am am you know what mean will overload in minute engine will overload in you re you re\n",
      "Topic 20:  pink revision 20 07 pink revision 19 07 no no no no camera to camera to to camera to camera\n",
      "Topic 21:  no no no no what are you doing what are you talking arctic warrior arctic warrior are you talking about\n",
      "Topic 22:  blue revision 03 26 4th blue revision 03 revision 03 26 10 and the warrior three what are you doing\n",
      "Topic 23:  we re gon na united state of america the united state of it gon na be come on baby light\n",
      "Topic 24:  from original script sequence sequence from original script script sequence from original original script sequence from sequence ted from original\n",
      "Topic 25:  that ai no crime crime that ai no ai no crime that no crime that ai do the time warp\n",
      "Topic 26:  what do you mean what are you doing penny whistle toy penny toy penny whistle toy whistle toy penny whistle\n",
      "Topic 27:  guardian screening script 12 screening script 12 avid red rum red rum rum red rum red what are you doing\n",
      "********\n",
      "Topic 0:  sanders final draft 02 deblois sanders final draft final draft 02 13 draft 02 13 2010 dragon deblois sanders final\n",
      "Topic 1:  what are you doing you re going to you re gon na we re going to what do you mean\n",
      "Topic 2:  one of the most you may be sure out of the country in the world the in the midst of\n",
      "Topic 3:  in and out of in the name of the heart of the of the united state the time of the\n",
      "Topic 4:  la la la la the scene dissolve to the scene cut to scene cut to the the scene fade out\n",
      "Topic 5:  wild hog writer first hog writer first draft first draft 06 05 writer first draft 06 we re going to\n",
      "Topic 6:  get out of here you re gon na out of here you took care of the wa at that dinner\n",
      "Topic 7:  valentine official green script blue valentine official green green script of 09 official green script of no subtitle japanese no\n",
      "Topic 8:  yes mr yes mr what do you mean evil dead ii yet for the rest of seen evil dead ii\n",
      "Topic 9:  thank you thank you thank you so much my name is cohen you do have to do you hear the\n",
      "Topic 10:  you re looking for we re going to what we re doing you re back you if you re looking\n",
      "Topic 11:  we ll have to the edge of the the center of the stop the pain stop the pain stop the\n",
      "Topic 12:  ha ha ha ha the side of the lequint dickey mining company request acknowledged request acknowledged by the neck until\n",
      "Topic 13:  keep an eye on did you see the what do you mean it thrill to be you re supposed to\n",
      "Topic 14:  you want me to it ha to be you said it wa it used to be want me to do\n",
      "Topic 15:  we ve got to my name is and and if he doe there only one way ve got to think\n",
      "Topic 16:  pick up the phone hang up the phone the scene cut to the door of the get to new york\n",
      "Topic 17:  am am am am head of mall security out of the way to thank you for in charge of this\n",
      "Topic 18:  get the fuck out none of my business the fuck out of it none of my got ta do somethin\n",
      "Topic 19:  guardian screening script 12 screening script 12 avid know what he doing and they dock ya is there anything you\n",
      "Topic 20:  you know what mean arctic warrior arctic warrior you re you re warrior this is united re not gon na\n",
      "Topic 21:  when you re done get off my lawn what do you want what would you do look out the window\n",
      "Topic 22:  united state of america the united state of we re gon na on baby light my come on baby light\n",
      "Topic 23:  screening script 12 avid guardian screening script 12 and then you re what do you think what do you mean\n",
      "Topic 24:  to camera to camera camera to camera to will overload in minute out of the room you re gon na\n",
      "Topic 25:  4th blue revision 03 blue revision 03 26 revision 03 26 10 cedar rapid 12 09 from original script sequence\n",
      "Topic 26:  in front of him real slow real slow what do you mean on the other end the end of the\n",
      "Topic 27:  pink revision 20 07 red rum red rum rum red rum red pink revision 19 07 we re gon na\n",
      "Topic 28:  you re gon na what do you want you want me to we re gon na get out of here\n",
      "********\n",
      "Topic 0:  draft 02 13 2010 final draft 02 13 deblois sanders final draft dragon deblois sanders final sanders final draft 02\n",
      "Topic 1:  what are you doing you re going to you re gon na we re going to what do you mean\n",
      "Topic 2:  one of the most you may be sure in the midst of in the world the out of the country\n",
      "Topic 3:  in and out of in the name of the heart of the of the united state and this is not\n",
      "Topic 4:  la la la la the scene dissolve to the scene cut to scene cut to the the scene fade out\n",
      "Topic 5:  hog writer first draft wild hog writer first writer first draft 06 first draft 06 05 we re going to\n",
      "Topic 6:  get out of here you re gon na took care of the wa at that dinner out of here you\n",
      "Topic 7:  valentine official green script blue valentine official green green script of 09 official green script of subtitle japanese no subtitle\n",
      "Topic 8:  yes mr yes mr mr yes mr yes for the rest of what do you mean seen evil dead ii\n",
      "Topic 9:  thank you thank you thank you so much do you hear the you do have to my name is cohen\n",
      "Topic 10:  the edge of the we ll have to he shake his head you ll have to nellis air force base\n",
      "Topic 11:  you re looking for he said he wa we re going to what we re doing re back you re\n",
      "Topic 12:  ha ha ha ha the lequint dickey mining the sound of the lequint dickey mining company eleven thousand five hundred\n",
      "Topic 13:  my name is and where are you from is and want to want to recruit you name is and want\n",
      "Topic 14:  keep an eye on you re gon na what do you mean it thrill to be go over to the\n",
      "Topic 15:  the top of the pick up the phone hang up the phone he pick up the up the phone and\n",
      "Topic 16:  head of mall security out of the way you to know that to thank you for in charge of this\n",
      "Topic 17:  you want me to at it cause once do you want me see it you re change every time you\n",
      "Topic 18:  and they dock ya know what he doing you punch in at you ll have to we all know what\n",
      "Topic 19:  you know what mean real slow real slow you re you re slow real slow real they re they re\n",
      "Topic 20:  what the fuck are the fuck are you get the fuck out fuck fuck fuck fuck the fuck out of\n",
      "Topic 21:  pink revision 20 07 pink revision 19 07 yellow revision 20 07 arctic warrior arctic warrior home in six month\n",
      "Topic 22:  what do you want are you talking about what are you talking do it do it do want to know\n",
      "Topic 23:  that ai no crime crime that ai no no crime that ai ai no crime that united state of america\n",
      "Topic 24:  revision 03 26 10 blue revision 03 26 4th blue revision 03 and the warrior three what are you doing\n",
      "Topic 25:  what the matter with we ve got to am am am am the matter with you what do you mean\n",
      "Topic 26:  cedar rapid 12 09 we re gon na do know how to are we gon na sex with my wife\n",
      "Topic 27:  guardian screening script 12 screening script 12 avid from original script sequence red rum red rum rum red rum red\n",
      "Topic 28:  no no no no what are you doing what do you mean are you talking about what are you talking\n",
      "Topic 29:  no no no no you re gon na request acknowledged request acknowledged the rest of the you re going to\n",
      "********\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(ngram_range = (4,4))\n",
    "text_sample = char_series.as_matrix()\n",
    "# so we essenatially create our document_term matrix.... \n",
    "#display(small_text_sample)\n",
    "document_term_matrix = count_vectorizer.fit_transform(text_sample)\n",
    "display(document_term_matrix)\n",
    "for i in range(5,31):\n",
    "    n_topics = i\n",
    "\n",
    "    lsa_model = TruncatedSVD(n_components=n_topics)\n",
    "    lsa_topic_matrix = lsa_model.fit_transform(document_term_matrix)\n",
    "   # display(lsa_topic_matrix)\n",
    "\n",
    "    lsa_keys = get_keys(lsa_topic_matrix)\n",
    "    lsa_categories, lsa_counts = keys_to_counts(lsa_keys)\n",
    "\n",
    "    top_n_words_lsa = get_top_n_words(5, lsa_keys, document_term_matrix, count_vectorizer)\n",
    "\n",
    "    for i in range(len(top_n_words_lsa)):\n",
    "        print(\"Topic {}: \".format(i), top_n_words_lsa[i])\n",
    "    print('********')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will do LSA with tokenization instead of lemmatization, just so that the outputted words make more sense!\n",
    "# Hopefully there's a way to do this with sequences of 2-3 words, because some of these are gibberish.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'char_series' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-710fa0bcef08>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mchar_series\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'char_series' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original but with just tokenization, no bigrams\n",
    "mov_dict = {}\n",
    "char_series = pd.Series()\n",
    "for fil in json_files:\n",
    "    # read the json file from the directory \n",
    "    with open(wd+ '\\\\diag_jsons\\\\'+fil, \"r\") as read_file:\n",
    "        data = json.load(read_file)\n",
    "        \n",
    "    mov_dict[fil.replace('.json','')] = data\n",
    "    # now that we have done this \n",
    "    # we may as well start loading up our pandas series with the character texts.\n",
    "    # so we will start iterating through each character in the new json file \n",
    "    # added to our dictionary and make each character a new iterm in our series \n",
    "    # which will be indexed by the title of the script/movie and the character's name \n",
    "    # with four '_'s between them \n",
    "    # so we can do easy retrieval of which character was from which movie :)\n",
    "    mov_chars = mov_dict[fil.replace('.json','')]['dialogues'].keys()\n",
    "    for char in mov_dict[fil.replace('.json','')]['dialogues'].keys():\n",
    "        #char_series.at[char+'____'+fil.replace('.json','')] = mov_dict[fil.replace('.json','')]['dialogues'][char]\n",
    "        char_series.at[char+'____'+fil.replace('.json','')] = re.sub(\"\\\\n\\[\\d+\\]\",'',mov_dict[fil.replace('.json','')]['dialogues'][char])\n",
    "        # until I think of a better way of replacing the names, \n",
    "        # I have to make the entire text lowercase \n",
    "        # so that way I get all instances of the character names being used.... \n",
    "        char_series.at[char+'____'+fil.replace('.json','')] = char_series.at[char+'____'+fil.replace('.json','')].lower()\n",
    "        # now we want to remove the names of the characters, because those seemed to be too important in creating clusters\n",
    "        char_series.at[char+'____'+fil.replace('.json','')] = remove_multiple_strings(char_series.at[char+'____'+fil.replace('.json','')],mov_chars,low=True)\n",
    "        \n",
    "        \n",
    "        tok_chars = sentence_tokenizer.tokenize(char_series.at[char+'____'+fil.replace('.json','')])\n",
    "            # so... we need to do this for every sentence.... YAY.... \n",
    "        lemz = ''\n",
    "        # I think I need to get rid of punctuation unless lemmatization gets rid of it \n",
    "        for sentence in tok_chars:\n",
    "            toks = treebank_tokenizer.tokenize(sentence)\n",
    "            #lemmatized_words = [wordnet_lemmatizer.lemmatize(token) for token in toks]\n",
    "    \n",
    "            lemz += \" \".join(toks) + ' '\n",
    "        char_series.at[char+'____'+fil.replace('.json','')] = lemz\n",
    "        # we get rid of those pesky \\n[number] patterns that denote a line change \n",
    "        # which I suppose I can add back IN as a second column if necessary , but that's just going to be unnecessary for the clustering!\n",
    "\n",
    "# I'm guessing that every thime there is a new line it will go with that kind of scheme \n",
    "# so I suppose my job will be to remove those bois if they still occur after tokenization.... hoping they just dissapear, lmao !!!\n",
    "\n",
    "# also, very nice, I can iterate over the ['dialogues.keys()']\n",
    "# this is VERY fun!!!! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is where I only keep verbs adverbs and adjectives\n",
    "# I will also get rid of any remaining stopwords. \n",
    "mov_dict = {}\n",
    "char_series = pd.Series()\n",
    "for fil in json_files:\n",
    "    # read the json file from the directory \n",
    "    with open(wd+ '\\\\diag_jsons\\\\'+fil, \"r\") as read_file:\n",
    "        data = json.load(read_file)\n",
    "        \n",
    "    mov_dict[fil.replace('.json','')] = data\n",
    "    # now that we have done this \n",
    "    # we may as well start loading up our pandas series with the character texts.\n",
    "    # so we will start iterating through each character in the new json file \n",
    "    # added to our dictionary and make each character a new iterm in our series \n",
    "    # which will be indexed by the title of the script/movie and the character's name \n",
    "    # with four '_'s between them \n",
    "    # so we can do easy retrieval of which character was from which movie :)\n",
    "    mov_chars = mov_dict[fil.replace('.json','')]['dialogues'].keys()\n",
    "    for char in mov_dict[fil.replace('.json','')]['dialogues'].keys():\n",
    "        #char_series.at[char+'____'+fil.replace('.json','')] = mov_dict[fil.replace('.json','')]['dialogues'][char]\n",
    "        char_series.at[char+'____'+fil.replace('.json','')] = re.sub(\"\\\\n\\[\\d+\\]\",'',mov_dict[fil.replace('.json','')]['dialogues'][char])\n",
    "        # until I think of a better way of replacing the names, \n",
    "        # I have to make the entire text lowercase \n",
    "        # so that way I get all instances of the character names being used.... \n",
    "        char_series.at[char+'____'+fil.replace('.json','')] = char_series.at[char+'____'+fil.replace('.json','')].lower()\n",
    "        # now we want to remove the names of the characters, because those seemed to be too important in creating clusters\n",
    "        char_series.at[char+'____'+fil.replace('.json','')] = remove_multiple_strings(char_series.at[char+'____'+fil.replace('.json','')],mov_chars,low=True)\n",
    "        \n",
    "        \n",
    "        tok_chars = sentence_tokenizer.tokenize(char_series.at[char+'____'+fil.replace('.json','')])\n",
    "            # so... we need to do this for every sentence.... YAY.... \n",
    "        lemz = ''\n",
    "        # I think I need to get rid of punctuation unless lemmatization gets rid of it \n",
    "        for sentence in tok_chars:\n",
    "            toks = treebank_tokenizer.tokenize(sentence)\n",
    "            #lemmatized_words = [wordnet_lemmatizer.lemmatize(token) for token in toks]\n",
    "    \n",
    "            lemz += \" \".join(toks) + ' '\n",
    "            lemz = [w for w in treebank_tokenizer.tokenize(lemz) if w not  in stpwds]\n",
    "            #print(lemz)\n",
    "            lemz =  \" \".join(lemz)\n",
    "            lemz = [str(tok) for tok in nlp(lemz) if tok.pos_ in ['NOUN','VERB','ADV','ADJ'] ]\n",
    "\n",
    "            lemz =  \" \".join(lemz) + ' '\n",
    "        char_series.at[char+'____'+fil.replace('.json','')] = lemz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seanp\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<11075x10000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 2789943 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:  youhhh ihhh hhh ithhh hhhi dohhhn thathhh mehhh hhhyou ll hhhand ihhhdo whathhh wehhh ve nohhh inhhhthe herehhh hehhh therehhh\n",
      "Topic 1:  hhh hhhthe hhhand hhhi uhhh ofhhhthe inhhhthe youhhh ithhh hhhyou hhhwe tohhhthe herehhh onhhhthe hhha mr hhhu hhhhe ihhh thathhh\n",
      "Topic 2:  ofhhhthe inhhhthe tohhhthe onhhhthe hhhand hhhthe athhhthe ithhh himhhh intohhhthe fromhhhthe thehhhdoor hhh andhhhthe hhha outhhhof hhhhe uphhh hehhh doorhhh\n",
      "Topic 3:  hhhand hhhi ofhhhthe hhhthe inhhhthe hhhbut mehhh ihhhwas tohhhthe ithhhwas ihhham hhh andhhhthe ihhhhad hhhhe tohhhbe ithhh hhhas thathhhi forhhhthe\n",
      "Topic 4:  yeahhhh ithhh thathhh heyhhh righthhh whathhh uphhh lethhh hehhh dohhhn ohhhh gohhh wehhh onhhh shhhgo hhhwe shhhthe gonhhhna hhhyeah hhhwhat\n",
      "Topic 5:  youhhh hhhyou rumhhh youhhhare hhhand youhhhknow thathhh wellhhh manhhh ofhhhthe tohhhyou timeshhh athhhall thishhh ithhhis dohhhn youhhhhave whathhh youhhhready thankhhhyou\n",
      "Topic 6:  aihhhn thhhno thathhhai crimehhh ihhh gonhhhna manhhh lethhh dohhhthe shhhdo thehhhtime againhhh therehhh righthhh outhhh hhhman mhhhgon fuckinhhh mhhha shithhh\n",
      "Topic 7:  hhh6 revisionhhh 07 20 pinkhhhrevision hhhpink 19 hhhwhite 29 10 15 17 warninghhh 95 14 12 wehhhhave sohhh 92 nothhha\n",
      "Topic 8:  dohhhn ihhhdo mehhh ihhham amhhh thhhknow thhhwant knowhhh wanthhhto yeshhh dohhhyou youhhhhave arehhhyou thathhh himhhh youhhhdo themhhh pleasehhh tohhhgo hhhplease\n",
      "Topic 9:  sirhhh hhhsir wehhh ve hhhwe mr killhhh wouldhhhyou youhhhlike likehhhan inhhhthe feethhh ushhh hhhmr theyhhh thehhhstory ofhhhus abouthhhthe ithhh brownhhh\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words='english', max_features=10000)\n",
    "text_sample = char_series.as_matrix()\n",
    "# so we essenatially create our document_term matrix.... \n",
    "#display(small_text_sample)\n",
    "document_term_matrix = count_vectorizer.fit_transform(text_sample)\n",
    "display(document_term_matrix)\n",
    "\n",
    "n_topics = i\n",
    "\n",
    "lsa_model = TruncatedSVD(n_components=n_topics)\n",
    "lsa_topic_matrix = lsa_model.fit_transform(document_term_matrix)\n",
    "   # display(lsa_topic_matrix)\n",
    "\n",
    "lsa_keys = get_keys(lsa_topic_matrix)\n",
    "lsa_categories, lsa_counts = keys_to_counts(lsa_keys)\n",
    "\n",
    "top_n_words_lsa = get_top_n_words(20, lsa_keys, document_term_matrix, count_vectorizer)\n",
    "\n",
    "for i in range(len(top_n_words_lsa)):\n",
    "    print(\"Topic {}: \".format(i), top_n_words_lsa[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THERE IS A graph algo called infomap... code available...all you provide it is a graph????? \n",
    "\n",
    "# i.e. character1 movie1 compared to character 7 movie 3... put the similarity in there.... \n",
    "\n",
    "# benefit of info map is it will just see it as 1 big grph and builds communities inside of graph... \n",
    "\n",
    "\n",
    "# pici villain from jaems bond movie... pick departed.. if you find bond character 1 and \n",
    "\n",
    "# we can remove stopwords... before \n",
    "\n",
    "# we can go oline and downlaoad it.... \n",
    "\n",
    "# mapequation.org.. it's a C++ implementation..... so we put in the graph..... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dictionary where each key will be the movie and each value will contain all of the \n",
    "mov_dict = {}\n",
    "char_series = pd.Series()\n",
    "for fil in json_files:\n",
    "    # read the json file from the directory \n",
    "    with open(wd+ '\\\\diag_jsons\\\\'+fil, \"r\") as read_file:\n",
    "        data = json.load(read_file)\n",
    "        \n",
    "    mov_dict[fil.replace('.json','')] = data\n",
    "    # now that we have done this \n",
    "    # we may as well start loading up our pandas series with the character texts.\n",
    "    # so we will start iterating through each character in the new json file \n",
    "    # added to our dictionary and make each character a new iterm in our series \n",
    "    # which will be indexed by the title of the script/movie and the character's name \n",
    "    # with four '_'s between them \n",
    "    # so we can do easy retrieval of which character was from which movie :)\n",
    "    mov_chars = mov_dict[fil.replace('.json','')]['dialogues'].keys()\n",
    "    for char in mov_dict[fil.replace('.json','')]['dialogues'].keys():\n",
    "        #char_series.at[char+'____'+fil.replace('.json','')] = mov_dict[fil.replace('.json','')]['dialogues'][char]\n",
    "        char_series.at[char+'____'+fil.replace('.json','')] = re.sub(\"\\\\n\\[\\d+\\]\",'',mov_dict[fil.replace('.json','')]['dialogues'][char])\n",
    "        # until I think of a better way of replacing the names, \n",
    "        # I have to make the entire text lowercase \n",
    "        # so that way I get all instances of the character names being used.... \n",
    "        char_series.at[char+'____'+fil.replace('.json','')] = char_series.at[char+'____'+fil.replace('.json','')].lower()\n",
    "        # now we want to remove the names of the characters, because those seemed to be too important in creating clusters\n",
    "        char_series.at[char+'____'+fil.replace('.json','')] = remove_multiple_strings(char_series.at[char+'____'+fil.replace('.json','')],mov_chars,low=True)\n",
    "        \n",
    "        \n",
    "        tok_chars = sentence_tokenizer.tokenize(char_series.at[char+'____'+fil.replace('.json','')])\n",
    "            # so... we need to do this for every sentence.... YAY.... \n",
    "        lemz = ''\n",
    "        # I think I need to get rid of punctuation unless lemmatization gets rid of it \n",
    "        for sentence in tok_chars:\n",
    "            toks = treebank_tokenizer.tokenize(sentence)\n",
    "            bigrams = nltk.bigrams(toks)\n",
    "            bgs = [\"_\".join(gram) for gram in bigrams ]\n",
    "                \n",
    "                    \n",
    "            #lemmatized_words = [wordnet_lemmatizer.lemmatize(token) for token in toks]\n",
    "    \n",
    "            lemz += \" \".join(bgs) + ' '\n",
    "        char_series.at[char+'____'+fil.replace('.json','')] = lemz\n",
    "        # we get rid of those pesky \\n[number] patterns that denote a line change \n",
    "        # which I suppose I can add back IN as a second column if necessary , but that's just going to be unnecessary for the clustering!\n",
    "\n",
    "# I'm guessing that every thime there is a new line it will go with that kind of scheme \n",
    "# so I suppose my job will be to remove those bois if they still occur after tokenization.... hoping they just dissapear, lmao !!!\n",
    "\n",
    "# also, very nice, I can iterate over the ['dialogues.keys()']\n",
    "# this is VERY fun!!!! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<11075x10000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1406463 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "document_term_matrix = count_vectorizer.fit_transform(text_sample)\n",
    "display(document_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seanp\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<11075x844605 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 5112881 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:  you_ i_ it_ _i re do_n that_ me_ _you ll _and i_do what_ we_ ve no_ in_the here_ he_ there_\n",
      "Topic 1:  of_the in_the _the to_the _and on_the at_the him_ it_ into_the from_the _a _he and_the out_of the_door up_ he_ her_ them_\n",
      "Topic 2:  _and _i of_the _the in_the _6 revision_ 07 _but 20 me_ to_the i_am i_was and_the pink_revision it_ it_was yes_ _a\n",
      "Topic 3:  yeah_ rum_ red_rum _you hey_ ai_n right_ it_ that_ai let_ t_no no_crime that_ crime_ what_ you_know mack_ again_ on_ here_\n",
      "Topic 4:  you_ _you re mr you_are times_ all_times gentlemen_ at_all sir_ ready_ this_ order_ here_ you_ready thank_you mr_vaughn re_a _let please_\n",
      "********\n",
      "Topic 0:  you_ i_ it_ _i re do_n that_ me_ _you ll _and i_do what_ we_ ve no_ in_the here_ he_ there_\n",
      "Topic 1:  of_the in_the _the to_the _and on_the at_the him_ it_ into_the _a from_the _he and_the out_of the_door up_ he_ her_ them_\n",
      "Topic 2:  _and _i of_the _the in_the _6 07 revision_ _but 20 me_ to_the i_am i_was pink_revision and_the it_ it_was _a yes_\n",
      "Topic 3:  yeah_ red_rum rum_ _you hey_ it_ that_ right_ what_ you_know mack_ okay_ on_ oh_ let_ _yeah here_ _mack man_ know_\n",
      "Topic 4:  you_ _you re mr you_are at_all all_times times_ gentlemen_ sir_ this_ order_ ready_ you_ready here_ _let re_a thank_you mr_vaughn please_\n",
      "Topic 5:  ai_n t_no that_ai no_crime crime_ gon_na fuckin_ man_ i_ s_do right_ do_the again_ the_time let_ time_warp warp_again there_ me_ shoo\n",
      "********\n",
      "Topic 0:  you_ i_ it_ _i re do_n that_ me_ _you ll _and i_do what_ we_ ve no_ in_the here_ he_ there_\n",
      "Topic 1:  of_the in_the _the to_the _and on_the at_the him_ it_ into_the _a from_the _he and_the out_of the_door up_ he_ her_ them_\n",
      "Topic 2:  _and _i of_the _the in_the _6 07 revision_ _but 20 me_ to_the i_was and_the pink_revision it_ it_was i_am _a yes_\n",
      "Topic 3:  yeah_ _you hey_ it_ right_ that_ what_ you_know mack_ okay_ here_ let_ _yeah on_ oh_ _mack man_ know_ up_ come_on\n",
      "Topic 4:  you_ _you re mr you_are all_times gentlemen_ times_ at_all sir_ order_ here_ you_ready ready_ this_ re_a go_back thank_you please_ mr_vaughn\n",
      "Topic 5:  ai_n t_no that_ai no_crime crime_ gon_na fuckin_ man_ i_ let_ warp_again right_ s_do the_time do_the again_ time_warp there_ you_got boom\n",
      "Topic 6:  do_n i_do red_rum rum_ me_ i_am am_ t_know want_to you_do t_want do_you that_ yes_ you_have them_ here_ know_ mr are_you\n",
      "********\n",
      "Topic 0:  you_ i_ it_ _i re do_n that_ me_ _you ll _and i_do what_ we_ ve no_ in_the here_ he_ there_\n",
      "Topic 1:  of_the in_the _the to_the _and on_the at_the him_ it_ into_the _a from_the _he and_the out_of the_door up_ he_ her_ them_\n",
      "Topic 2:  _and _i of_the _the in_the _but me_ to_the i_was and_the it_ it_was i_am _a yes_ _as _in _he _we _it\n",
      "Topic 3:  yeah_ _you hey_ it_ that_ right_ what_ you_know mack_ okay_ here_ oh_ let_ on_ _yeah _mack man_ up_ know_ do_n\n",
      "Topic 4:  you_ _you re mr you_are all_times gentlemen_ times_ at_all sir_ order_ here_ you_ready ready_ this_ re_a go_back thank_you please_ mr_vaughn\n",
      "Topic 5:  ai_n t_no no_crime that_ai gon_na crime_ fuckin_ man_ i_ warp_again time_warp s_do let_ the_time do_the right_ again_ there_ shoo shit_\n",
      "Topic 6:  do_n i_do red_rum rum_ me_ i_am am_ t_know t_want want_to you_do do_you acknowledged_ yes_ request_acknowledged that_ mr them_ you_have here_\n",
      "Topic 7:  _6 07 revision_ 20 pink_revision _pink white_revision 19 _white yellow_revision 10 29 15 _yellow 17 14 07_77 klingon_cruisers 07_90 _bearing\n",
      "********\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-203a05617bc0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mlsa_categories\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlsa_counts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeys_to_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlsa_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mtop_n_words_lsa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_top_n_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlsa_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocument_term_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount_vectorizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtop_n_words_lsa\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-32-214ea340b518>\u001b[0m in \u001b[0;36mget_top_n_words\u001b[1;34m(n, keys, document_term_matrix, count_vectorizer)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mtopic\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m                 \u001b[0mtemp_vector_sum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mdocument_term_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0mtemp_vector_sum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemp_vector_sum\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mtop_n_word_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_vector_sum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m__add__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    415\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"inconsistent shapes\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 417\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_sparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    418\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misdense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m             \u001b[0mother\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbroadcast_to\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36m_add_sparse\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_add_sparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 350\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_binopt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_plus_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_sub_sparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36m_binopt\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   1172\u001b[0m            \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1173\u001b[0m            \u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1174\u001b[1;33m            indptr, indices, data)\n\u001b[0m\u001b[0;32m   1175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1176\u001b[0m         \u001b[0mA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "text_sample = char_series.as_matrix()\n",
    "# so we essenatially create our document_term matrix.... \n",
    "#display(small_text_sample)\n",
    "document_term_matrix = count_vectorizer.fit_transform(text_sample)\n",
    "display(document_term_matrix)\n",
    "for i in range(5,31):\n",
    "    n_topics = i\n",
    "\n",
    "    lsa_model = TruncatedSVD(n_components=n_topics)\n",
    "    lsa_topic_matrix = lsa_model.fit_transform(document_term_matrix)\n",
    "   # display(lsa_topic_matrix)\n",
    "\n",
    "    lsa_keys = get_keys(lsa_topic_matrix)\n",
    "    lsa_categories, lsa_counts = keys_to_counts(lsa_keys)\n",
    "\n",
    "    top_n_words_lsa = get_top_n_words(20, lsa_keys, document_term_matrix, count_vectorizer)\n",
    "\n",
    "    for i in range(len(top_n_words_lsa)):\n",
    "        print(\"Topic {}: \".format(i), top_n_words_lsa[i])\n",
    "    print('********')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i, you, 're, i, getting, you, 're, i, be, you, were, i, got, being, you, were, am, i]\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "ADV\n",
      "VERB\n",
      "ADV\n",
      "ADV\n",
      "VERB\n",
      "ADJ\n",
      "NOUN\n",
      "NOUN\n",
      "VERB\n",
      "ADV\n",
      "NOUN\n",
      "ADJ\n",
      "NOUN\n",
      "NOUN\n",
      "ADV\n",
      "ADV\n",
      "ADJ\n",
      "NOUN\n",
      "ADV\n",
      "VERB\n",
      "VERB\n",
      "ADV\n",
      "ADV\n",
      "ADV\n",
      "ADV\n",
      "VERB\n",
      "ADJ\n",
      "ADJ\n",
      "NOUN\n",
      "NOUN\n",
      "VERB\n",
      "NOUN\n",
      "NOUN\n",
      "VERB\n",
      "NOUN\n",
      "NOUN\n",
      "VERB\n",
      "NOUN\n",
      "NOUN\n",
      "NOUN\n",
      "VERB\n",
      "ADJ\n",
      "ADJ\n",
      "NOUN\n",
      "NOUN\n",
      "NOUN\n",
      "NOUN\n",
      "NOUN\n",
      "ADJ\n",
      "VERB\n",
      "ADJ\n",
      "NOUN\n",
      "VERB\n",
      "ADV\n",
      "ADJ\n",
      "NOUN\n",
      "ADV\n",
      "VERB\n",
      "VERB\n",
      "NOUN\n",
      "ADV\n",
      "NOUN\n",
      "ADJ\n",
      "ADV\n",
      "VERB\n",
      "ADJ\n",
      "ADJ\n",
      "NOUN\n",
      "NOUN\n",
      "VERB\n",
      "VERB\n",
      "NOUN\n",
      "VERB\n",
      "NOUN\n",
      "NOUN\n",
      "ADV\n",
      "VERB\n",
      "ADJ\n",
      "NOUN\n",
      "VERB\n",
      "ADJ\n",
      "VERB\n",
      "VERB\n",
      "ADJ\n",
      "NOUN\n",
      "VERB\n",
      "ADJ\n",
      "VERB\n",
      "ADV\n",
      "ADJ\n",
      "VERB\n",
      "NOUN\n",
      "ADV\n",
      "ADJ\n",
      "NOUN\n",
      "NOUN\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "ADV\n",
      "ADJ\n",
      "NOUN\n",
      "VERB\n",
      "NOUN\n",
      "VERB\n",
      "VERB\n",
      "NOUN\n",
      "ADJ\n",
      "NOUN\n",
      "VERB\n",
      "ADV\n",
      "ADJ\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "ADJ\n",
      "NOUN\n",
      "VERB\n",
      "NOUN\n",
      "ADJ\n",
      "ADJ\n",
      "NOUN\n",
      "VERB\n",
      "VERB\n",
      "ADV\n",
      "ADJ\n",
      "ADJ\n",
      "NOUN\n",
      "NOUN\n",
      "NOUN\n",
      "VERB\n",
      "NOUN\n",
      "NOUN\n",
      "VERB\n",
      "ADV\n",
      "ADJ\n",
      "NOUN\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "ADV\n",
      "VERB\n",
      "VERB\n",
      "ADV\n",
      "VERB\n",
      "ADV\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "NOUN\n",
      "ADJ\n",
      "NOUN\n",
      "VERB\n",
      "NOUN\n",
      "VERB\n",
      "ADJ\n",
      "NOUN\n",
      "VERB\n",
      "ADJ\n",
      "ADJ\n",
      "NOUN\n",
      "VERB\n",
      "ADJ\n",
      "NOUN\n",
      "ADJ\n",
      "NOUN\n",
      "NOUN\n",
      "NOUN\n",
      "VERB\n",
      "ADV\n",
      "ADV\n",
      "ADJ\n",
      "VERB\n",
      "NOUN\n",
      "NOUN\n",
      "ADV\n",
      "VERB\n",
      "VERB\n",
      "NOUN\n",
      "NOUN\n",
      "VERB\n",
      "ADJ\n",
      "NOUN\n",
      "NOUN\n",
      "VERB\n",
      "ADJ\n",
      "ADJ\n",
      "ADV\n",
      "VERB\n",
      "ADV\n",
      "ADV\n",
      "VERB\n",
      "VERB\n",
      "ADV\n",
      "ADJ\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "NOUN\n",
      "VERB\n",
      "ADJ\n",
      "NOUN\n",
      "NOUN\n",
      "ADJ\n",
      "NOUN\n",
      "VERB\n",
      "VERB\n",
      "ADJ\n",
      "NOUN\n",
      "NOUN\n",
      "ADV\n",
      "ADJ\n",
      "NOUN\n",
      "NOUN\n",
      "NOUN\n",
      "NOUN\n",
      "VERB\n",
      "VERB\n",
      "ADJ\n",
      "NOUN\n",
      "VERB\n",
      "VERB\n",
      "NOUN\n",
      "NOUN\n",
      "NOUN\n",
      "NOUN\n",
      "NOUN\n",
      "NOUN\n",
      "VERB\n",
      "NOUN\n",
      "ADV\n",
      "ADV\n",
      "ADV\n",
      "VERB\n",
      "ADV\n",
      "ADV\n",
      "VERB\n",
      "ADJ\n",
      "NOUN\n",
      "ADJ\n",
      "NOUN\n",
      "VERB\n",
      "ADJ\n",
      "NOUN\n",
      "ADJ\n",
      "NOUN\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "NOUN\n",
      "VERB\n",
      "ADJ\n",
      "VERB\n",
      "ADV\n",
      "VERB\n",
      "ADJ\n",
      "NOUN\n",
      "NOUN\n",
      "NOUN\n",
      "VERB\n",
      "NOUN\n",
      "VERB\n",
      "NOUN\n",
      "ADV\n",
      "ADJ\n",
      "NOUN\n",
      "NOUN\n",
      "ADV\n",
      "VERB\n",
      "VERB\n",
      "ADJ\n",
      "NOUN\n",
      "ADV\n",
      "VERB\n",
      "VERB\n",
      "NOUN\n",
      "VERB\n",
      "NOUN\n",
      "ADV\n",
      "NOUN\n",
      "ADV\n",
      "NOUN\n",
      "NOUN\n",
      "VERB\n",
      "ADV\n",
      "VERB\n",
      "ADJ\n",
      "NOUN\n",
      "NOUN\n",
      "ADJ\n",
      "ADV\n",
      "ADJ\n",
      "VERB\n",
      "NOUN\n",
      "VERB\n",
      "NOUN\n",
      "VERB\n",
      "ADV\n",
      "VERB\n",
      "ADJ\n",
      "NOUN\n",
      "VERB\n",
      "ADV\n",
      "ADV\n",
      "VERB\n",
      "VERB\n",
      "ADJ\n",
      "NOUN\n",
      "ADV\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "ADJ\n",
      "NOUN\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "NOUN\n",
      "ADJ\n",
      "NOUN\n",
      "VERB\n",
      "ADV\n",
      "ADV\n",
      "VERB\n",
      "ADJ\n",
      "NOUN\n",
      "VERB\n",
      "VERB\n",
      "ADV\n",
      "VERB\n",
      "VERB\n",
      "NOUN\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "NOUN\n",
      "ADV\n",
      "NOUN\n",
      "VERB\n",
      "NOUN\n",
      "ADV\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "ADJ\n",
      "NOUN\n",
      "NOUN\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "ADJ\n",
      "ADJ\n",
      "NOUN\n",
      "VERB\n",
      "VERB\n",
      "NOUN\n",
      "VERB\n",
      "VERB\n",
      "NOUN\n",
      "NOUN\n",
      "VERB\n",
      "VERB\n",
      "NOUN\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "NOUN\n",
      "VERB\n",
      "VERB\n",
      "NOUN\n",
      "VERB\n",
      "NOUN\n",
      "VERB\n",
      "ADV\n",
      "VERB\n",
      "NOUN\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "ADV\n",
      "VERB\n",
      "VERB\n",
      "NOUN\n",
      "VERB\n",
      "VERB\n",
      "ADV\n",
      "VERB\n",
      "ADV\n",
      "VERB\n",
      "ADJ\n",
      "ADJ\n",
      "NOUN\n",
      "NOUN\n",
      "ADV\n",
      "VERB\n",
      "NOUN\n",
      "NOUN\n",
      "VERB\n",
      "NOUN\n",
      "NOUN\n",
      "VERB\n",
      "VERB\n",
      "ADV\n",
      "ADV\n",
      "ADJ\n",
      "NOUN\n",
      "VERB\n",
      "ADJ\n",
      "ADJ\n",
      "VERB\n",
      "ADV\n",
      "VERB\n",
      "ADV\n",
      "ADJ\n",
      "NOUN\n",
      "NOUN\n",
      "VERB\n",
      "NOUN\n",
      "ADV\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "NOUN\n",
      "NOUN\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "ADV\n",
      "NOUN\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "ADJ\n",
      "NOUN\n",
      "ADV\n",
      "ADJ\n",
      "ADV\n",
      "VERB\n",
      "ADJ\n",
      "NOUN\n",
      "ADV\n",
      "VERB\n",
      "VERB\n",
      "ADJ\n",
      "NOUN\n",
      "VERB\n",
      "ADV\n",
      "NOUN\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "NOUN\n",
      "ADJ\n",
      "VERB\n",
      "ADJ\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "NOUN\n",
      "VERB\n",
      "ADJ\n",
      "VERB\n",
      "ADV\n",
      "VERB\n",
      "VERB\n",
      "ADV\n",
      "VERB\n",
      "ADV\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "ADV\n",
      "VERB\n",
      "VERB\n",
      "ADJ\n",
      "ADJ\n",
      "VERB\n",
      "ADV\n",
      "ADJ\n",
      "ADV\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "ADV\n",
      "VERB\n",
      "VERB\n",
      "ADV\n",
      "VERB\n",
      "VERB\n",
      "ADV\n",
      "VERB\n",
      "VERB\n",
      "NOUN\n",
      "VERB\n",
      "VERB\n",
      "NOUN\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "ADJ\n",
      "VERB\n",
      "ADV\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "NOUN\n",
      "ADV\n",
      "VERB\n",
      "NOUN\n",
      "VERB\n",
      "NOUN\n",
      "VERB\n",
      "ADV\n",
      "VERB\n",
      "ADJ\n",
      "VERB\n",
      "ADJ\n",
      "NOUN\n",
      "VERB\n",
      "NOUN\n",
      "VERB\n",
      "VERB\n",
      "ADV\n",
      "ADV\n",
      "VERB\n",
      "VERB\n",
      "ADJ\n",
      "NOUN\n",
      "ADV\n",
      "VERB\n",
      "VERB\n",
      "NOUN\n",
      "VERB\n",
      "ADV\n",
      "ADV\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "ADJ\n",
      "NOUN\n",
      "VERB\n",
      "ADV\n",
      "VERB\n",
      "ADV\n",
      "ADV\n",
      "VERB\n",
      "VERB\n",
      "ADJ\n",
      "NOUN\n",
      "NOUN\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "ADJ\n",
      "ADV\n",
      "VERB\n",
      "VERB\n",
      "NOUN\n",
      "VERB\n",
      "ADV\n",
      "VERB\n",
      "VERB\n",
      "ADV\n",
      "NOUN\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "ADV\n",
      "VERB\n",
      "NOUN\n",
      "VERB\n",
      "NOUN\n",
      "VERB\n",
      "VERB\n",
      "ADJ\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "ADJ\n",
      "NOUN\n",
      "VERB\n",
      "ADV\n",
      "VERB\n",
      "NOUN\n",
      "NOUN\n",
      "VERB\n",
      "VERB\n",
      "NOUN\n",
      "ADJ\n",
      "NOUN\n",
      "VERB\n",
      "VERB\n",
      "NOUN\n",
      "ADJ\n",
      "NOUN\n",
      "VERB\n",
      "VERB\n",
      "NOUN\n",
      "VERB\n",
      "VERB\n",
      "ADJ\n",
      "NOUN\n",
      "VERB\n",
      "VERB\n",
      "NOUN\n",
      "NOUN\n",
      "VERB\n",
      "VERB\n",
      "ADV\n",
      "VERB\n",
      "ADV\n",
      "ADJ\n",
      "VERB\n",
      "ADV\n",
      "NOUN\n",
      "VERB\n",
      "VERB\n",
      "NOUN\n",
      "ADV\n",
      "ADJ\n",
      "NOUN\n",
      "ADJ\n",
      "ADJ\n",
      "ADJ\n",
      "NOUN\n",
      "NOUN\n",
      "VERB\n",
      "NOUN\n",
      "NOUN\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "NOUN\n",
      "NOUN\n",
      "VERB\n",
      "ADJ\n",
      "NOUN\n",
      "VERB\n",
      "ADJ\n",
      "ADV\n",
      "VERB\n",
      "NOUN\n",
      "NOUN\n",
      "NOUN\n",
      "ADV\n",
      "VERB\n",
      "ADJ\n",
      "NOUN\n",
      "VERB\n",
      "NOUN\n",
      "NOUN\n",
      "ADV\n",
      "VERB\n",
      "ADV\n",
      "ADV\n",
      "ADJ\n",
      "NOUN\n",
      "NOUN\n",
      "NOUN\n",
      "VERB\n",
      "VERB\n",
      "NOUN\n",
      "NOUN\n",
      "ADJ\n",
      "NOUN\n",
      "VERB\n",
      "VERB\n",
      "ADV\n",
      "VERB\n",
      "NOUN\n",
      "VERB\n",
      "ADV\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "NOUN\n",
      "VERB\n",
      "NOUN\n",
      "VERB\n",
      "ADV\n",
      "ADV\n",
      "ADJ\n",
      "VERB\n",
      "ADJ\n",
      "ADJ\n",
      "NOUN\n",
      "ADV\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "ADV\n",
      "VERB\n",
      "VERB\n",
      "NOUN\n",
      "NOUN\n",
      "ADJ\n",
      "NOUN\n",
      "VERB\n",
      "ADJ\n",
      "NOUN\n",
      "VERB\n",
      "ADV\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "NOUN\n",
      "VERB\n",
      "VERB\n",
      "ADV\n",
      "VERB\n",
      "VERB\n",
      "NOUN\n",
      "NOUN\n",
      "VERB\n",
      "ADV\n",
      "VERB\n",
      "ADJ\n",
      "NOUN\n",
      "VERB\n",
      "ADJ\n",
      "ADV\n",
      "VERB\n",
      "NOUN\n",
      "ADV\n",
      "NOUN\n",
      "NOUN\n",
      "NOUN\n",
      "VERB\n",
      "NOUN\n",
      "NOUN\n",
      "NOUN\n",
      "VERB\n",
      "NOUN\n",
      "ADJ\n",
      "NOUN\n",
      "ADJ\n",
      "NOUN\n",
      "NOUN\n",
      "ADV\n",
      "VERB\n",
      "NOUN\n",
      "VERB\n",
      "ADV\n",
      "NOUN\n",
      "VERB\n",
      "NOUN\n",
      "ADV\n",
      "VERB\n",
      "VERB\n",
      "NOUN\n",
      "NOUN\n",
      "VERB\n",
      "ADV\n",
      "VERB\n",
      "VERB\n",
      "ADV\n",
      "VERB\n",
      "VERB\n",
      "ADJ\n",
      "NOUN\n",
      "ADV\n",
      "VERB\n",
      "VERB\n",
      "NOUN\n",
      "VERB\n",
      "VERB\n",
      "ADJ\n",
      "NOUN\n",
      "VERB\n",
      "ADJ\n",
      "NOUN\n",
      "NOUN\n",
      "VERB\n",
      "VERB\n",
      "NOUN\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "ADV\n",
      "VERB\n",
      "ADV\n",
      "VERB\n",
      "ADJ\n",
      "NOUN\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "ADJ\n",
      "NOUN\n",
      "VERB\n",
      "NOUN\n",
      "ADJ\n",
      "ADJ\n",
      "NOUN\n",
      "ADV\n",
      "NOUN\n",
      "ADV\n",
      "NOUN\n",
      "ADV\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "NOUN\n",
      "NOUN\n",
      "ADJ\n",
      "NOUN\n",
      "ADV\n",
      "VERB\n",
      "ADV\n",
      "ADV\n",
      "ADJ\n",
      "NOUN\n",
      "VERB\n",
      "NOUN\n",
      "VERB\n",
      "ADV\n",
      "VERB\n",
      "ADV\n",
      "ADV\n",
      "ADV\n",
      "VERB\n",
      "VERB\n",
      "ADV\n",
      "VERB\n",
      "ADV\n",
      "ADV\n",
      "ADJ\n",
      "VERB\n",
      "ADJ\n",
      "ADV\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "ADV\n",
      "VERB\n",
      "NOUN\n",
      "ADV\n",
      "NOUN\n",
      "ADV\n",
      "VERB\n",
      "VERB\n",
      "ADV\n",
      "ADV\n",
      "NOUN\n",
      "NOUN\n",
      "ADJ\n",
      "ADJ\n",
      "ADJ\n",
      "ADJ\n",
      "NOUN\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "ADJ\n",
      "ADJ\n",
      "NOUN\n",
      "VERB\n",
      "ADV\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "ADV\n",
      "NOUN\n",
      "VERB\n",
      "ADJ\n",
      "VERB\n",
      "ADV\n",
      "ADV\n",
      "VERB\n",
      "NOUN\n",
      "VERB\n",
      "VERB\n",
      "NOUN\n",
      "NOUN\n",
      "VERB\n",
      "VERB\n",
      "NOUN\n",
      "VERB\n",
      "ADJ\n",
      "ADV\n",
      "VERB\n",
      "VERB\n",
      "NOUN\n",
      "ADJ\n",
      "NOUN\n",
      "VERB\n",
      "ADV\n",
      "NOUN\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "NOUN\n",
      "VERB\n",
      "ADJ\n",
      "NOUN\n",
      "ADV\n",
      "NOUN\n",
      "VERB\n",
      "ADJ\n",
      "NOUN\n",
      "VERB\n",
      "VERB\n",
      "ADJ\n",
      "ADJ\n",
      "NOUN\n",
      "NOUN\n",
      "NOUN\n",
      "VERB\n",
      "VERB\n",
      "ADV\n",
      "VERB\n",
      "ADJ\n",
      "NOUN\n",
      "NOUN\n",
      "VERB\n",
      "ADV\n",
      "ADJ\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "NOUN\n",
      "ADV\n",
      "VERB\n",
      "VERB\n",
      "NOUN\n",
      "ADV\n",
      "NOUN\n",
      "VERB\n",
      "ADJ\n",
      "NOUN\n",
      "ADV\n",
      "ADV\n",
      "NOUN\n",
      "VERB\n",
      "VERB\n",
      "ADV\n",
      "ADV\n",
      "NOUN\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "ADJ\n",
      "ADV\n",
      "VERB\n",
      "VERB\n",
      "NOUN\n",
      "ADJ\n",
      "ADJ\n",
      "NOUN\n",
      "NOUN\n",
      "NOUN\n",
      "NOUN\n",
      "ADV\n",
      "VERB\n",
      "NOUN\n",
      "NOUN\n",
      "NOUN\n",
      "VERB\n",
      "NOUN\n",
      "ADJ\n",
      "VERB\n",
      "ADV\n",
      "VERB\n",
      "NOUN\n",
      "NOUN\n",
      "NOUN\n",
      "VERB\n",
      "NOUN\n",
      "ADV\n",
      "VERB\n",
      "VERB\n",
      "NOUN\n",
      "ADV\n",
      "NOUN\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "ADJ\n",
      "ADV\n",
      "ADV\n",
      "VERB\n",
      "VERB\n",
      "NOUN\n",
      "VERB\n",
      "VERB\n",
      "ADV\n",
      "VERB\n",
      "VERB\n",
      "NOUN\n",
      "VERB\n",
      "NOUN\n",
      "NOUN\n",
      "VERB\n",
      "ADJ\n",
      "VERB\n",
      "NOUN\n",
      "VERB\n",
      "ADJ\n",
      "NOUN\n",
      "VERB\n",
      "ADV\n",
      "VERB\n",
      "VERB\n",
      "ADJ\n",
      "ADJ\n",
      "ADJ\n",
      "NOUN\n",
      "ADV\n",
      "NOUN\n",
      "VERB\n",
      "ADJ\n",
      "VERB\n",
      "ADV\n",
      "ADV\n",
      "VERB\n",
      "VERB\n",
      "ADV\n",
      "VERB\n",
      "NOUN\n",
      "VERB\n",
      "NOUN\n",
      "VERB\n",
      "ADV\n",
      "VERB\n",
      "VERB\n",
      "ADV\n",
      "VERB\n",
      "ADV\n",
      "VERB\n",
      "VERB\n",
      "ADV\n",
      "ADJ\n",
      "ADV\n",
      "VERB\n",
      "NOUN\n",
      "VERB\n",
      "VERB\n",
      "ADV\n",
      "ADJ\n",
      "NOUN\n",
      "VERB\n",
      "ADV\n",
      "VERB\n",
      "ADV\n",
      "VERB\n",
      "NOUN\n",
      "VERB\n",
      "NOUN\n",
      "NOUN\n",
      "NOUN\n",
      "ADV\n",
      "ADV\n",
      "ADJ\n",
      "ADV\n",
      "ADV\n",
      "ADJ\n",
      "NOUN\n",
      "ADV\n",
      "ADV\n",
      "ADV\n",
      "ADV\n",
      "ADJ\n",
      "NOUN\n",
      "VERB\n",
      "VERB\n",
      "ADJ\n",
      "VERB\n",
      "ADV\n",
      "ADV\n",
      "VERB\n",
      "NOUN\n",
      "NOUN\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "ADV\n",
      "ADV\n",
      "VERB\n",
      "VERB\n",
      "ADV\n",
      "VERB\n",
      "ADJ\n",
      "ADJ\n",
      "NOUN\n",
      "ADV\n",
      "VERB\n",
      "NOUN\n",
      "NOUN\n",
      "VERB\n",
      "VERB\n",
      "ADJ\n",
      "NOUN\n",
      "NOUN\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "ADV\n",
      "VERB\n",
      "VERB\n",
      "VERB\n",
      "ADJ\n",
      "NOUN\n",
      "NOUN\n",
      "ADJ\n",
      "ADJ\n",
      "NOUN\n",
      "NOUN\n",
      "NOUN\n"
     ]
    }
   ],
   "source": [
    "# playing with spacy to get the passive voice tokens \n",
    "\n",
    "nlp = spacy.load('C://Users//seanp//Anaconda3//Lib//site-packages//en_core_web_sm//en_core_web_sm-2.0.0')\n",
    "each_sentence =  feat_frame.iloc[0,0]\n",
    "doc=nlp(each_sentence)\n",
    "\n",
    "passive_toks=[tok for tok in doc if (tok.dep_ == \"nsubjpass\" or tok.dep_ ==\"auxpass\") ]\n",
    "if passive_toks != []:\n",
    "    print(passive_toks)\n",
    "    \n",
    "for tok in doc:\n",
    "    if tok.pos_ in ['NOUN','VERB','ADV','ADJ']:\n",
    "        print(tok.pos_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosDF = pd.Series(index = char_series.index)\n",
    "cosDF = pd.DataFrame(cosDF)\n",
    "for movChar in char_series.index:\n",
    "    cosDF[str(movChar)] = pd.Series(np.nan,index = char_series.index)\n",
    "cosDF.drop(cosDF.columns[0], axis=1,inplace = True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Testing out bigrams code char_series.head()\n",
    "\n",
    "mytest = char_series[0]\n",
    "sentTest = sentence_tokenizer.tokenize(mytest)\n",
    "sentences_words = [treebank_tokenizer.tokenize(sentence) for sentence in sentTest]\n",
    "all_tokens = [word for sentence in sentences_words for word in sentence]\n",
    "bigrams = nltk.bigrams(all_tokens)\n",
    "new_text = ''\n",
    "bgs = [\"***\".join(gram) for gram in bigrams ]\n",
    "bigramz = \" \".join(bgs)\n",
    "bigramz\n",
    "\n",
    "\n",
    "# nouns verbs adverbs adjectives... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Playing with vader sentiment \n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "#miniboi = pd.Series()\n",
    "#for i in mov_dict['10-Things-I-Hate-About-You_script']['dialogues'].keys():\n",
    "#    miniboi.at[i+'____''10-Things-I-Hate-About-You_script'] = mov_dict['10-Things-I-Hate-About-You_script']['dialogues'][i]\n",
    "#miniboi\n",
    "feat_frame = pd.DataFrame(char_series)\n",
    "feat_frame['overall_polarity'] = ['WRONG']* char_series.shape[0]\n",
    "feat_frame.columns = ['dialogue','overall_polarity']\n",
    "feat_frame.head()\n",
    "for index, row in feat_frame.iterrows():\n",
    "    \n",
    "       feat_frame.loc[index,row.index[1]] = analyser.polarity_scores(row[0])['compound']\n",
    "\n",
    "\n",
    "# ok, so I suppose we will need to figure out how we will create the new index for the Pandas.Series\n",
    "# heck, I suppose I could even add a movie title AFTER... but meh, I'm lazy... I think I'll just leave it as the series for now! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_nostop = [w for w in treebank_tokenizer.tokenize(tizz) if w not  in stpwds] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'char_series' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-b150134aa306>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mchar_series\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'char_series' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KAT____10-Things-I-Hate-About-You_script        leave it i said , leave it ! why did n't we ju...\n",
       "PATRICK____10-Things-I-Hate-About-You_script    i missed you . it was a bratwurst . i was eati...\n",
       "BIANCA____10-Things-I-Hate-About-You_script     did you change your hair ? you might wan na th...\n",
       "CAMERON____10-Things-I-Hate-About-You_script    i do n't think so , ma'am so they tell me ... ...\n",
       "MICHAEL____10-Things-I-Hate-About-You_script    you the new guy ? c'mon . i 'm supposed to giv...\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### start here for the clustering features\n",
    "\n",
    "char_series.head()\n",
    "# alright, so analyzing the sentiment of what they are saying ABOUT each other is going to take a loooong fucking time \n",
    "# so how about doing 1: basic sentiment \n",
    "# FUCK I can't do passive voice bc of spacy... SHIT \n",
    "# maybe I ask pedro to do it in the cloud\n",
    "# so .... count # of stopwords... \n",
    "# count number of sentences???? \n",
    "# need better.... \n",
    "# check for b-b- for stutters??? \n",
    "# that's toufh "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for index,row in pd.DataFrame(char_series).iterrows():\n",
    "    \n",
    "# number of first person pronouns\n",
    "\n",
    "# % of sentences with hedges\n",
    "# sentiment of the verbs \n",
    "# number of verbs per sentence? \n",
    "# look for concession dictionary\n",
    "# look for FILLER/PAUSE dictionaries... (those might be stopwords) \n",
    "# look for contrast dictionaries..... \n",
    "# look for OPINION... \n",
    "\n",
    "hedge_words = ['largely','generally',\n",
    "'often','rarely','sometimes','frequently','occasionally','seldom','usually','most','several','some','almost','practically',\n",
    "'apparently','virtually','basically','approximately','roughly','somewhat','somehow','partially','actually','like','something',\n",
    "'someone','somebody','somewhere',\n",
    "'assume','assumes','assumed','appear','appears','appeared','seem','seems',\n",
    "'seemed','suppose','supposes','supposed','guess','guesses','guessed','estimate','estimates','estimated',\n",
    "'speculate',\n",
    "'speculates',\n",
    "'speculated',\n",
    "'suggest',\n",
    "'suggests',\n",
    "'suggested',\n",
    "'may',\n",
    "'could',\n",
    "'should',\n",
    "'might',\n",
    "'surely',\n",
    "'probably',\n",
    "'likely',\n",
    "'maybe',\n",
    "'perhaps',\n",
    "'unsure',\n",
    "'probable',\n",
    "'unlikely',\n",
    "'possibly',\n",
    "'possible',\n",
    "'read',\n",
    "'say',\n",
    "'says',\n",
    "'looks like',\n",
    "'look like',\n",
    "\"do n't know\",\n",
    "\"necessarily\",\n",
    "\"kind of\",\n",
    "\"much\",\n",
    "\"bunch\",\n",
    "\"couple\",\n",
    "\"few\",\n",
    "\"little\",\n",
    "\"really\",\n",
    "\"and all that\",\n",
    "\"and so forth\",\n",
    "\"et cetera\",\n",
    "\"if i 'm understanding you correctly\",\n",
    "\"something or other\",\n",
    "\"so far\",\n",
    "\"at least\",\n",
    "\"around\",\n",
    "\"can\",\n",
    "\"effectively\",\n",
    "\"evidently\",\n",
    "\"fairly\",\n",
    "\"hopefully\",\n",
    "\"in general\",\n",
    "\"mainly\",\n",
    "\"more or less\",\n",
    "\"mostly\",\n",
    "\"overall\",\n",
    "\"presumably\",\n",
    "\"pretty\",\n",
    "\"quite clearly\",\n",
    "\"quite\",\n",
    "\"rather\",\n",
    "\"sort of\",\n",
    "\"supposedly\",\n",
    "\"tend\",\n",
    "\"appear to be\",\n",
    "\"doubt\",\n",
    "\"be sure\",\n",
    "\"indicate\",\n",
    "\"must\",\n",
    "\"would\",\n",
    "\"certainly\",\n",
    "\"definitely\",\n",
    "\"clearly\",\n",
    "\"conceivably\",\n",
    "\"certain\",\n",
    "\"definite\",\n",
    "\"clear\",\n",
    "\"assumption\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "feels = ['think','thinks','thought','believe','believed','believes','consider','considers','considered',\"in my mind\",\n",
    "\"in my opinion\",\n",
    "\"their impression\",\n",
    "\"my impression\",\n",
    "\"my understanding\",\n",
    "\"my thinking is\",\n",
    "\"my understanding is\",\n",
    "\"in my view\", 'my stance', 'my two cents',\n",
    "'if you ask me']\n",
    "p = re.compile('(u+h+)|(u+m+)|(e+r+m*)|(\\.\\.\\.)|(h+m+)|(huh)')\n",
    "p2 = re.compile('(f+u+c+k)|(s+h+i+t)|(d+a+m+n)|(b+i+t+c+h)|(b+a+s+t+a+r+d+)|(c+o+c+k)|(d+i+c+k)|(n+i+g+a+)|(b+a+l+l+s)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "char_series = pd.DataFrame(char_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from readability import Readability  #https://github.com/cdimascio/py-readability-metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we processed 10\n",
      "we processed 100\n",
      "we processed 500\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# so... I guess I could just tokenize every sentnece and go from there??? \n",
    "# nononnonon for regex, we HAVE to actually do the word tokenizer... \n",
    "countar = 0 \n",
    "for index, row in char_series.iterrows():\n",
    "    countar +=1\n",
    "    if countar == 10:\n",
    "        print('we processed 10')\n",
    "    if countar == 100:\n",
    "        print('we processed 100')\n",
    "    if countar == 500:\n",
    "        print('we processed 500')\n",
    "    words = word_tokenize(row[0])\n",
    "\n",
    "    sentz = sentence_tokenizer.tokenize(row[0])\n",
    "        # iterate though each string inside of the list of sentences       \n",
    "        #num_passive = 0\n",
    "        #coref_sents = 0 \n",
    "    num_fill = 0\n",
    "    num_feels = 0\n",
    "    num_hw = 0\n",
    "    for sentence in sentz:\n",
    "        feelList = [feelWyd for feelWyd in feels if feelWyd in sentence]\n",
    "        if feelList == []:\n",
    "            num_feels +=0\n",
    "        else:\n",
    "            num_feels += len(feelList)\n",
    "        hwList = [hwWyd for hwWyd in hedge_words if hwWyd in sentence]\n",
    "        if hwList == []:\n",
    "            num_hw += 0\n",
    "        else:\n",
    "            num_hw += len(hwList)\n",
    "            \n",
    "        fillWyds = [word for word in word_tokenize(sentence) if p.fullmatch(word) != None]\n",
    "        if fillWyds == []:\n",
    "            num_fill +=0\n",
    "        else:\n",
    "            num_fill+= len(fillWyds)\n",
    "    char_series.loc[index,'tot_feel'] = num_feels\n",
    "    char_series.loc[index,'tot_fill'] = num_fill\n",
    "    char_series.loc[index,'tot_hw'] = num_hw\n",
    "\n",
    "    punct_sings = \".!?,;:-_--\\'\\'\\'``...\"\n",
    "    words_stemmed = [porter_stemmer.stem(word) for word in words if word not in punct_sings]\n",
    "        # Gets the number of unique stems\n",
    "    char_series.loc[index,'uniqwords'] = len(set(words_stemmed))\n",
    "        \n",
    "        # FEAT #2: Flesch Kincaid Read Level per character --> Categorical\n",
    "        # https://github.com/cdimascio/py-readability-metrics\n",
    "  \n",
    "        \n",
    "        # FEAT #3: Number of Stop Words per character --> Numerical\n",
    "    char_series.loc[index,'stopwords']  =  len(set([w for w in words if w.lower() in stpwds]))\n",
    "    char_series.loc[index,'polarity'] = analyser.polarity_scores(row[0])['compound']\n",
    "\n",
    "      \n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Must have equal len keys and value when setting with an iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-79-17615b40f978>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mchar_series\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cursewords'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchar_series\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mchar_series\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'FK'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchar_series\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mchar_series\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'polarity'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchar_series\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    187\u001b[0m             \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_setitem_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer\u001b[1;34m(self, indexer, value)\u001b[0m\n\u001b[0;32m    604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m                         raise ValueError('Must have equal len keys and value '\n\u001b[0m\u001b[0;32m    607\u001b[0m                                          'when setting with an iterable')\n\u001b[0;32m    608\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Must have equal len keys and value when setting with an iterable"
     ]
    }
   ],
   "source": [
    "char_series['tot_feel'] = np.zeros(char_series.shape[0])\n",
    "char_series['tot_fill'] = np.zeros(char_series.shape[0])\n",
    "char_series['tot_hw'] = np.zeros(char_series.shape[0])\n",
    "char_series['stopwords'] = np.zeros(char_series.shape[0])\n",
    "char_series['uniqwords'] = np.zeros(char_series.shape[0])\n",
    "char_series['polarity'] = np.zeros(char_series.shape[0])\n",
    "char_series['cursewords'] = np.zeros(char_series.shape[0])\n",
    "char_series['FK'] = np.zeros(char_series.shape[0])\n",
    "char_series.loc[index,'polarity'] = np.zeros(char_series.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KAT____10-Things-I-Hate-About-You_script           0.0\n",
       "PATRICK____10-Things-I-Hate-About-You_script       0.0\n",
       "BIANCA____10-Things-I-Hate-About-You_script        0.0\n",
       "CAMERON____10-Things-I-Hate-About-You_script       0.0\n",
       "MICHAEL____10-Things-I-Hate-About-You_script       0.0\n",
       "JOEY____10-Things-I-Hate-About-You_script          0.0\n",
       "WALTER____10-Things-I-Hate-About-You_script        0.0\n",
       "MANDELLA____10-Things-I-Hate-About-You_script      0.0\n",
       "MISS PERKY____10-Things-I-Hate-About-You_script    0.0\n",
       "CHASTITY____10-Things-I-Hate-About-You_script      0.0\n",
       "SHARON____10-Things-I-Hate-About-You_script        0.0\n",
       "BRUCE____10-Things-I-Hate-About-You_script         0.0\n",
       "TEACHER____10-Things-I-Hate-About-You_script       0.0\n",
       "COLE____12-Monkeys_script                          0.0\n",
       "RAILLY____12-Monkeys_script                        0.0\n",
       "JEFFREY____12-Monkeys_script                       0.0\n",
       "ASTROPHYSICIST____12-Monkeys_script                0.0\n",
       "RASPY VOICE____12-Monkeys_script                   0.0\n",
       "FALE____12-Monkeys_script                          0.0\n",
       "BOTANIST____12-Monkeys_script                      0.0\n",
       "MICROBIOLOGIST____12-Monkeys_script                0.0\n",
       "JOSE____12-Monkeys_script                          0.0\n",
       "ENGINEER____12-Monkeys_script                      0.0\n",
       "BILLINGS____12-Monkeys_script                      0.0\n",
       "GEOLOGIST____12-Monkeys_script                     0.0\n",
       "BEN____12-Monkeys_script                           0.0\n",
       "ZOOLOGIST____12-Monkeys_script                     0.0\n",
       "WALLACE____12-Monkeys_script                       0.0\n",
       "FRANKI____12-Monkeys_script                        0.0\n",
       "RAILLY'S VOICE____12-Monkeys_script                0.0\n",
       "                                                  ... \n",
       "PILOT____Zero-Dark-Thirty_script                   0.0\n",
       "FARAJ____Zero-Dark-Thirty_script                   0.0\n",
       "JARED____Zero-Dark-Thirty_script                   0.0\n",
       "COMMANDING OFFICER____Zero-Dark-Thirty_script      0.0\n",
       "SOLDIER INTERROGATOR____Zero-Dark-Thirty_script    0.0\n",
       "REPORTER____Zero-Dark-Thirty_script                0.0\n",
       "SECURTY GUARD____Zero-Dark-Thirty_script           0.0\n",
       "WOLF____Zero-Dark-Thirty_script                    0.0\n",
       "SABER____Zero-Dark-Thirty_script                   0.0\n",
       "HOPPS____Zootopia_script                           0.0\n",
       "NICK____Zootopia_script                            0.0\n",
       "BOGO____Zootopia_script                            0.0\n",
       "STU HOPPS____Zootopia_script                       0.0\n",
       "BELLWETHER____Zootopia_script                      0.0\n",
       "BONNIE HOPPS____Zootopia_script                    0.0\n",
       "FLASH____Zootopia_script                           0.0\n",
       "CLAWHAUSER____Zootopia_script                      0.0\n",
       "YOUNG JUDY____Zootopia_script                      0.0\n",
       "LIONHEART____Zootopia_script                       0.0\n",
       "DUKE WEASELTON____Zootopia_script                  0.0\n",
       "GIDEON GREY____Zootopia_script                     0.0\n",
       "JUDY____Zootopia_script                            0.0\n",
       "GAZELLE____Zootopia_script                         0.0\n",
       "FRU FRU SHREW____Zootopia_script                   0.0\n",
       "YAX THE HIPPIE YAK____Zootopia_script              0.0\n",
       "MAJOR FRIEDKIN____Zootopia_script                  0.0\n",
       "ORYX POOTOSSER____Zootopia_script                  0.0\n",
       "YAX____Zootopia_script                             0.0\n",
       "MEAN KID ANIMAL____Zootopia_script                 0.0\n",
       "DOUG____Zootopia_script                            0.0\n",
       "Name: tot_hw, Length: 11075, dtype: float64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_series.tot_hw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyser = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>tot_feel</th>\n",
       "      <th>tot_fill</th>\n",
       "      <th>tot_hw</th>\n",
       "      <th>stopwords</th>\n",
       "      <th>polarity</th>\n",
       "      <th>cursewords</th>\n",
       "      <th>FK</th>\n",
       "      <th>uniqwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>KAT____10-Things-I-Hate-About-You_script</th>\n",
       "      <td>leave it i said , leave it ! why did n't we ju...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-0.9984</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>603.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PATRICK____10-Things-I-Hate-About-You_script</th>\n",
       "      <td>i missed you . it was a bratwurst . i was eati...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>-0.9952</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>465.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BIANCA____10-Things-I-Hate-About-You_script</th>\n",
       "      <td>did you change your hair ? you might wan na th...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>418.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CAMERON____10-Things-I-Hate-About-You_script</th>\n",
       "      <td>i do n't think so , ma'am so they tell me ... ...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>316.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MICHAEL____10-Things-I-Hate-About-You_script</th>\n",
       "      <td>you the new guy ? c'mon . i 'm supposed to giv...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>0.9984</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>419.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                              0  \\\n",
       "KAT____10-Things-I-Hate-About-You_script      leave it i said , leave it ! why did n't we ju...   \n",
       "PATRICK____10-Things-I-Hate-About-You_script  i missed you . it was a bratwurst . i was eati...   \n",
       "BIANCA____10-Things-I-Hate-About-You_script   did you change your hair ? you might wan na th...   \n",
       "CAMERON____10-Things-I-Hate-About-You_script  i do n't think so , ma'am so they tell me ... ...   \n",
       "MICHAEL____10-Things-I-Hate-About-You_script  you the new guy ? c'mon . i 'm supposed to giv...   \n",
       "\n",
       "                                              tot_feel  tot_fill  tot_hw  \\\n",
       "KAT____10-Things-I-Hate-About-You_script          18.0       0.0    87.0   \n",
       "PATRICK____10-Things-I-Hate-About-You_script       6.0      13.0    60.0   \n",
       "BIANCA____10-Things-I-Hate-About-You_script       13.0       9.0    75.0   \n",
       "CAMERON____10-Things-I-Hate-About-You_script       8.0       8.0    58.0   \n",
       "MICHAEL____10-Things-I-Hate-About-You_script       7.0       3.0    46.0   \n",
       "\n",
       "                                              stopwords  polarity  cursewords  \\\n",
       "KAT____10-Things-I-Hate-About-You_script           97.0   -0.9984         0.0   \n",
       "PATRICK____10-Things-I-Hate-About-You_script       86.0   -0.9952         0.0   \n",
       "BIANCA____10-Things-I-Hate-About-You_script        85.0    0.9968         0.0   \n",
       "CAMERON____10-Things-I-Hate-About-You_script       71.0    0.9951         0.0   \n",
       "MICHAEL____10-Things-I-Hate-About-You_script       83.0    0.9984         0.0   \n",
       "\n",
       "                                               FK  uniqwords  \n",
       "KAT____10-Things-I-Hate-About-You_script      0.0      603.0  \n",
       "PATRICK____10-Things-I-Hate-About-You_script  0.0      465.0  \n",
       "BIANCA____10-Things-I-Hate-About-You_script   0.0      418.0  \n",
       "CAMERON____10-Things-I-Hate-About-You_script  0.0      316.0  \n",
       "MICHAEL____10-Things-I-Hate-About-You_script  0.0      419.0  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_series.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>tot_feel</th>\n",
       "      <th>tot_fill</th>\n",
       "      <th>tot_hw</th>\n",
       "      <th>stopwords</th>\n",
       "      <th>polarity</th>\n",
       "      <th>cursewords</th>\n",
       "      <th>FK</th>\n",
       "      <th>uniqwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>KAT____10-Things-I-Hate-About-You_script</th>\n",
       "      <td>leave it i said , leave it ! why did n't we ju...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-0.9984</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>603.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PATRICK____10-Things-I-Hate-About-You_script</th>\n",
       "      <td>i missed you . it was a bratwurst . i was eati...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>-0.9952</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>465.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BIANCA____10-Things-I-Hate-About-You_script</th>\n",
       "      <td>did you change your hair ? you might wan na th...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>418.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CAMERON____10-Things-I-Hate-About-You_script</th>\n",
       "      <td>i do n't think so , ma'am so they tell me ... ...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>316.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MICHAEL____10-Things-I-Hate-About-You_script</th>\n",
       "      <td>you the new guy ? c'mon . i 'm supposed to giv...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>0.9984</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>419.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JOEY____10-Things-I-Hate-About-You_script</th>\n",
       "      <td>as opposed to a bitter self-righteous hag who ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0.9830</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>249.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WALTER____10-Things-I-Hate-About-You_script</th>\n",
       "      <td>i hope dinner 's ready because i only have ten...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0.9483</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>232.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MANDELLA____10-Things-I-Hate-About-You_script</th>\n",
       "      <td>uh , yeah , i read it all have n't you ? your ...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0.9972</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>198.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MISS PERKY____10-Things-I-Hate-About-You_script</th>\n",
       "      <td>i 'm sure you wo n't find padua any different ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.8633</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>176.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHASTITY____10-Things-I-Hate-About-You_script</th>\n",
       "      <td>no . who ? great is he oily or dry ? , i do n'...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.7131</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SHARON____10-Things-I-Hate-About-You_script</th>\n",
       "      <td>in the microwave . what 's a synonym for throb...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.9384</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BRUCE____10-Things-I-Hate-About-You_script</th>\n",
       "      <td>i can count . go ahead . and you take it easy ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.6747</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TEACHER____10-Things-I-Hate-About-You_script</th>\n",
       "      <td>i realize the language of mr. shakespeare make...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.9601</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COLE____12-Monkeys_script</th>\n",
       "      <td>ssssst ! , what 's going on ? i did n't volunt...</td>\n",
       "      <td>24.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>554.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAILLY____12-Monkeys_script</th>\n",
       "      <td>he 's been tested for drugs ? you have him in ...</td>\n",
       "      <td>24.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>0.9034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>576.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                 0  \\\n",
       "KAT____10-Things-I-Hate-About-You_script         leave it i said , leave it ! why did n't we ju...   \n",
       "PATRICK____10-Things-I-Hate-About-You_script     i missed you . it was a bratwurst . i was eati...   \n",
       "BIANCA____10-Things-I-Hate-About-You_script      did you change your hair ? you might wan na th...   \n",
       "CAMERON____10-Things-I-Hate-About-You_script     i do n't think so , ma'am so they tell me ... ...   \n",
       "MICHAEL____10-Things-I-Hate-About-You_script     you the new guy ? c'mon . i 'm supposed to giv...   \n",
       "JOEY____10-Things-I-Hate-About-You_script        as opposed to a bitter self-righteous hag who ...   \n",
       "WALTER____10-Things-I-Hate-About-You_script      i hope dinner 's ready because i only have ten...   \n",
       "MANDELLA____10-Things-I-Hate-About-You_script    uh , yeah , i read it all have n't you ? your ...   \n",
       "MISS PERKY____10-Things-I-Hate-About-You_script  i 'm sure you wo n't find padua any different ...   \n",
       "CHASTITY____10-Things-I-Hate-About-You_script    no . who ? great is he oily or dry ? , i do n'...   \n",
       "SHARON____10-Things-I-Hate-About-You_script      in the microwave . what 's a synonym for throb...   \n",
       "BRUCE____10-Things-I-Hate-About-You_script       i can count . go ahead . and you take it easy ...   \n",
       "TEACHER____10-Things-I-Hate-About-You_script     i realize the language of mr. shakespeare make...   \n",
       "COLE____12-Monkeys_script                        ssssst ! , what 's going on ? i did n't volunt...   \n",
       "RAILLY____12-Monkeys_script                      he 's been tested for drugs ? you have him in ...   \n",
       "\n",
       "                                                 tot_feel  tot_fill  tot_hw  \\\n",
       "KAT____10-Things-I-Hate-About-You_script             18.0       0.0    87.0   \n",
       "PATRICK____10-Things-I-Hate-About-You_script          6.0      13.0    60.0   \n",
       "BIANCA____10-Things-I-Hate-About-You_script          13.0       9.0    75.0   \n",
       "CAMERON____10-Things-I-Hate-About-You_script          8.0       8.0    58.0   \n",
       "MICHAEL____10-Things-I-Hate-About-You_script          7.0       3.0    46.0   \n",
       "JOEY____10-Things-I-Hate-About-You_script             3.0       0.0    17.0   \n",
       "WALTER____10-Things-I-Hate-About-You_script           3.0       0.0    14.0   \n",
       "MANDELLA____10-Things-I-Hate-About-You_script         4.0       2.0    20.0   \n",
       "MISS PERKY____10-Things-I-Hate-About-You_script       1.0       2.0    15.0   \n",
       "CHASTITY____10-Things-I-Hate-About-You_script         2.0       0.0     3.0   \n",
       "SHARON____10-Things-I-Hate-About-You_script           0.0       1.0     2.0   \n",
       "BRUCE____10-Things-I-Hate-About-You_script            0.0       0.0     2.0   \n",
       "TEACHER____10-Things-I-Hate-About-You_script          0.0       0.0     6.0   \n",
       "COLE____12-Monkeys_script                            24.0      89.0   120.0   \n",
       "RAILLY____12-Monkeys_script                          24.0      68.0   117.0   \n",
       "\n",
       "                                                 stopwords  polarity  \\\n",
       "KAT____10-Things-I-Hate-About-You_script              97.0   -0.9984   \n",
       "PATRICK____10-Things-I-Hate-About-You_script          86.0   -0.9952   \n",
       "BIANCA____10-Things-I-Hate-About-You_script           85.0    0.9968   \n",
       "CAMERON____10-Things-I-Hate-About-You_script          71.0    0.9951   \n",
       "MICHAEL____10-Things-I-Hate-About-You_script          83.0    0.9984   \n",
       "JOEY____10-Things-I-Hate-About-You_script             71.0    0.9830   \n",
       "WALTER____10-Things-I-Hate-About-You_script           72.0    0.9483   \n",
       "MANDELLA____10-Things-I-Hate-About-You_script         56.0    0.9972   \n",
       "MISS PERKY____10-Things-I-Hate-About-You_script       52.0    0.8633   \n",
       "CHASTITY____10-Things-I-Hate-About-You_script         32.0    0.7131   \n",
       "SHARON____10-Things-I-Hate-About-You_script           15.0    0.9384   \n",
       "BRUCE____10-Things-I-Hate-About-You_script            24.0    0.6747   \n",
       "TEACHER____10-Things-I-Hate-About-You_script          22.0    0.9601   \n",
       "COLE____12-Monkeys_script                             92.0    0.9980   \n",
       "RAILLY____12-Monkeys_script                           94.0    0.9034   \n",
       "\n",
       "                                                 cursewords   FK  uniqwords  \n",
       "KAT____10-Things-I-Hate-About-You_script                0.0  0.0      603.0  \n",
       "PATRICK____10-Things-I-Hate-About-You_script            0.0  0.0      465.0  \n",
       "BIANCA____10-Things-I-Hate-About-You_script             0.0  0.0      418.0  \n",
       "CAMERON____10-Things-I-Hate-About-You_script            0.0  0.0      316.0  \n",
       "MICHAEL____10-Things-I-Hate-About-You_script            0.0  0.0      419.0  \n",
       "JOEY____10-Things-I-Hate-About-You_script               0.0  0.0      249.0  \n",
       "WALTER____10-Things-I-Hate-About-You_script             0.0  0.0      232.0  \n",
       "MANDELLA____10-Things-I-Hate-About-You_script           0.0  0.0      198.0  \n",
       "MISS PERKY____10-Things-I-Hate-About-You_script         0.0  0.0      176.0  \n",
       "CHASTITY____10-Things-I-Hate-About-You_script           0.0  0.0       69.0  \n",
       "SHARON____10-Things-I-Hate-About-You_script             0.0  0.0       47.0  \n",
       "BRUCE____10-Things-I-Hate-About-You_script              0.0  0.0       56.0  \n",
       "TEACHER____10-Things-I-Hate-About-You_script            0.0  0.0       56.0  \n",
       "COLE____12-Monkeys_script                               0.0  0.0      554.0  \n",
       "RAILLY____12-Monkeys_script                             0.0  0.0      576.0  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_series.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_series.to_csv(\"character_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_counts = {}\n",
    "for i, char in enumerate(char_series.index[:-1]):\n",
    "    movie_title = char[char.find(\"_\")+4:]\n",
    "    next_movie = char_series.index[i+1][char_series.index[i+1].find(\"_\")+4:]\n",
    "    if next_movie != movie_title:\n",
    "        diag_counts[movie_title][char] = len(char_series.loc[char][0])\n",
    "        diag_counts[next_movie] = {}\n",
    "    else:\n",
    "        if movie_title not in diag_counts:\n",
    "            diag_counts[movie_title] = {}\n",
    "            diag_counts[movie_title][char] = len(char_series.loc[char][0])\n",
    "        else:\n",
    "            diag_counts[movie_title][char] = len(char_series.loc[char][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "keep = []\n",
    "for diag_dict in diag_counts.values():\n",
    "    sorted_diag_dict = sorted(diag_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    keep += [name[0] for name in list(sorted_diag_dict)[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "keepp = []\n",
    "for char in char_series.index[:-1]:\n",
    "    if char in keep:\n",
    "        keepp.append(char)\n",
    "drop = []\n",
    "for char in char_series.index[:-1]:\n",
    "    if char not in keepp:\n",
    "        drop.append(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_series_top_5 = char_series.drop(drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>tot_feel</th>\n",
       "      <th>tot_fill</th>\n",
       "      <th>tot_hw</th>\n",
       "      <th>stopwords</th>\n",
       "      <th>polarity</th>\n",
       "      <th>cursewords</th>\n",
       "      <th>FK</th>\n",
       "      <th>uniqwords</th>\n",
       "      <th>tot_sents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>KAT____10-Things-I-Hate-About-You_script</th>\n",
       "      <td>leave it i said , leave it ! why did n't we ju...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-0.9984</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>603.0</td>\n",
       "      <td>268.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PATRICK____10-Things-I-Hate-About-You_script</th>\n",
       "      <td>i missed you . it was a bratwurst . i was eati...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>-0.9952</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>465.0</td>\n",
       "      <td>207.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BIANCA____10-Things-I-Hate-About-You_script</th>\n",
       "      <td>did you change your hair ? you might wan na th...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>418.0</td>\n",
       "      <td>173.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CAMERON____10-Things-I-Hate-About-You_script</th>\n",
       "      <td>i do n't think so , ma'am so they tell me ... ...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>316.0</td>\n",
       "      <td>116.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MICHAEL____10-Things-I-Hate-About-You_script</th>\n",
       "      <td>you the new guy ? c'mon . i 'm supposed to giv...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>0.9984</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>419.0</td>\n",
       "      <td>133.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COLE____12-Monkeys_script</th>\n",
       "      <td>ssssst ! , what 's going on ? i did n't volunt...</td>\n",
       "      <td>24.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>554.0</td>\n",
       "      <td>477.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAILLY____12-Monkeys_script</th>\n",
       "      <td>he 's been tested for drugs ? you have him in ...</td>\n",
       "      <td>24.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>0.9034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>576.0</td>\n",
       "      <td>327.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JEFFREY____12-Monkeys_script</th>\n",
       "      <td>how much you gon na pay me ? huh ? i 'd be doi...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>-0.9976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>547.0</td>\n",
       "      <td>233.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RASPY VOICE____12-Monkeys_script</th>\n",
       "      <td>hey ! who 's that ? hey , bob ... what 's your...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.9486</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FALE____12-Monkeys_script</th>\n",
       "      <td>uh , can we help you ? excuse me . you looking...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>-0.9636</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                              0  \\\n",
       "KAT____10-Things-I-Hate-About-You_script      leave it i said , leave it ! why did n't we ju...   \n",
       "PATRICK____10-Things-I-Hate-About-You_script  i missed you . it was a bratwurst . i was eati...   \n",
       "BIANCA____10-Things-I-Hate-About-You_script   did you change your hair ? you might wan na th...   \n",
       "CAMERON____10-Things-I-Hate-About-You_script  i do n't think so , ma'am so they tell me ... ...   \n",
       "MICHAEL____10-Things-I-Hate-About-You_script  you the new guy ? c'mon . i 'm supposed to giv...   \n",
       "COLE____12-Monkeys_script                     ssssst ! , what 's going on ? i did n't volunt...   \n",
       "RAILLY____12-Monkeys_script                   he 's been tested for drugs ? you have him in ...   \n",
       "JEFFREY____12-Monkeys_script                  how much you gon na pay me ? huh ? i 'd be doi...   \n",
       "RASPY VOICE____12-Monkeys_script              hey ! who 's that ? hey , bob ... what 's your...   \n",
       "FALE____12-Monkeys_script                     uh , can we help you ? excuse me . you looking...   \n",
       "\n",
       "                                              tot_feel  tot_fill  tot_hw  \\\n",
       "KAT____10-Things-I-Hate-About-You_script          18.0       0.0    87.0   \n",
       "PATRICK____10-Things-I-Hate-About-You_script       6.0      13.0    60.0   \n",
       "BIANCA____10-Things-I-Hate-About-You_script       13.0       9.0    75.0   \n",
       "CAMERON____10-Things-I-Hate-About-You_script       8.0       8.0    58.0   \n",
       "MICHAEL____10-Things-I-Hate-About-You_script       7.0       3.0    46.0   \n",
       "COLE____12-Monkeys_script                         24.0      89.0   120.0   \n",
       "RAILLY____12-Monkeys_script                       24.0      68.0   117.0   \n",
       "JEFFREY____12-Monkeys_script                       9.0      20.0    48.0   \n",
       "RASPY VOICE____12-Monkeys_script                   2.0       7.0    21.0   \n",
       "FALE____12-Monkeys_script                          0.0       8.0    10.0   \n",
       "\n",
       "                                              stopwords  polarity  cursewords  \\\n",
       "KAT____10-Things-I-Hate-About-You_script           97.0   -0.9984         0.0   \n",
       "PATRICK____10-Things-I-Hate-About-You_script       86.0   -0.9952         0.0   \n",
       "BIANCA____10-Things-I-Hate-About-You_script        85.0    0.9968         0.0   \n",
       "CAMERON____10-Things-I-Hate-About-You_script       71.0    0.9951         0.0   \n",
       "MICHAEL____10-Things-I-Hate-About-You_script       83.0    0.9984         0.0   \n",
       "COLE____12-Monkeys_script                          92.0    0.9980         0.0   \n",
       "RAILLY____12-Monkeys_script                        94.0    0.9034         0.0   \n",
       "JEFFREY____12-Monkeys_script                       86.0   -0.9976         0.0   \n",
       "RASPY VOICE____12-Monkeys_script                   46.0    0.9486         0.0   \n",
       "FALE____12-Monkeys_script                          52.0   -0.9636         0.0   \n",
       "\n",
       "                                               FK  uniqwords  tot_sents  \n",
       "KAT____10-Things-I-Hate-About-You_script      0.0      603.0      268.0  \n",
       "PATRICK____10-Things-I-Hate-About-You_script  0.0      465.0      207.0  \n",
       "BIANCA____10-Things-I-Hate-About-You_script   0.0      418.0      173.0  \n",
       "CAMERON____10-Things-I-Hate-About-You_script  0.0      316.0      116.0  \n",
       "MICHAEL____10-Things-I-Hate-About-You_script  0.0      419.0      133.0  \n",
       "COLE____12-Monkeys_script                     0.0      554.0      477.0  \n",
       "RAILLY____12-Monkeys_script                   0.0      576.0      327.0  \n",
       "JEFFREY____12-Monkeys_script                  0.0      547.0      233.0  \n",
       "RASPY VOICE____12-Monkeys_script              0.0      149.0       54.0  \n",
       "FALE____12-Monkeys_script                     0.0      177.0       30.0  "
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_series_top_5.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'KAT____10-Things-I-Hate-About-You_script': 9936,\n",
       " 'PATRICK____10-Things-I-Hate-About-You_script': 7205,\n",
       " 'BIANCA____10-Things-I-Hate-About-You_script': 6832,\n",
       " 'CAMERON____10-Things-I-Hate-About-You_script': 4372,\n",
       " 'MICHAEL____10-Things-I-Hate-About-You_script': 5198,\n",
       " 'JOEY____10-Things-I-Hate-About-You_script': 2742,\n",
       " 'WALTER____10-Things-I-Hate-About-You_script': 2652,\n",
       " 'MANDELLA____10-Things-I-Hate-About-You_script': 1950,\n",
       " 'MISS PERKY____10-Things-I-Hate-About-You_script': 1518,\n",
       " 'CHASTITY____10-Things-I-Hate-About-You_script': 510,\n",
       " 'SHARON____10-Things-I-Hate-About-You_script': 341,\n",
       " 'BRUCE____10-Things-I-Hate-About-You_script': 356,\n",
       " 'TEACHER____10-Things-I-Hate-About-You_script': 409}"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diag_counts['10-Things-I-Hate-About-You_script']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3306"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_series_top_5.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11075"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_series.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "#indiaroma\n",
    "#indiarama \n",
    "char_series_top_5.to_csv('top5_chars.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,row in char_series_top_5.iterrows():\n",
    "    sentz = sentence_tokenizer.tokenize(row[0])\n",
    "    char_series_top_5.loc[index,'tot_sents'] = len(sentz)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_series_top_5['tot_sents'] = np.zeros(char_series_top_5.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have to normalize most of these for number of sentences \n",
    "char_series_top_5.tot_feel =  char_series_top_5.tot_feel/ char_series_top_5.tot_sents\n",
    "char_series_top_5.tot_fill =  char_series_top_5.tot_fill/  char_series_top_5.tot_sents\n",
    "char_series_top_5.tot_hw =  char_series_top_5.tot_hw/  char_series_top_5.tot_sents\n",
    "char_series_top_5.stopwords =  char_series_top_5.stopwords/  char_series_top_5.tot_sents\n",
    "char_series_top_5.uniqwords =  char_series_top_5.uniqwords/  char_series_top_5.tot_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>tot_feel</th>\n",
       "      <th>tot_fill</th>\n",
       "      <th>tot_hw</th>\n",
       "      <th>stopwords</th>\n",
       "      <th>polarity</th>\n",
       "      <th>cursewords</th>\n",
       "      <th>FK</th>\n",
       "      <th>uniqwords</th>\n",
       "      <th>tot_sents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>KAT____10-Things-I-Hate-About-You_script</th>\n",
       "      <td>leave it i said , leave it ! why did n't we ju...</td>\n",
       "      <td>0.067164</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.324627</td>\n",
       "      <td>0.361940</td>\n",
       "      <td>-0.9984</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>268.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PATRICK____10-Things-I-Hate-About-You_script</th>\n",
       "      <td>i missed you . it was a bratwurst . i was eati...</td>\n",
       "      <td>0.028986</td>\n",
       "      <td>0.062802</td>\n",
       "      <td>0.289855</td>\n",
       "      <td>0.415459</td>\n",
       "      <td>-0.9952</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.246377</td>\n",
       "      <td>207.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BIANCA____10-Things-I-Hate-About-You_script</th>\n",
       "      <td>did you change your hair ? you might wan na th...</td>\n",
       "      <td>0.075145</td>\n",
       "      <td>0.052023</td>\n",
       "      <td>0.433526</td>\n",
       "      <td>0.491329</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.416185</td>\n",
       "      <td>173.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CAMERON____10-Things-I-Hate-About-You_script</th>\n",
       "      <td>i do n't think so , ma'am so they tell me ... ...</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.612069</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.724138</td>\n",
       "      <td>116.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MICHAEL____10-Things-I-Hate-About-You_script</th>\n",
       "      <td>you the new guy ? c'mon . i 'm supposed to giv...</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.022556</td>\n",
       "      <td>0.345865</td>\n",
       "      <td>0.624060</td>\n",
       "      <td>0.9984</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.150376</td>\n",
       "      <td>133.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                              0  \\\n",
       "KAT____10-Things-I-Hate-About-You_script      leave it i said , leave it ! why did n't we ju...   \n",
       "PATRICK____10-Things-I-Hate-About-You_script  i missed you . it was a bratwurst . i was eati...   \n",
       "BIANCA____10-Things-I-Hate-About-You_script   did you change your hair ? you might wan na th...   \n",
       "CAMERON____10-Things-I-Hate-About-You_script  i do n't think so , ma'am so they tell me ... ...   \n",
       "MICHAEL____10-Things-I-Hate-About-You_script  you the new guy ? c'mon . i 'm supposed to giv...   \n",
       "\n",
       "                                              tot_feel  tot_fill    tot_hw  \\\n",
       "KAT____10-Things-I-Hate-About-You_script      0.067164  0.000000  0.324627   \n",
       "PATRICK____10-Things-I-Hate-About-You_script  0.028986  0.062802  0.289855   \n",
       "BIANCA____10-Things-I-Hate-About-You_script   0.075145  0.052023  0.433526   \n",
       "CAMERON____10-Things-I-Hate-About-You_script  0.068966  0.068966  0.500000   \n",
       "MICHAEL____10-Things-I-Hate-About-You_script  0.052632  0.022556  0.345865   \n",
       "\n",
       "                                              stopwords  polarity  cursewords  \\\n",
       "KAT____10-Things-I-Hate-About-You_script       0.361940   -0.9984         0.0   \n",
       "PATRICK____10-Things-I-Hate-About-You_script   0.415459   -0.9952         0.0   \n",
       "BIANCA____10-Things-I-Hate-About-You_script    0.491329    0.9968         0.0   \n",
       "CAMERON____10-Things-I-Hate-About-You_script   0.612069    0.9951         0.0   \n",
       "MICHAEL____10-Things-I-Hate-About-You_script   0.624060    0.9984         0.0   \n",
       "\n",
       "                                               FK  uniqwords  tot_sents  \n",
       "KAT____10-Things-I-Hate-About-You_script      0.0   2.250000      268.0  \n",
       "PATRICK____10-Things-I-Hate-About-You_script  0.0   2.246377      207.0  \n",
       "BIANCA____10-Things-I-Hate-About-You_script   0.0   2.416185      173.0  \n",
       "CAMERON____10-Things-I-Hate-About-You_script  0.0   2.724138      116.0  \n",
       "MICHAEL____10-Things-I-Hate-About-You_script  0.0   3.150376      133.0  "
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_series_top_5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_series_top_5.to_csv('top5_chars.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
