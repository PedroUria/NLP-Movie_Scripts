{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Dead-Poets-Society_script.json') as f:\n",
    "    story = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# setup a new hash to store the results in\n",
    "def script_cleaning(script):\n",
    "    processed_article_hash = {}\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # iterate through the keys, i.e. document ids, in the hash to pull out the stored text and process\n",
    "    for key in script.keys():\n",
    "        text_of_article = script[key]\n",
    "        word_tokens = word_tokenize(text_of_article)\n",
    "        words = [word for word in word_tokens if word.isalpha()]\n",
    "        words = [str.lower(w) for w in words if not str.lower(w) in stop_words]\n",
    "        processed_article_hash[key] = [porter_stemmer.stem(word) for word in words]\n",
    "    return processed_article_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def jacardian_distance(document_1_data, document_2_data):\n",
    "    words_in_doc_1_not_in_doc_2 = list(set(document_1_data) - set(document_2_data))\n",
    "    words_in_doc_2_not_in_doc_1 = list(set(document_2_data) - set(document_1_data))\n",
    "    words_in_both_doc_1_and_doc_2 = list(set.intersection(*[set(document_1_data), set(document_2_data)]))\n",
    "    \n",
    "    jacardian = len(words_in_both_doc_1_and_doc_2)/(len(words_in_doc_1_not_in_doc_2)+\n",
    "                                                    len(words_in_doc_2_not_in_doc_1)+\n",
    "                                                    len(words_in_both_doc_1_and_doc_2))# divide the counts appropiately\n",
    "    \n",
    "    return jacardian\n",
    "\n",
    "def cosine_similarity(document_1_data, document_2_data):\n",
    "    document_vector_word_index = list(set.union(set(document_1_data),set(document_2_data))) # here fill this with an ordered list of all the unique words across both documents\n",
    "    document_1_vector = np.array([document_1_data.count(word) for word in document_vector_word_index]) # fill in the array with the frequency of the words in the document\n",
    "    document_2_vector = np.array([document_2_data.count(word) for word in document_vector_word_index]) # fill in the array with the frequency of the words in the document\n",
    "    dot_product_of_two_document_vectors = document_1_vector.dot(document_2_vector)/(np.sqrt(np.dot(document_1_vector,document_1_vector)) * np.sqrt(np.dot(document_2_vector,document_2_vector)))\n",
    "    return dot_product_of_two_document_vectors # you can refer to the numpy information on how to calculate the dot product of vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# create a variable to store your table data... you could use a hash or some other data structure. \n",
    "# We just want it to identify which document is being compared to which other document.\n",
    "def prep_data_structs(processed_article_hash):\n",
    "    data_structure_for_jacard_similarity = {}#\n",
    "    data_structure_for_cosine_similarity = {}#\n",
    "\n",
    "    for doc_1_key in processed_article_hash.keys():\n",
    "        data_structure_for_jacard_similarity[doc_1_key] = {}\n",
    "        data_structure_for_cosine_similarity[doc_1_key] = {}\n",
    "        for doc_2_key in processed_article_hash.keys():\n",
    "            # we have the nested for loops as one way to compare each document to each other document\n",
    "            data_structure_for_jacard_similarity[doc_1_key][doc_2_key] = jacardian_distance(processed_article_hash[doc_1_key], \n",
    "                                                                                            processed_article_hash[doc_2_key])\n",
    "            data_structure_for_cosine_similarity[doc_1_key][doc_2_key] = cosine_similarity(processed_article_hash[doc_1_key], processed_article_hash[doc_2_key])\n",
    "    return data_structure_for_jacard_similarity, data_structure_for_cosine_similarity\n",
    "# finally, find some way to present this data back. Either as a straight table or a heatmap.\n",
    "def create_heatmap(data_dict):\n",
    "    temp = pd.DataFrame(data_dict)\n",
    "    temp = temp.sort_index()[temp.sort_index().index]\n",
    "    \n",
    "    article_row = [str(each) for each in temp.index]\n",
    "    article_col = [str(each) for each in temp.columns]\n",
    "\n",
    "    data = temp.values\n",
    "    fig, ax = plt.subplots(figsize=(12,12))\n",
    "    im = ax.imshow(data)\n",
    "\n",
    "    # We want to show all ticks...\n",
    "    ax.set_xticks(np.arange(len(article_col)))\n",
    "    ax.set_yticks(np.arange(len(article_row)))\n",
    "    # ... and label them with the respective list entries\n",
    "    ax.set_xticklabels(article_col)\n",
    "    ax.set_yticklabels(article_row)\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    for i in range(len(article_row)):\n",
    "        for j in range(len(article_col)):\n",
    "            text = ax.text(j, i, round(data[i, j],2),\n",
    "                           ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "    ax.set_title(\"Similarity Heatmap\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEIL 6.1779615018250436 0.22448133704296158 427\n",
      "KEATING 5.961120375308899 0.20746671642834685 617\n",
      "CHARLIE 6.205067926273911 0.21409281467343858 310\n",
      "KNOX 5.949016213828811 0.20524334396625382 190\n",
      "TODD 5.239155157677 0.22385699157921035 146\n",
      "CAMERON 5.396288632530922 0.22601329867719602 203\n",
      "MEEKS 3.8618271983108734 0.2087160435435516 117\n",
      "CHRIS 3.8367990249902584 0.20623553278627615 74\n",
      "PITTS 4.839671300385916 0.20981461016793404 112\n",
      "MCALLISTER 2.4397637965689043 0.20945734504874425 59\n",
      "BOYS 2.4568313827951553 0.21470326341100868 44\n",
      "MR NOLAN 2.7964977952980736 0.2106717526902178 86\n",
      "CHET 2.4844876727004928 0.2200488480472969 15\n",
      "MR PERRY 3.6108990281309143 0.21449725424829866 44\n",
      "HAGER 2.801439454046898 0.21378917566662448 39\n",
      "GLORIA 2.7330743595791516 0.21949435240040732 15\n",
      "BUBBA 2.451435290492159 0.21280178977650388 28\n",
      "BOY 3.022597085274888 0.2110331797514609 16\n",
      "PUCK 1.9955281236760793 0.21535707967300263 94\n"
     ]
    }
   ],
   "source": [
    "with open('Dead-Poets-Society_script.json') as f:\n",
    "    story = json.load(f)\n",
    "\n",
    "processed_data = script_cleaning(story['dialogues'])\n",
    "data_structs_j, data_structs_c = prep_data_structs(processed_data)\n",
    "#create_heatmap(data_structs_c)\n",
    "#create_heatmap(data_structs_j)\n",
    "for k in data_structs_c.keys():\n",
    "    print(k, sum(data_structs_c[k].values()), np.std(list(data_structs_c[k].values())), len(set(processed_data[k])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEO 5.846177656602462 0.24518717217528516 239\n",
      "MORPHEUS 5.5328876676720595 0.24762542200642695 434\n",
      "TRINITY 5.691913787052784 0.2494729132247568 187\n",
      "AGENT SMITH 4.453894308800624 0.23285085474369022 278\n",
      "TANK 4.892356151799113 0.22841507267809152 174\n",
      "CYPHER 5.510050522675804 0.24767711194581804 201\n",
      "GIZMO 3.44746916739152 0.22625130785993552 52\n",
      "ORACLE 4.534667291478955 0.22797570750137616 72\n",
      "AGENT JONES 2.4211850361440317 0.23602006758723632 27\n",
      "APOC 3.2412507046602195 0.24289311912833547 13\n",
      "MOUSE 3.7765998645012395 0.23103800770284924 22\n",
      "AGENT BROWN 1.2839238097521912 0.2454413326364293 12\n",
      "SCREEN 3.381209747659355 0.22769090331528014 56\n",
      "DOZER 1.8156339648855822 0.23917954904974856 11\n",
      "CABLE 2.246286296370231 0.23433216507306828 15\n"
     ]
    }
   ],
   "source": [
    "with open('Matrix,-The_script.json') as f:\n",
    "    story = json.load(f)\n",
    "\n",
    "processed_data = script_cleaning(story['dialogues'])\n",
    "data_structs_j, data_structs_c = prep_data_structs(processed_data)\n",
    "#create_heatmap(data_structs_c)\n",
    "#create_heatmap(data_structs_j)\n",
    "for k in data_structs_c.keys():\n",
    "    print(k, sum(data_structs_c[k].values()), np.std(list(data_structs_c[k].values())), len(set(processed_data[k])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDIANA 2.925704549160656 0.27527275823115915 593\n",
      "WILLIE 3.1022257572357503 0.2738923714690337 386\n",
      "SHORT ROUND 1.9593718006668235 0.3198393470480051 136\n",
      "CHATTAR LAL 2.3947724434206132 0.27292709430860534 161\n",
      "LAO 1.984761214370947 0.3007090545166056 43\n",
      "MOLA RAM 1.9988767739663618 0.2907239358784528 72\n",
      "MAHARAJAH 2.1265517425237634 0.2830271902890434 68\n",
      "SHAMAN 2.0527455510641173 0.29671035301941806 49\n"
     ]
    }
   ],
   "source": [
    "with open('Indiana-Jones-and-the-Temple-of-Doom_script.json') as f:\n",
    "    story = json.load(f)\n",
    "\n",
    "processed_data = script_cleaning(story['dialogues'])\n",
    "data_structs_j, data_structs_c = prep_data_structs(processed_data)\n",
    "#create_heatmap(data_structs_c)\n",
    "#create_heatmap(data_structs_j)\n",
    "for k in data_structs_c.keys():\n",
    "    print(k, sum(data_structs_c[k].values()), np.std(list(data_structs_c[k].values())), len(set(processed_data[k])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RED 5.363662332771118 0.23648591573735847 973\n",
      "ANDY 5.002264781135497 0.2459950527208025 628\n",
      "NORTON 4.8041832895401395 0.23836073514155867 356\n",
      "HEYWOOD 4.542109221321405 0.22424701734870925 186\n",
      "HADLEY 4.5728359125164735 0.21131617525597685 189\n",
      "BROOKS 4.119083688838406 0.239573833812448 219\n",
      "TOMMY 4.418295624929571 0.23251212915044617 157\n",
      "FLOYD 2.631345905856876 0.23578352158700114 44\n",
      "BOGS 2.867453952944398 0.23508433692972247 52\n",
      "VOICE 1.3999211375289493 0.2510827252494794 15\n",
      "SNOOZE 3.506759902153036 0.22709693656338684 25\n",
      "GUARD 2.077734591854184 0.24031056093079597 21\n",
      "JIGGER 3.009562358619279 0.22940361021826666 21\n",
      "HAIG 2.8051084709796728 0.2312043550296021 21\n"
     ]
    }
   ],
   "source": [
    "with open('Shawshank-Redemption,-The_script.json') as f:\n",
    "    story = json.load(f)\n",
    "\n",
    "processed_data = script_cleaning(story['dialogues'])\n",
    "data_structs_j, data_structs_c = prep_data_structs(processed_data)\n",
    "#create_heatmap(data_structs_c)\n",
    "#create_heatmap(data_structs_j)\n",
    "for k in data_structs_c.keys():\n",
    "    print(k, sum(data_structs_c[k].values()), np.std(list(data_structs_c[k].values())), len(set(processed_data[k])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COLIN 7.489506993077247 0.199803858711933 532\n",
      "BILLY 8.24340686580522 0.17543250503524876 494\n",
      "COSTELLO 7.358684980331508 0.1970240005338749 482\n",
      "MADOLYN 5.350708067284642 0.20904295732514003 222\n",
      "QUEENAN 5.976914411747699 0.20781494741600878 248\n",
      "ELLERBY 6.346910127503907 0.20776358431534891 282\n",
      "DIGNAM 6.112070626623671 0.19197149904863997 185\n",
      "MISTER FRENCH 6.182231892197827 0.19360183562509184 132\n",
      "BROWN 4.034620785462869 0.21326583457464882 73\n",
      "DELAHUNT 5.553701067588109 0.1911385211821531 76\n",
      "FITZY 5.0411949026083045 0.20229531770830544 59\n",
      "SEAN 4.493532644143483 0.20262556398523537 90\n",
      "YOUNG COSTELLO 3.9205965707443156 0.21603274901799305 71\n",
      "DETECTIVE 1 4.379925439840374 0.20944194173981312 31\n",
      "BARRIGAN 4.670121747437632 0.20025165515423776 38\n",
      "BANKROBBER 3.813685596487633 0.21226630973120608 26\n"
     ]
    }
   ],
   "source": [
    "with open('Departed,-The_script.json') as f:\n",
    "    story = json.load(f)\n",
    "\n",
    "processed_data = script_cleaning(story['dialogues'])\n",
    "data_structs_j, data_structs_c = prep_data_structs(processed_data)\n",
    "#create_heatmap(data_structs_c)\n",
    "#create_heatmap(data_structs_j)\n",
    "for k in data_structs_c.keys():\n",
    "    print(k, sum(data_structs_c[k].values()), np.std(list(data_structs_c[k].values())), len(set(processed_data[k])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOHN 4.713708270525782 0.1976218444307212 227\n",
      "VIGGO 4.912649858322166 0.1990259766088584 195\n",
      "IOSEF 3.51780442908033 0.22664439965651456 57\n",
      "AURELIO 4.4031958092365135 0.20765389486425784 45\n",
      "MANAGER 3.3048531186716414 0.26057193735152673 58\n",
      "WAITRESS 3.1140849070892394 0.26478745007431864 21\n",
      "CAPTAIN 2.950061336806725 0.2191558932377404 32\n",
      "MARCUS 4.042232293007286 0.22434541459482674 74\n",
      "HARRY 4.413165122291295 0.24851339037037737 26\n",
      "VIKTOR 2.812274379151498 0.23416884707174576 28\n",
      "EDWARDO 3.46725439817502 0.2278626813105147 44\n",
      "CHARLIE 4.000871888667173 0.2518701945473562 37\n",
      "JENNY 2.6561951644328383 0.22636821578402522 30\n",
      "JIMMY 3.8771157805157253 0.24276025696229905 20\n",
      "BANK MANAGER 1.8860229615871291 0.24158349352241612 5\n",
      "EDDIE 2.249099916781039 0.23000381885945975 10\n"
     ]
    }
   ],
   "source": [
    "with open('John-Wick_script.json') as f:\n",
    "    story = json.load(f)\n",
    "\n",
    "processed_data = script_cleaning(story['dialogues'])\n",
    "data_structs_j, data_structs_c = prep_data_structs(processed_data)\n",
    "#create_heatmap(data_structs_c)\n",
    "#create_heatmap(data_structs_j)\n",
    "for k in data_structs_c.keys():\n",
    "    print(k, sum(data_structs_c[k].values()), np.std(list(data_structs_c[k].values())), len(set(processed_data[k])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JORDAN 7.737368620566339 0.19367404295659255 1477\n",
      "DONNIE 6.180322370977525 0.20237925536677015 291\n",
      "NAOMI 5.402352301658026 0.19813534443600644 207\n",
      "AGENT DENHAM 4.308255125616418 0.1948103688198134 125\n",
      "TERESA 4.5798295507525735 0.19601574501432853 55\n",
      "AUNT EMMA 3.72653910463981 0.19464654407763363 62\n",
      "MAX 4.09784870799753 0.20692733331507054 80\n",
      "BRAD 3.604241401915729 0.19975221121151998 53\n",
      "BO DIETL 4.7062671289473 0.19656291591819847 92\n",
      "SAUREL 3.3124262195981253 0.19329198618581217 115\n",
      "MARK HANNA 4.945603129369456 0.19954630645323737 136\n",
      "JORDAN  (CONT'D) 4.795907067225681 0.1945248018747822 102\n",
      "SEA OTTER 3.917100678867187 0.19501143118097594 39\n",
      "STEVE MADDEN 2.506547065147964 0.19904919173324082 52\n",
      "RUGRAT 2.4691689987839855 0.1996281367782609 28\n",
      "GENE HACKMAN 1.21884150769059 0.20696076533739766 19\n",
      "JERRY FOGEL 3.7470869893867906 0.20214439341642976 34\n",
      "DWAYNE 3.228153079864293 0.19886777640057832 54\n",
      "CHESTER MING 3.2503499101417836 0.19490574866114882 27\n",
      "JANET 2.2893351244290714 0.20336032709443744 23\n",
      "MANNY RISKIN 3.6743665107026406 0.193898512233187 73\n",
      "CAPTAIN TED 1.937217028162057 0.20228268529119858 29\n"
     ]
    }
   ],
   "source": [
    "with open('Wolf-of-Wall-Street,-The_script.json') as f:\n",
    "    story = json.load(f)\n",
    "\n",
    "processed_data = script_cleaning(story['dialogues'])\n",
    "data_structs_j, data_structs_c = prep_data_structs(processed_data)\n",
    "#create_heatmap(data_structs_c)\n",
    "#create_heatmap(data_structs_j)\n",
    "for k in data_structs_c.keys():\n",
    "    print(k, sum(data_structs_c[k].values()), np.std(list(data_structs_c[k].values())), len(set(processed_data[k])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HICCUP 4.981710506036841 0.20795574842536127 457\n",
      "STOICK 4.483731055516061 0.22250188189607745 226\n",
      "VALKA 4.67898571402542 0.22368359353188003 232\n",
      "ASTRID 4.189342420675554 0.2203174168130256 144\n",
      "ERET 4.344440863489888 0.2276038839406494 166\n",
      "DRAGO 3.9099432528016553 0.24000232950532957 93\n",
      "GOBBER 3.2869439779245595 0.23542587153745714 166\n",
      "RUFFNUT 3.033708305593434 0.23550098630051775 46\n",
      "FISHLEGS 3.0282447404375157 0.2336490467310706 45\n",
      "TUFFNUT 3.2436644741119225 0.2281556897532403 49\n",
      "SNOTLOUT 2.5016563980603 0.24670943021879682 40\n"
     ]
    }
   ],
   "source": [
    "with open('How-to-Train-Your-Dragon-2_script.json') as f:\n",
    "    story = json.load(f)\n",
    "\n",
    "processed_data = script_cleaning(story['dialogues'])\n",
    "data_structs_j, data_structs_c = prep_data_structs(processed_data)\n",
    "#create_heatmap(data_structs_c)\n",
    "#create_heatmap(data_structs_j)\n",
    "for k in data_structs_c.keys():\n",
    "    print(k, sum(data_structs_c[k].values()), np.std(list(data_structs_c[k].values())), len(set(processed_data[k])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
