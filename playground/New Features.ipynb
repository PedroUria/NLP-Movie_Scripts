{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup a new hash to store the results in\n",
    "def script_cleaning(script):\n",
    "    processed_article_hash = {}\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # iterate through the keys, i.e. document ids, in the hash to pull out the stored text and process\n",
    "    for key in script.keys():\n",
    "        text_of_article = script[key]\n",
    "        word_tokens = word_tokenize(text_of_article)\n",
    "        words = [word for word in word_tokens if word.isalpha()]\n",
    "        words = [str.lower(w) for w in words if not str.lower(w) in stop_words]\n",
    "        processed_article_hash[key] = [porter_stemmer.stem(word) for word in words]\n",
    "    return processed_article_hash\n",
    "\n",
    "def cosine_similarity(document_1_data, document_2_data):\n",
    "    document_vector_word_index = list(set.union(set(document_1_data),set(document_2_data))) \n",
    "    document_1_vector = np.array([document_1_data.count(word) for word in document_vector_word_index])\n",
    "    document_2_vector = np.array([document_2_data.count(word) for word in document_vector_word_index])\n",
    "    dot_product_of_two_document_vectors = document_1_vector.dot(document_2_vector)/(np.sqrt(np.dot(document_1_vector,document_1_vector)) * np.sqrt(np.dot(document_2_vector,document_2_vector)))\n",
    "    return dot_product_of_two_document_vectors \n",
    "\n",
    "def prep_data_structs(processed_article_hash):\n",
    "    data_structure_for_cosine_similarity = {}#\n",
    "\n",
    "    for doc_1_key in processed_article_hash.keys():\n",
    "        data_structure_for_cosine_similarity[doc_1_key] = {}\n",
    "        for doc_2_key in processed_article_hash.keys():\n",
    "            # we have the nested for loops as one way to compare each document to each other document\n",
    "            data_structure_for_cosine_similarity[doc_1_key][doc_2_key] = cosine_similarity(processed_article_hash[doc_1_key], processed_article_hash[doc_2_key])\n",
    "    return data_structure_for_cosine_similarity\n",
    "\n",
    "def create_heatmap(data_dict):\n",
    "    temp = pd.DataFrame(data_dict)\n",
    "    temp = temp.sort_index()[temp.sort_index().index]\n",
    "    \n",
    "    article_row = [str(each) for each in temp.index]\n",
    "    article_col = [str(each) for each in temp.columns]\n",
    "\n",
    "    data = temp.values\n",
    "    fig, ax = plt.subplots(figsize=(12,12))\n",
    "    im = ax.imshow(data)\n",
    "\n",
    "    # We want to show all ticks...\n",
    "    ax.set_xticks(np.arange(len(article_col)))\n",
    "    ax.set_yticks(np.arange(len(article_row)))\n",
    "    # ... and label them with the respective list entries\n",
    "    ax.set_xticklabels(article_col)\n",
    "    ax.set_yticklabels(article_row)\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    for i in range(len(article_row)):\n",
    "        for j in range(len(article_col)):\n",
    "            text = ax.text(j, i, round(data[i, j],2),\n",
    "                           ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "    ax.set_title(\"Similarity Heatmap\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def read_json(file):\n",
    "    with open(file) as f:\n",
    "        story = json.load(f)\n",
    "    return story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akartikay/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in true_divide\n",
      "/Users/akartikay/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "movie_dict = {}\n",
    "for file in os.listdir('../diag_jsons/'):\n",
    "    filepath = '../diag_jsons/' + file\n",
    "    movie = read_json(filepath)\n",
    "    processed_data = script_cleaning(movie['dialogues'])\n",
    "    data_structs_c = prep_data_structs(processed_data)\n",
    "    temp = [np.mean(list(data_structs_c[k].values())) for k in data_structs_c.keys()]\n",
    "    movie_dict[filepath] = np.mean(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('../feat_extraction/movies_with_feats_use_me.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df = pd.DataFrame({'filepath':list(movie_dict.keys()),'avg_sim_score':list(movie_dict.values())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df['filepath'] = movie_df['filepath'].str.split('/').str[-1].str.split('.').str[0].str.split('_').str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df.columns = ['Processed Title','avg_sim_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie = data.merge(movie_df, on='Processed Title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success_data = pd.read_csv(\"../success_data.csv\").drop('Unnamed: 0', axis=1) #, index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize(row):\n",
    "    if row[\"Worldwide ROI (%)\"] > 100:\n",
    "        return 3\n",
    "    elif 100 > row[\"Worldwide ROI (%)\"] > 25:\n",
    "        return 2\n",
    "    elif 25 > row[\"Worldwide ROI (%)\"] > -25:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success_data['target'] = success_data.apply(discretize,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df = movie.merge(success_data, on='Processed Title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df = movie_df.drop(['Unnamed: 0','Unnamed: 0.1'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filterCols = ['passive_ratio', 'pct_coref_sents', 'tot_unique_per_sent','tot_stop_per_sent', 'std_of_overall_polarity', \n",
    "              'wav_polarity', 'avg_FK', 'sign_check_char_mention_polairty', 'std_of_char_mention_polarity', \n",
    "              'Processed Title', 'avg_sim_score', 'Success']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = movie_df[filterCols]\n",
    "df = df.replace('?',np.NaN)\n",
    "df = df.dropna(how='any')\n",
    "\n",
    "X = df.drop('Success',axis=1)\n",
    "y = df['Success']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler()\n",
    "X_id = X['Processed Title']\n",
    "X = ss.fit_transform(X.drop('Processed Title',axis=1).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "clfs = {'lr': LogisticRegression(random_state=0),\n",
    "        'mlp': MLPClassifier(random_state=0),\n",
    "        'dt': DecisionTreeClassifier(random_state=0),\n",
    "        'rf': RandomForestClassifier(random_state=0),\n",
    "        'svc': SVC(random_state=0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pipe_clfs = {}\n",
    "\n",
    "for name, clf in clfs.items():\n",
    "    # Implement me\n",
    "    pipe_clfs[name] = Pipeline([('StandardScaler',StandardScaler()),('clf',clf)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grids = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_range = [10 ** i for i in range(-4, 5)]\n",
    "\n",
    "param_grid = [{'clf__multi_class': ['ovr'], \n",
    "               'clf__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "               'clf__C': C_range},\n",
    "              {'clf__multi_class': ['multinomial'],\n",
    "               'clf__solver': ['newton-cg', 'lbfgs', 'sag', 'saga'],\n",
    "               'clf__C': C_range}]\n",
    "\n",
    "# Implement me\n",
    "param_grids['lr'] = param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [{'clf__hidden_layer_sizes': [10, 100, 200],\n",
    "               'clf__activation': ['identity', 'logistic', 'tanh', 'relu']}]\n",
    "\n",
    "# Implement me\n",
    "param_grids['mlp'] = param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [{'clf__min_samples_split': [2, 10, 30],\n",
    "               'clf__min_samples_leaf': [1, 10, 30]}]\n",
    "\n",
    "# Implement me\n",
    "param_grids['dt'] = param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [{'clf__n_estimators': [2, 10, 30],\n",
    "               'clf__min_samples_split': [2, 10, 30],\n",
    "               'clf__min_samples_leaf': [1, 10, 30]}]\n",
    "\n",
    "# Implement me\n",
    "param_grids['rf'] = param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [{'clf__C': [0.01, 0.1, 1, 10, 100],\n",
    "               'clf__gamma': [0.01, 0.1, 1, 10, 100],\n",
    "               'clf__kernel': ['linear', 'poly', 'rbf', 'sigmoid']}]\n",
    "\n",
    "# Implement me\n",
    "param_grids['svc'] = param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# The list of [best_score_, best_params_, best_estimator_]\n",
    "best_score_param_estimators = []\n",
    "\n",
    "# For each classifier\n",
    "for name in pipe_clfs.keys():\n",
    "    # GridSearchCV\n",
    "    # Implement me\n",
    "    gs = GridSearchCV(estimator=pipe_clfs[name], param_grid=param_grids[name], scoring='accuracy', n_jobs=-1, cv=StratifiedKFold(\n",
    "    n_splits=10, shuffle=True, random_state=10))\n",
    "    \n",
    "    # Fit the pipeline\n",
    "    gs.fit(X, y)\n",
    "    \n",
    "    # Update best_score_param_estimators\n",
    "    best_score_param_estimators.append([gs.best_score_, gs.best_params_, gs.best_estimator_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score_param_estimators = sorted(best_score_param_estimators, key=lambda x : x[0], reverse=True)\n",
    "\n",
    "# For each [best_score_, best_params_, best_estimator_]\n",
    "for best_score_param_estimator in best_score_param_estimators:\n",
    "    # Print out [best_score_, best_params_, best_estimator_], where best_estimator_ is a pipeline\n",
    "    # Since we only print out the type of classifier of the pipeline\n",
    "    print([best_score_param_estimator[0], best_score_param_estimator[1], type(best_score_param_estimator[2].named_steps['clf'])], end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score_param_estimators[0][-1].predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ros = RandomOverSampler()\n",
    "X_ros, y_ros = ros.fit_sample(X, y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_ros, y_ros, test_size=0.3, random_state=0, stratify=y_ros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ros, y_ros = ros.fit_sample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ros.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ros.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_ros, y_ros, test_size=0.3, random_state=0, stratify=y_ros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score_param_estimators[0][-1].predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../playground/nlp_movie_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
